//go:build !noasm && arm64
// Code generated by gocc rev-5dd7267-dirty -- DO NOT EDIT.
//
// Source file         : ascii_neon.c
// Clang version       : Apple clang version 16.0.0 (clang-1600.0.26.6)
// Target architecture : arm64
// Compiler options    : [none]

#include "textflag.h"

DATA LCPI0_0<>+0x00(SB)/8, $0x8040201008040201
DATA LCPI0_0<>+0x08(SB)/8, $0x8040201008040201
GLOBL LCPI0_0<>(SB), (RODATA|NOPTR), $16

TEXT ·indexAnyNeonBitset(SB), NOSPLIT, $0-56
	MOVD  data+0(FP), R0
	MOVD  data_len+8(FP), R1
	MOVD  bitset0+16(FP), R2
	MOVD  bitset1+24(FP), R3
	MOVD  bitset2+32(FP), R4
	MOVD  bitset3+40(FP), R5
	CBZ   R1, LBB0_13        // <--                                  // cbz	x1, .LBB0_13
	NOP                      // (skipped)                            // stp	x29, x30, [sp, #-16]!
	ADD   R1, R0, R9         // <--                                  // add	x9, x0, x1
	AND   $15, R1, R8        // <--                                  // and	x8, x1, #0xf
	SUB   R8, R9, R10        // <--                                  // sub	x10, x9, x8
	MOVD  R0, R9             // <--                                  // mov	x9, x0
	CMP   R0, R10            // <--                                  // cmp	x10, x0
	NOP                      // (skipped)                            // mov	x29, sp
	BLS   LBB0_5             // <--                                  // b.ls	.LBB0_5
	FMOVD R2, F0             // <--                                  // fmov	d0, x2
	FMOVD R3, F2             // <--                                  // fmov	d2, x3
	MOVD  $LCPI0_0<>(SB), R9 // <--                                  // adrp	x9, .LCPI0_0
	MOVD  ZR, R11            // <--                                  // mov	x11, xzr
	VMOV  V2.D[0], V0.D[1]   // <--                                  // mov	v0.d[1], v2.d[0]
	FMOVD R5, F2             // <--                                  // fmov	d2, x5
	FMOVD R4, F1             // <--                                  // fmov	d1, x4
	WORD  $0x3dc00123        // FMOVQ (R9), F3                       // ldr	q3, [x9, :lo12:.LCPI0_0]
	VMOV  V2.D[0], V1.D[1]   // <--                                  // mov	v1.d[1], v2.d[0]
	WORD  $0x4f00e4e2        // VMOVI $7, V2.B16                     // movi	v2.16b, #7

LBB0_3:
	WORD   $0x3ceb6804                      // FMOVQ (R0)(R11), F4                  // ldr	q4, [x0, x11]
	WORD   $0x6f0d0485                      // VUSHR $3, V4.B16, V5.B16             // ushr	v5.16b, v4.16b, #3
	VAND   V2.B16, V4.B16, V4.B16           // <--                                  // and	v4.16b, v4.16b, v2.16b
	VTBL   V5.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v5.16b
	VTBL   V4.B16, [V3.B16], V4.B16         // <--                                  // tbl	v4.16b, { v3.16b }, v4.16b
	VCMTST V5.B16, V4.B16, V4.B16           // <--                                  // cmtst	v4.16b, v4.16b, v5.16b
	WORD   $0x0f0c8484                      // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD  F4, R9                           // <--                                  // fmov	x9, d4
	CBNZ   R9, LBB0_14                      // <--                                  // cbnz	x9, .LBB0_14
	ADD    $16, R11, R11                    // <--                                  // add	x11, x11, #16
	ADD    R11, R0, R9                      // <--                                  // add	x9, x0, x11
	CMP    R10, R9                          // <--                                  // cmp	x9, x10
	BCC    LBB0_3                           // <--                                  // b.lo	.LBB0_3

LBB0_5:
	CBZ R8, LBB0_12 // <--                                  // cbz	x8, .LBB0_12
	SUB R0, R9, R0  // <--                                  // sub	x0, x9, x0

LBB0_7:
	WORD $0x3840152a     // MOVBU.P 1(R9), R10                   // ldrb	w10, [x9], #1
	MOVD R2, R11         // <--                                  // mov	x11, x2
	LSRW $6, R10, R12    // <--                                  // lsr	w12, w10, #6
	CBZW R12, LBB0_10    // <--                                  // cbz	w12, .LBB0_10
	MOVD R4, R11         // <--                                  // mov	x11, x4
	CMPW $2, R12         // <--                                  // cmp	w12, #2
	BEQ  LBB0_10         // <--                                  // b.eq	.LBB0_10
	CMPW $1, R12         // <--                                  // cmp	w12, #1
	CSEL NE, R5, R3, R11 // <--                                  // csel	x11, x5, x3, ne

LBB0_10:
	LSR  R10, R11, R10    // <--                                  // lsr	x10, x11, x10
	TBNZ $0, R10, LBB0_15 // <--                                  // tbnz	w10, #0, .LBB0_15
	ADD  $1, R0, R0       // <--                                  // add	x0, x0, #1
	SUBS $1, R8, R8       // <--                                  // subs	x8, x8, #1
	BNE  LBB0_7           // <--                                  // b.ne	.LBB0_7

LBB0_12:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_13:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_14:
	RBIT R9, R8         // <--                                  // rbit	x8, x9
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R11, R0 // <--                                  // add	x0, x11, x8, lsr #2

LBB0_15:
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

TEXT ·ValidString(SB), NOSPLIT, $0-17
	MOVD data+0(FP), R0
	MOVD length+8(FP), R1
	NOP                   // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP  $16, R1          // <--                                  // cmp	x1, #16
	NOP                   // (skipped)                            // mov	x29, sp
	BCC  LBB1_7           // <--                                  // b.lo	.LBB1_7
	ADD  R1, R0, R9       // <--                                  // add	x9, x0, x1
	AND  $63, R1, R8      // <--                                  // and	x8, x1, #0x3f
	SUB  R8, R9, R9       // <--                                  // sub	x9, x9, x8
	CMP  R0, R9           // <--                                  // cmp	x9, x0
	BLS  LBB1_4           // <--                                  // b.ls	.LBB1_4

LBB1_2:
	VLD1  (R0), [V0.B16, V1.B16, V2.B16, V3.B16] // <--                                  // ld1	{ v0.16b, v1.16b, v2.16b, v3.16b }, [x0]
	VORR  V1.B16, V0.B16, V4.B16                 // <--                                  // orr	v4.16b, v0.16b, v1.16b
	VORR  V2.B16, V3.B16, V0.B16                 // <--                                  // orr	v0.16b, v3.16b, v2.16b
	VORR  V0.B16, V4.B16, V0.B16                 // <--                                  // orr	v0.16b, v4.16b, v0.16b
	WORD  $0x4e20a800                            // VCMLT $0, V0.B16, V0.B16             // cmlt	v0.16b, v0.16b, #0
	WORD  $0x0f0c8400                            // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R10                                // <--                                  // fmov	x10, d0
	CBNZ  R10, LBB1_12                           // <--                                  // cbnz	x10, .LBB1_12
	ADD   $64, R0, R0                            // <--                                  // add	x0, x0, #64
	CMP   R9, R0                                 // <--                                  // cmp	x0, x9
	BCC   LBB1_2                                 // <--                                  // b.lo	.LBB1_2

LBB1_4:
	ADD R8, R0, R8  // <--                                  // add	x8, x0, x8
	AND $15, R1, R1 // <--                                  // and	x1, x1, #0xf
	SUB R1, R8, R8  // <--                                  // sub	x8, x8, x1
	CMP R8, R0      // <--                                  // cmp	x0, x8
	BCS LBB1_7      // <--                                  // b.hs	.LBB1_7

LBB1_5:
	WORD  $0x3dc00000 // FMOVQ (R0), F0                       // ldr	q0, [x0]
	WORD  $0x4e20a800 // VCMLT $0, V0.B16, V0.B16             // cmlt	v0.16b, v0.16b, #0
	WORD  $0x0f0c8400 // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R9      // <--                                  // fmov	x9, d0
	CBNZ  R9, LBB1_12 // <--                                  // cbnz	x9, .LBB1_12
	ADD   $16, R0, R0 // <--                                  // add	x0, x0, #16
	CMP   R8, R0      // <--                                  // cmp	x0, x8
	BCC   LBB1_5      // <--                                  // b.lo	.LBB1_5

LBB1_7:
	CMP   $8, R1          // <--                                  // cmp	x1, #8
	BCS   LBB1_11         // <--                                  // b.hs	.LBB1_11
	TBNZ  $2, R1, LBB1_13 // <--                                  // tbnz	w1, #2, .LBB1_13
	CBZ   R1, LBB1_15     // <--                                  // cbz	x1, .LBB1_15
	LSR   $1, R1, R8      // <--                                  // lsr	x8, x1, #1
	ADD   R1, R0, R9      // <--                                  // add	x9, x0, x1
	WORD  $0x3940000a     // MOVBU (R0), R10                      // ldrb	w10, [x0]
	WORD  $0x38686808     // MOVBU (R0)(R8), R8                   // ldrb	w8, [x0, x8]
	WORD  $0x385ff129     // LDURBW -1(R9), R9                    // ldurb	w9, [x9, #-1]
	ORRW  R9, R10, R9     // <--                                  // orr	w9, w10, w9
	ORRW  R9, R8, R8      // <--                                  // orr	w8, w8, w9
	SXTBW R8, R8          // <--                                  // sxtb	w8, w8
	CMPW  $0, R8          // <--                                  // cmp	w8, #0
	CSETW GE, R0          // <--                                  // cset	w0, ge
	NOP                   // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP)  // <--
	RET                   // <--                                  // ret

LBB1_11:
	ADD  R1, R0, R8                // <--                                  // add	x8, x0, x1
	WORD $0xf9400009               // MOVD (R0), R9                        // ldr	x9, [x0]
	WORD $0xf85f8108               // MOVD -8(R8), R8                      // ldur	x8, [x8, #-8]
	ORR  R9, R8, R8                // <--                                  // orr	x8, x8, x9
	TST  $-9187201950435737472, R8 // <--                                  // tst	x8, #0x8080808080808080
	JMP  LBB1_14                   // <--                                  // b	.LBB1_14

LBB1_12:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+16(FP) // <--
	RET                 // <--                                  // ret

LBB1_13:
	ADD  R1, R0, R8      // <--                                  // add	x8, x0, x1
	WORD $0xb9400009     // MOVWU (R0), R9                       // ldr	w9, [x0]
	WORD $0xb85fc108     // MOVWU -4(R8), R8                     // ldur	w8, [x8, #-4]
	ORRW R9, R8, R8      // <--                                  // orr	w8, w8, w9
	TSTW $2155905152, R8 // <--                                  // tst	w8, #0x80808080

LBB1_14:
	CSETW EQ, R0         // <--                                  // cset	w0, eq
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP) // <--
	RET                  // <--                                  // ret

LBB1_15:
	MOVW $1, R0         // <--                                  // mov	w0, #1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+16(FP) // <--
	RET                 // <--                                  // ret

TEXT ·IndexMask(SB), NOSPLIT, $0-32
	MOVD data+0(FP), R0
	MOVD length+8(FP), R1
	MOVB mask+16(FP), R2
	NOP                   // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP  $16, R1          // <--                                  // cmp	x1, #16
	NOP                   // (skipped)                            // mov	x29, sp
	BCC  LBB2_10          // <--                                  // b.lo	.LBB2_10
	ADD  R1, R0, R8       // <--                                  // add	x8, x0, x1
	AND  $63, R1, R11     // <--                                  // and	x11, x1, #0x3f
	SUB  R11, R8, R12     // <--                                  // sub	x12, x8, x11
	MOVD R0, R8           // <--                                  // mov	x8, x0
	VDUP R2, V0.B16       // <--                                  // dup	v0.16b, w2
	CMP  R0, R12          // <--                                  // cmp	x12, x0
	BLS  LBB2_11          // <--                                  // b.ls	.LBB2_11
	MOVD ZR, R9           // <--                                  // mov	x9, xzr
	MOVW $16, R10         // <--                                  // mov	w10, #16
	JMP  LBB2_4           // <--                                  // b	.LBB2_4

LBB2_3:
	ADD $64, R9, R9 // <--                                  // add	x9, x9, #64
	ADD R9, R0, R8  // <--                                  // add	x8, x0, x9
	CMP R12, R8     // <--                                  // cmp	x8, x12
	BCS LBB2_11     // <--                                  // b.hs	.LBB2_11

LBB2_4:
	ADD    R9, R0, R8                             // <--                                  // add	x8, x0, x9
	VLD1   (R8), [V1.B16, V2.B16, V3.B16, V4.B16] // <--                                  // ld1	{ v1.16b, v2.16b, v3.16b, v4.16b }, [x8]
	VORR   V1.B16, V2.B16, V5.B16                 // <--                                  // orr	v5.16b, v2.16b, v1.16b
	VORR   V4.B16, V3.B16, V6.B16                 // <--                                  // orr	v6.16b, v3.16b, v4.16b
	VORR   V6.B16, V5.B16, V5.B16                 // <--                                  // orr	v5.16b, v5.16b, v6.16b
	VCMTST V0.B16, V5.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v5.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R8                                 // <--                                  // fmov	x8, d5
	CBZ    R8, LBB2_3                             // <--                                  // cbz	x8, .LBB2_3
	VCMTST V0.B16, V1.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v1.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R8                                 // <--                                  // fmov	x8, d5
	CBNZ   R8, LBB2_34                            // <--                                  // cbnz	x8, .LBB2_34
	VCMTST V0.B16, V2.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v2.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R8                                 // <--                                  // fmov	x8, d5
	CBNZ   R8, LBB2_36                            // <--                                  // cbnz	x8, .LBB2_36
	VCMTST V0.B16, V3.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v3.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R8                                 // <--                                  // fmov	x8, d5
	CBNZ   R8, LBB2_35                            // <--                                  // cbnz	x8, .LBB2_35
	VCMTST V0.B16, V4.B16, V1.B16                 // <--                                  // cmtst	v1.16b, v4.16b, v0.16b
	WORD   $0x0f0c8421                            // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD  F1, R8                                 // <--                                  // fmov	x8, d1
	CBZ    R8, LBB2_3                             // <--                                  // cbz	x8, .LBB2_3
	MOVW   $48, R10                               // <--                                  // mov	w10, #48
	JMP    LBB2_36                                // <--                                  // b	.LBB2_36

LBB2_10:
	MOVD R0, R8  // <--                                  // mov	x8, x0
	JMP  LBB2_15 // <--                                  // b	.LBB2_15

LBB2_11:
	ADD R11, R8, R9 // <--                                  // add	x9, x8, x11
	AND $15, R1, R1 // <--                                  // and	x1, x1, #0xf
	SUB R1, R9, R9  // <--                                  // sub	x9, x9, x1
	CMP R9, R8      // <--                                  // cmp	x8, x9
	BCS LBB2_15     // <--                                  // b.hs	.LBB2_15
	SUB R0, R8, R10 // <--                                  // sub	x10, x8, x0

LBB2_13:
	WORD   $0x3dc00101            // FMOVQ (R8), F1                       // ldr	q1, [x8]
	VCMTST V0.B16, V1.B16, V1.B16 // <--                                  // cmtst	v1.16b, v1.16b, v0.16b
	WORD   $0x0f0c8421            // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD  F1, R11                // <--                                  // fmov	x11, d1
	CBNZ   R11, LBB2_33           // <--                                  // cbnz	x11, .LBB2_33
	ADD    $16, R8, R8            // <--                                  // add	x8, x8, #16
	ADD    $16, R10, R10          // <--                                  // add	x10, x10, #16
	CMP    R9, R8                 // <--                                  // cmp	x8, x9
	BCC    LBB2_13                // <--                                  // b.lo	.LBB2_13

LBB2_15:
	ANDW $255, R2, R9    // <--                                  // and	w9, w2, #0xff
	MOVW $16843009, R10  // <--                                  // mov	w10, #16843009
	MULW R10, R9, R9     // <--                                  // mul	w9, w9, w10
	SUBS $8, R1, R10     // <--                                  // subs	x10, x1, #8
	BCC  LBB2_19         // <--                                  // b.lo	.LBB2_19
	WORD $0xf940010b     // MOVD (R8), R11                       // ldr	x11, [x8]
	ORR  R9<<32, R9, R12 // <--                                  // orr	x12, x9, x9, lsl #32
	ANDS R12, R11, R11   // <--                                  // ands	x11, x11, x12
	BEQ  LBB2_18         // <--                                  // b.eq	.LBB2_18
	RBIT R11, R9         // <--                                  // rbit	x9, x11
	SUB  R0, R8, R8      // <--                                  // sub	x8, x8, x0
	CLZ  R9, R9          // <--                                  // clz	x9, x9
	ADD  R9>>3, R8, R0   // <--                                  // add	x0, x8, x9, lsr #3
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP)  // <--
	RET                  // <--                                  // ret

LBB2_18:
	ADD  $8, R8, R8 // <--                                  // add	x8, x8, #8
	MOVD R10, R1    // <--                                  // mov	x1, x10

LBB2_19:
	SUBS  $4, R1, R10  // <--                                  // subs	x10, x1, #4
	BCC   LBB2_23      // <--                                  // b.lo	.LBB2_23
	WORD  $0xb940010b  // MOVWU (R8), R11                      // ldr	w11, [x8]
	ANDSW R9, R11, R11 // <--                                  // ands	w11, w11, w9
	BEQ   LBB2_22      // <--                                  // b.eq	.LBB2_22
	RBITW R11, R9      // <--                                  // rbit	w9, w11
	JMP   LBB2_28      // <--                                  // b	.LBB2_28

LBB2_22:
	ADD  $4, R8, R8 // <--                                  // add	x8, x8, #4
	MOVD R10, R1    // <--                                  // mov	x1, x10

LBB2_23:
	CMP   $1, R1            // <--                                  // cmp	x1, #1
	BEQ   LBB2_29           // <--                                  // b.eq	.LBB2_29
	CMP   $2, R1            // <--                                  // cmp	x1, #2
	BEQ   LBB2_31           // <--                                  // b.eq	.LBB2_31
	CMP   $3, R1            // <--                                  // cmp	x1, #3
	BNE   LBB2_32           // <--                                  // b.ne	.LBB2_32
	WORD  $0x7940010a       // MOVHU (R8), R10                      // ldrh	w10, [x8]
	WORD  $0x3940090b       // MOVBU 2(R8), R11                     // ldrb	w11, [x8, #2]
	ORRW  R11<<16, R10, R10 // <--                                  // orr	w10, w10, w11, lsl #16
	ANDSW R9, R10, R9       // <--                                  // ands	w9, w10, w9
	BEQ   LBB2_30           // <--                                  // b.eq	.LBB2_30

LBB2_27:
	RBITW R9, R9 // <--                                  // rbit	w9, w9

LBB2_28:
	CLZW R9, R9     // <--                                  // clz	w9, w9
	SUB  R0, R8, R8 // <--                                  // sub	x8, x8, x0
	LSRW $3, R9, R9 // <--                                  // lsr	w9, w9, #3
	JMP  LBB2_37    // <--                                  // b	.LBB2_37

LBB2_29:
	WORD  $0x3940010a // MOVBU (R8), R10                      // ldrb	w10, [x8]
	ANDSW R9, R10, R9 // <--                                  // ands	w9, w10, w9
	BNE   LBB2_27     // <--                                  // b.ne	.LBB2_27

LBB2_30:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

LBB2_31:
	WORD  $0x7940010a // MOVHU (R8), R10                      // ldrh	w10, [x8]
	ANDSW R9, R10, R9 // <--                                  // ands	w9, w10, w9
	BNE   LBB2_27     // <--                                  // b.ne	.LBB2_27
	JMP   LBB2_30     // <--                                  // b	.LBB2_30

LBB2_32:
	MOVW  ZR, R10     // <--                                  // mov	w10, wzr
	ANDSW R9, R10, R9 // <--                                  // ands	w9, w10, w9
	BNE   LBB2_27     // <--                                  // b.ne	.LBB2_27
	JMP   LBB2_30     // <--                                  // b	.LBB2_30

LBB2_33:
	RBIT R11, R8        // <--                                  // rbit	x8, x11
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R10, R0 // <--                                  // add	x0, x10, x8, lsr #2
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

LBB2_34:
	MOVD ZR, R10 // <--                                  // mov	x10, xzr
	JMP  LBB2_36 // <--                                  // b	.LBB2_36

LBB2_35:
	MOVW $32, R10 // <--                                  // mov	w10, #32

LBB2_36:
	RBIT R8, R8         // <--                                  // rbit	x8, x8
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ORR  R8>>2, R10, R8 // <--                                  // orr	x8, x10, x8, lsr #2

LBB2_37:
	ADD  R9, R8, R0     // <--                                  // add	x0, x8, x9
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

TEXT ·EqualFold(SB), NOSPLIT, $0-33
	MOVD a+0(FP), R0
	MOVD a_len+8(FP), R1
	MOVD b+16(FP), R2
	MOVD b_len+24(FP), R3
	CMP  R3, R1                      // <--                                  // cmp	x1, x3
	BNE  LBB3_9                      // <--                                  // b.ne	.LBB3_9
	TBNZ $63, R1, LBB3_9             // <--                                  // tbnz	x1, #63, .LBB3_9
	NOP                              // (skipped)                            // stp	x29, x30, [sp, #-16]!
	MOVD $uppercasingTable<>(SB), R8 // <--                                  // adrp	x8, uppercasingTable
	ADD  $0, R8, R8                  // <--                                  // add	x8, x8, :lo12:uppercasingTable
	ADD  R1, R0, R9                  // <--                                  // add	x9, x0, x1
	NOP                              // (skipped)                            // mov	x29, sp
	VLD1 (R8), [V0.B16, V1.B16]      // <--                                  // ld1	{ v0.16b, v1.16b }, [x8]
	AND  $15, R1, R8                 // <--                                  // and	x8, x1, #0xf
	SUB  R8, R9, R9                  // <--                                  // sub	x9, x9, x8
	CMP  R0, R9                      // <--                                  // cmp	x9, x0
	BLS  LBB3_6                      // <--                                  // b.ls	.LBB3_6
	WORD $0x4f05e402                 // VMOVI $160, V2.B16                   // movi	v2.16b, #160

LBB3_4:
	WORD  $0x3dc00003                      // FMOVQ (R0), F3                       // ldr	q3, [x0]
	WORD  $0x3dc00044                      // FMOVQ (R2), F4                       // ldr	q4, [x2]
	VADD  V2.B16, V3.B16, V3.B16           // <--                                  // add	v3.16b, v3.16b, v2.16b
	VADD  V2.B16, V4.B16, V4.B16           // <--                                  // add	v4.16b, v4.16b, v2.16b
	VTBL  V3.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v3.16b
	VTBL  V4.B16, [V0.B16, V1.B16], V6.B16 // <--                                  // tbl	v6.16b, { v0.16b, v1.16b }, v4.16b
	VSUB  V5.B16, V3.B16, V3.B16           // <--                                  // sub	v3.16b, v3.16b, v5.16b
	VSUB  V6.B16, V4.B16, V4.B16           // <--                                  // sub	v4.16b, v4.16b, v6.16b
	VCMEQ V4.B16, V3.B16, V3.B16           // <--                                  // cmeq	v3.16b, v3.16b, v4.16b
	WORD  $0x0f0c8463                      // VSHRN $4, V3.H8, V3.B8               // shrn	v3.8b, v3.8h, #4
	FMOVD F3, R10                          // <--                                  // fmov	x10, d3
	CMN   $1, R10                          // <--                                  // cmn	x10, #1
	BNE   LBB3_8                           // <--                                  // b.ne	.LBB3_8
	ADD   $16, R0, R0                      // <--                                  // add	x0, x0, #16
	ADD   $16, R2, R2                      // <--                                  // add	x2, x2, #16
	CMP   R9, R0                           // <--                                  // cmp	x0, x9
	BCC   LBB3_4                           // <--                                  // b.lo	.LBB3_4

LBB3_6:
	CMP   $8, R8                         // <--                                  // cmp	x8, #8
	BCC   LBB3_11                        // <--                                  // b.lo	.LBB3_11
	WORD  $0x0f05e403                    // VMOVI $160, V3.B8                    // movi	v3.8b, #160
	WORD  $0xfc408402                    // FMOVD.P 8(R0), F2                    // ldr	d2, [x0], #8
	WORD  $0xfc408444                    // FMOVD.P 8(R2), F4                    // ldr	d4, [x2], #8
	VADD  V3.B8, V2.B8, V2.B8            // <--                                  // add	v2.8b, v2.8b, v3.8b
	VADD  V3.B8, V4.B8, V3.B8            // <--                                  // add	v3.8b, v4.8b, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V5.B8 // <--                                  // tbl	v5.8b, { v0.16b, v1.16b }, v3.8b
	VSUB  V4.B8, V2.B8, V2.B8            // <--                                  // sub	v2.8b, v2.8b, v4.8b
	VSUB  V5.B8, V3.B8, V3.B8            // <--                                  // sub	v3.8b, v3.8b, v5.8b
	VCMEQ V3.B8, V2.B8, V2.B8            // <--                                  // cmeq	v2.8b, v2.8b, v3.8b
	FMOVD F2, R8                         // <--                                  // fmov	x8, d2
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	BEQ   LBB3_10                        // <--                                  // b.eq	.LBB3_10

LBB3_8:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_9:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_10:
	AND $7, R1, R8 // <--                                  // and	x8, x1, #0x7

LBB3_11:
	CBZ  R8, LBB3_17 // <--                                  // cbz	x8, .LBB3_17
	SUBS $4, R8, R11 // <--                                  // subs	x11, x8, #4
	BCC  LBB3_18     // <--                                  // b.lo	.LBB3_18
	WORD $0xb8404409 // MOVWU.P 4(R0), R9                    // ldr	w9, [x0], #4
	WORD $0xb840444a // MOVWU.P 4(R2), R10                   // ldr	w10, [x2], #4
	MOVD R11, R8     // <--                                  // mov	x8, x11
	CMP  $1, R8      // <--                                  // cmp	x8, #1
	BEQ  LBB3_19     // <--                                  // b.eq	.LBB3_19

LBB3_14:
	CMP  $2, R8           // <--                                  // cmp	x8, #2
	BEQ  LBB3_20          // <--                                  // b.eq	.LBB3_20
	CMP  $3, R8           // <--                                  // cmp	x8, #3
	BNE  LBB3_21          // <--                                  // b.ne	.LBB3_21
	WORD $0x79400008      // MOVHU (R0), R8                       // ldrh	w8, [x0]
	LSL  $24, R9, R9      // <--                                  // lsl	x9, x9, #24
	WORD $0x7940004c      // MOVHU (R2), R12                      // ldrh	w12, [x2]
	LSL  $24, R10, R10    // <--                                  // lsl	x10, x10, #24
	WORD $0x3940080b      // MOVBU 2(R0), R11                     // ldrb	w11, [x0, #2]
	WORD $0x3940084d      // MOVBU 2(R2), R13                     // ldrb	w13, [x2, #2]
	ORR  R8<<8, R9, R8    // <--                                  // orr	x8, x9, x8, lsl #8
	ORR  R12<<8, R10, R10 // <--                                  // orr	x10, x10, x12, lsl #8
	ORR  R11, R8, R9      // <--                                  // orr	x9, x8, x11
	ORR  R13, R10, R10    // <--                                  // orr	x10, x10, x13
	JMP  LBB3_21          // <--                                  // b	.LBB3_21

LBB3_17:
	MOVW $1, R0         // <--                                  // mov	w0, #1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_18:
	MOVD ZR, R10 // <--                                  // mov	x10, xzr
	MOVD ZR, R9  // <--                                  // mov	x9, xzr
	CMP  $1, R8  // <--                                  // cmp	x8, #1
	BNE  LBB3_14 // <--                                  // b.ne	.LBB3_14

LBB3_19:
	WORD $0x39400008      // MOVBU (R0), R8                       // ldrb	w8, [x0]
	WORD $0x3940004b      // MOVBU (R2), R11                      // ldrb	w11, [x2]
	ORR  R9<<8, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #8
	ORR  R10<<8, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #8
	JMP  LBB3_21          // <--                                  // b	.LBB3_21

LBB3_20:
	WORD $0x79400008       // MOVHU (R0), R8                       // ldrh	w8, [x0]
	WORD $0x7940004b       // MOVHU (R2), R11                      // ldrh	w11, [x2]
	ORR  R9<<16, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #16
	ORR  R10<<16, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #16

LBB3_21:
	WORD  $0x0f05e402                    // VMOVI $160, V2.B8                    // movi	v2.8b, #160
	FMOVD R9, F3                         // <--                                  // fmov	d3, x9
	FMOVD R10, F4                        // <--                                  // fmov	d4, x10
	VADD  V2.B8, V3.B8, V3.B8            // <--                                  // add	v3.8b, v3.8b, v2.8b
	VADD  V2.B8, V4.B8, V2.B8            // <--                                  // add	v2.8b, v4.8b, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V0.B8 // <--                                  // tbl	v0.8b, { v0.16b, v1.16b }, v2.8b
	VSUB  V4.B8, V3.B8, V1.B8            // <--                                  // sub	v1.8b, v3.8b, v4.8b
	VSUB  V0.B8, V2.B8, V0.B8            // <--                                  // sub	v0.8b, v2.8b, v0.8b
	VCMEQ V0.B8, V1.B8, V0.B8            // <--                                  // cmeq	v0.8b, v1.8b, v0.8b
	FMOVD F0, R8                         // <--                                  // fmov	x8, d0
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	CSETW EQ, R0                         // <--                                  // cset	w0, eq
	NOP                                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+32(FP)                 // <--
	RET                                  // <--                                  // ret

TEXT ·IndexByteFoldNeon(SB), NOSPLIT, $0-40
	MOVD  haystack+0(FP), R0
	MOVD  haystack_len+8(FP), R1
	MOVB  target+16(FP), R2
	MOVD  is_letter+24(FP), R3
	CMP   $1, R1                 // <--                                  // cmp	x1, #1
	BLT   LBB4_7                 // <--                                  // b.lt	.LBB4_7
	NOP                          // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP   $0, R3                 // <--                                  // cmp	x3, #0
	ORRW  $32, R2, R8            // <--                                  // orr	w8, w2, #0x20
	CSELW EQ, R2, R8, R9         // <--                                  // csel	w9, w2, w8, eq
	CSETW NE, R8                 // <--                                  // cset	w8, ne
	NOP                          // (skipped)                            // mov	x29, sp
	VDUP  R9, V0.B16             // <--                                  // dup	v0.16b, w9
	CBZ   R3, LBB4_8             // <--                                  // cbz	x3, .LBB4_8
	LSLW  $5, R8, R8             // <--                                  // lsl	w8, w8, #5
	CMP   $64, R1                // <--                                  // cmp	x1, #64
	VDUP  R8, V1.B16             // <--                                  // dup	v1.16b, w8
	BCS   LBB4_13                // <--                                  // b.hs	.LBB4_13
	MOVD  ZR, R8                 // <--                                  // mov	x8, xzr

LBB4_4:
	ADD   $16, R8, R10           // <--                                  // add	x10, x8, #16
	CMP   R1, R10                // <--                                  // cmp	x10, x1
	BGT   LBB4_21                // <--                                  // b.gt	.LBB4_21
	WORD  $0x3ce86802            // FMOVQ (R0)(R8), F2                   // ldr	q2, [x0, x8]
	MOVD  R10, R8                // <--                                  // mov	x8, x10
	VORR  V1.B16, V2.B16, V2.B16 // <--                                  // orr	v2.16b, v2.16b, v1.16b
	VCMEQ V0.B16, V2.B16, V2.B16 // <--                                  // cmeq	v2.16b, v2.16b, v0.16b
	WORD  $0x0f0c8442            // VSHRN $4, V2.H8, V2.B8               // shrn	v2.8b, v2.8h, #4
	FMOVD F2, R11                // <--                                  // fmov	x11, d2
	CBZ   R11, LBB4_4            // <--                                  // cbz	x11, .LBB4_4
	RBIT  R11, R8                // <--                                  // rbit	x8, x11
	CLZ   R8, R8                 // <--                                  // clz	x8, x8
	ADD   R8>>2, R10, R8         // <--                                  // add	x8, x10, x8, lsr #2
	SUB   $16, R8, R8            // <--                                  // sub	x8, x8, #16
	JMP   LBB4_43                // <--                                  // b	.LBB4_43

LBB4_7:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	MOVD R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB4_8:
	CMP  $64, R1 // <--                                  // cmp	x1, #64
	BCS  LBB4_24 // <--                                  // b.hs	.LBB4_24
	MOVD ZR, R9  // <--                                  // mov	x9, xzr

LBB4_10:
	ADD   $16, R9, R8            // <--                                  // add	x8, x9, #16
	CMP   R1, R8                 // <--                                  // cmp	x8, x1
	BGT   LBB4_32                // <--                                  // b.gt	.LBB4_32
	WORD  $0x3ce96801            // FMOVQ (R0)(R9), F1                   // ldr	q1, [x0, x9]
	MOVD  R8, R9                 // <--                                  // mov	x9, x8
	VCMEQ V0.B16, V1.B16, V1.B16 // <--                                  // cmeq	v1.16b, v1.16b, v0.16b
	WORD  $0x0f0c8421            // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD F1, R10                // <--                                  // fmov	x10, d1
	CBZ   R10, LBB4_10           // <--                                  // cbz	x10, .LBB4_10
	RBIT  R10, R9                // <--                                  // rbit	x9, x10
	CLZ   R9, R9                 // <--                                  // clz	x9, x9
	ADD   R9>>2, R8, R8          // <--                                  // add	x8, x8, x9, lsr #2
	SUB   $16, R8, R8            // <--                                  // sub	x8, x8, #16
	JMP   LBB4_43                // <--                                  // b	.LBB4_43

LBB4_13:
	MOVD ZR, R10 // <--                                  // mov	x10, xzr
	JMP  LBB4_15 // <--                                  // b	.LBB4_15

LBB4_14:
	ADD  $64, R10, R8   // <--                                  // add	x8, x10, #64
	ADD  $128, R10, R11 // <--                                  // add	x11, x10, #128
	MOVD R8, R10        // <--                                  // mov	x10, x8
	CMP  R1, R11        // <--                                  // cmp	x11, x1
	BGT  LBB4_4         // <--                                  // b.gt	.LBB4_4

LBB4_15:
	ADD   R10, R0, R8                            // <--                                  // add	x8, x0, x10
	VLD1  (R8), [V2.B16, V3.B16, V4.B16, V5.B16] // <--                                  // ld1	{ v2.16b, v3.16b, v4.16b, v5.16b }, [x8]
	VORR  V1.B16, V2.B16, V6.B16                 // <--                                  // orr	v6.16b, v2.16b, v1.16b
	VORR  V1.B16, V3.B16, V7.B16                 // <--                                  // orr	v7.16b, v3.16b, v1.16b
	VORR  V1.B16, V4.B16, V16.B16                // <--                                  // orr	v16.16b, v4.16b, v1.16b
	VORR  V1.B16, V5.B16, V2.B16                 // <--                                  // orr	v2.16b, v5.16b, v1.16b
	VCMEQ V0.B16, V6.B16, V5.B16                 // <--                                  // cmeq	v5.16b, v6.16b, v0.16b
	VCMEQ V0.B16, V16.B16, V3.B16                // <--                                  // cmeq	v3.16b, v16.16b, v0.16b
	VCMEQ V0.B16, V2.B16, V2.B16                 // <--                                  // cmeq	v2.16b, v2.16b, v0.16b
	VCMEQ V0.B16, V7.B16, V4.B16                 // <--                                  // cmeq	v4.16b, v7.16b, v0.16b
	VORR  V2.B16, V3.B16, V6.B16                 // <--                                  // orr	v6.16b, v3.16b, v2.16b
	VORR  V5.B16, V4.B16, V7.B16                 // <--                                  // orr	v7.16b, v4.16b, v5.16b
	VORR  V7.B16, V6.B16, V6.B16                 // <--                                  // orr	v6.16b, v6.16b, v7.16b
	WORD  $0x6e30a8c6                            // VUMAXV V6.B16, V6                    // umaxv	b6, v6.16b
	FMOVS F6, R8                                 // <--                                  // fmov	w8, s6
	CBZW  R8, LBB4_14                            // <--                                  // cbz	w8, .LBB4_14
	WORD  $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD F5, R8                                 // <--                                  // fmov	x8, d5
	CBNZ  R8, LBB4_36                            // <--                                  // cbnz	x8, .LBB4_36
	WORD  $0x0f0c8484                            // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD F4, R8                                 // <--                                  // fmov	x8, d4
	CBNZ  R8, LBB4_37                            // <--                                  // cbnz	x8, .LBB4_37
	WORD  $0x0f0c8463                            // VSHRN $4, V3.H8, V3.B8               // shrn	v3.8b, v3.8h, #4
	FMOVD F3, R8                                 // <--                                  // fmov	x8, d3
	CBNZ  R8, LBB4_38                            // <--                                  // cbnz	x8, .LBB4_38
	WORD  $0x0f0c8442                            // VSHRN $4, V2.H8, V2.B8               // shrn	v2.8b, v2.8h, #4
	FMOVD F2, R8                                 // <--                                  // fmov	x8, d2
	CBZ   R8, LBB4_14                            // <--                                  // cbz	x8, .LBB4_14
	RBIT  R8, R8                                 // <--                                  // rbit	x8, x8
	CLZ   R8, R8                                 // <--                                  // clz	x8, x8
	ADD   R8>>2, R10, R8                         // <--                                  // add	x8, x10, x8, lsr #2
	ADD   $48, R8, R8                            // <--                                  // add	x8, x8, #48
	JMP   LBB4_43                                // <--                                  // b	.LBB4_43

LBB4_21:
	CMP R1, R8  // <--                                  // cmp	x8, x1
	BGE LBB4_35 // <--                                  // b.ge	.LBB4_35

LBB4_22:
	WORD $0x3868680a   // MOVBU (R0)(R8), R10                  // ldrb	w10, [x0, x8]
	ORRW $32, R10, R10 // <--                                  // orr	w10, w10, #0x20
	CMPW R9.UXTB, R10  // <--                                  // cmp	w10, w9, uxtb
	BEQ  LBB4_43       // <--                                  // b.eq	.LBB4_43
	ADD  $1, R8, R8    // <--                                  // add	x8, x8, #1
	CMP  R1, R8        // <--                                  // cmp	x8, x1
	BLT  LBB4_22       // <--                                  // b.lt	.LBB4_22
	JMP  LBB4_35       // <--                                  // b	.LBB4_35

LBB4_24:
	MOVD ZR, R8  // <--                                  // mov	x8, xzr
	JMP  LBB4_26 // <--                                  // b	.LBB4_26

LBB4_25:
	ADD  $64, R8, R9   // <--                                  // add	x9, x8, #64
	ADD  $128, R8, R10 // <--                                  // add	x10, x8, #128
	MOVD R9, R8        // <--                                  // mov	x8, x9
	CMP  R1, R10       // <--                                  // cmp	x10, x1
	BGT  LBB4_10       // <--                                  // b.gt	.LBB4_10

LBB4_26:
	ADD   R8, R0, R9                                 // <--                                  // add	x9, x0, x8
	VLD1  (R9), [V16.B16, V17.B16, V18.B16, V19.B16] // <--                                  // ld1	{ v16.16b, v17.16b, v18.16b, v19.16b }, [x9]
	VCMEQ V0.B16, V16.B16, V4.B16                    // <--                                  // cmeq	v4.16b, v16.16b, v0.16b
	VCMEQ V0.B16, V18.B16, V2.B16                    // <--                                  // cmeq	v2.16b, v18.16b, v0.16b
	VCMEQ V0.B16, V19.B16, V1.B16                    // <--                                  // cmeq	v1.16b, v19.16b, v0.16b
	VCMEQ V0.B16, V17.B16, V3.B16                    // <--                                  // cmeq	v3.16b, v17.16b, v0.16b
	VORR  V1.B16, V2.B16, V5.B16                     // <--                                  // orr	v5.16b, v2.16b, v1.16b
	VORR  V4.B16, V3.B16, V6.B16                     // <--                                  // orr	v6.16b, v3.16b, v4.16b
	VORR  V6.B16, V5.B16, V5.B16                     // <--                                  // orr	v5.16b, v5.16b, v6.16b
	WORD  $0x6e30a8a5                                // VUMAXV V5.B16, V5                    // umaxv	b5, v5.16b
	FMOVS F5, R9                                     // <--                                  // fmov	w9, s5
	CBZW  R9, LBB4_25                                // <--                                  // cbz	w9, .LBB4_25
	WORD  $0x0f0c8484                                // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD F4, R9                                     // <--                                  // fmov	x9, d4
	CBNZ  R9, LBB4_39                                // <--                                  // cbnz	x9, .LBB4_39
	WORD  $0x0f0c8463                                // VSHRN $4, V3.H8, V3.B8               // shrn	v3.8b, v3.8h, #4
	FMOVD F3, R9                                     // <--                                  // fmov	x9, d3
	CBNZ  R9, LBB4_40                                // <--                                  // cbnz	x9, .LBB4_40
	WORD  $0x0f0c8442                                // VSHRN $4, V2.H8, V2.B8               // shrn	v2.8b, v2.8h, #4
	FMOVD F2, R9                                     // <--                                  // fmov	x9, d2
	CBNZ  R9, LBB4_41                                // <--                                  // cbnz	x9, .LBB4_41
	WORD  $0x0f0c8421                                // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD F1, R9                                     // <--                                  // fmov	x9, d1
	CBZ   R9, LBB4_25                                // <--                                  // cbz	x9, .LBB4_25
	RBIT  R9, R9                                     // <--                                  // rbit	x9, x9
	CLZ   R9, R9                                     // <--                                  // clz	x9, x9
	ADD   R9>>2, R8, R8                              // <--                                  // add	x8, x8, x9, lsr #2
	ADD   $48, R8, R8                                // <--                                  // add	x8, x8, #48
	JMP   LBB4_43                                    // <--                                  // b	.LBB4_43

LBB4_32:
	CMP R1, R9  // <--                                  // cmp	x9, x1
	BGE LBB4_35 // <--                                  // b.ge	.LBB4_35

LBB4_33:
	WORD $0x38696808 // MOVBU (R0)(R9), R8                   // ldrb	w8, [x0, x9]
	CMPW R2.UXTB, R8 // <--                                  // cmp	w8, w2, uxtb
	BEQ  LBB4_42     // <--                                  // b.eq	.LBB4_42
	ADD  $1, R9, R9  // <--                                  // add	x9, x9, #1
	MOVD $-1, R8     // <--                                  // mov	x8, #-1
	CMP  R1, R9      // <--                                  // cmp	x9, x1
	BLT  LBB4_33     // <--                                  // b.lt	.LBB4_33
	JMP  LBB4_43     // <--                                  // b	.LBB4_43

LBB4_35:
	MOVD $-1, R8 // <--                                  // mov	x8, #-1
	JMP  LBB4_43 // <--                                  // b	.LBB4_43

LBB4_36:
	RBIT R8, R8         // <--                                  // rbit	x8, x8
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R10, R8 // <--                                  // add	x8, x10, x8, lsr #2
	JMP  LBB4_43        // <--                                  // b	.LBB4_43

LBB4_37:
	RBIT R8, R8         // <--                                  // rbit	x8, x8
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R10, R8 // <--                                  // add	x8, x10, x8, lsr #2
	ADD  $16, R8, R8    // <--                                  // add	x8, x8, #16
	JMP  LBB4_43        // <--                                  // b	.LBB4_43

LBB4_38:
	RBIT R8, R8         // <--                                  // rbit	x8, x8
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R10, R8 // <--                                  // add	x8, x10, x8, lsr #2
	ADD  $32, R8, R8    // <--                                  // add	x8, x8, #32
	JMP  LBB4_43        // <--                                  // b	.LBB4_43

LBB4_39:
	RBIT R9, R9        // <--                                  // rbit	x9, x9
	CLZ  R9, R9        // <--                                  // clz	x9, x9
	ADD  R9>>2, R8, R8 // <--                                  // add	x8, x8, x9, lsr #2
	JMP  LBB4_43       // <--                                  // b	.LBB4_43

LBB4_40:
	RBIT R9, R9        // <--                                  // rbit	x9, x9
	CLZ  R9, R9        // <--                                  // clz	x9, x9
	ADD  R9>>2, R8, R8 // <--                                  // add	x8, x8, x9, lsr #2
	ADD  $16, R8, R8   // <--                                  // add	x8, x8, #16
	JMP  LBB4_43       // <--                                  // b	.LBB4_43

LBB4_41:
	RBIT R9, R9        // <--                                  // rbit	x9, x9
	CLZ  R9, R9        // <--                                  // clz	x9, x9
	ADD  R9>>2, R8, R8 // <--                                  // add	x8, x8, x9, lsr #2
	ADD  $32, R8, R8   // <--                                  // add	x8, x8, #32
	JMP  LBB4_43       // <--                                  // b	.LBB4_43

LBB4_42:
	MOVD R9, R8 // <--                                  // mov	x8, x9

LBB4_43:
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

DATA uppercasingTable<>+0x00(SB)/8, $0x2020202020202000
DATA uppercasingTable<>+0x08(SB)/8, $0x2020202020202020
DATA uppercasingTable<>+0x10(SB)/8, $0x2020202020202020
DATA uppercasingTable<>+0x18(SB)/8, $0x0000000000202020
GLOBL uppercasingTable<>(SB), (RODATA|NOPTR), $32

TEXT ·IndexTwoBytesFoldNeon(SB), NOSPLIT, $0-72
	MOVD  haystack+0(FP), R0
	MOVD  haystack_len+8(FP), R1
	MOVB  rare1+16(FP), R2
	MOVD  off1+24(FP), R3
	MOVD  is_letter1+32(FP), R4
	MOVB  rare2+40(FP), R5
	MOVD  off2+48(FP), R6
	MOVD  is_letter2+56(FP), R7
	CMP   $1, R1                 // <--                                  // cmp	x1, #1
	BLT   LBB5_7                 // <--                                  // b.lt	.LBB5_7
	NOP                          // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP   $0, R4                 // <--                                  // cmp	x4, #0
	ORRW  $32, R2, R8            // <--                                  // orr	w8, w2, #0x20
	CSETW NE, R10                // <--                                  // cset	w10, ne
	CSELW EQ, R2, R8, R9         // <--                                  // csel	w9, w2, w8, eq
	CMP   $0, R7                 // <--                                  // cmp	x7, #0
	LSLW  $5, R10, R8            // <--                                  // lsl	w8, w10, #5
	CSETW NE, R10                // <--                                  // cset	w10, ne
	NOP                          // (skipped)                            // mov	x29, sp
	LSLW  $5, R10, R11           // <--                                  // lsl	w11, w10, #5
	ORRW  $32, R5, R10           // <--                                  // orr	w10, w5, #0x20
	CSELW EQ, R5, R10, R10       // <--                                  // csel	w10, w5, w10, eq
	VDUP  R9, V0.B16             // <--                                  // dup	v0.16b, w9
	VDUP  R8, V1.B16             // <--                                  // dup	v1.16b, w8
	CMP   $64, R1                // <--                                  // cmp	x1, #64
	VDUP  R11, V2.B16            // <--                                  // dup	v2.16b, w11
	VDUP  R10, V3.B16            // <--                                  // dup	v3.16b, w10
	BCS   LBB5_8                 // <--                                  // b.hs	.LBB5_8
	MOVD  ZR, R8                 // <--                                  // mov	x8, xzr

LBB5_3:
	ADD R6, R0, R11 // <--                                  // add	x11, x0, x6
	ADD R3, R0, R12 // <--                                  // add	x12, x0, x3

LBB5_4:
	ADD   $16, R8, R13           // <--                                  // add	x13, x8, #16
	CMP   R1, R13                // <--                                  // cmp	x13, x1
	BGT   LBB5_16                // <--                                  // b.gt	.LBB5_16
	WORD  $0x3ce86984            // FMOVQ (R12)(R8), F4                  // ldr	q4, [x12, x8]
	WORD  $0x3ce86965            // FMOVQ (R11)(R8), F5                  // ldr	q5, [x11, x8]
	MOVD  R13, R8                // <--                                  // mov	x8, x13
	VORR  V1.B16, V4.B16, V4.B16 // <--                                  // orr	v4.16b, v4.16b, v1.16b
	VORR  V2.B16, V5.B16, V5.B16 // <--                                  // orr	v5.16b, v5.16b, v2.16b
	VCMEQ V0.B16, V4.B16, V4.B16 // <--                                  // cmeq	v4.16b, v4.16b, v0.16b
	VCMEQ V3.B16, V5.B16, V5.B16 // <--                                  // cmeq	v5.16b, v5.16b, v3.16b
	VAND  V4.B16, V5.B16, V4.B16 // <--                                  // and	v4.16b, v5.16b, v4.16b
	WORD  $0x0f0c8484            // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD F4, R14                // <--                                  // fmov	x14, d4
	CBZ   R14, LBB5_4            // <--                                  // cbz	x14, .LBB5_4
	RBIT  R14, R8                // <--                                  // rbit	x8, x14
	CLZ   R8, R8                 // <--                                  // clz	x8, x8
	ADD   R8>>2, R13, R8         // <--                                  // add	x8, x13, x8, lsr #2
	SUB   $16, R8, R8            // <--                                  // sub	x8, x8, #16
	JMP   LBB5_25                // <--                                  // b	.LBB5_25

LBB5_7:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret

LBB5_8:
	MOVD ZR, R11     // <--                                  // mov	x11, xzr
	ADD  R6, R0, R12 // <--                                  // add	x12, x0, x6
	ADD  R3, R0, R13 // <--                                  // add	x13, x0, x3
	JMP  LBB5_10     // <--                                  // b	.LBB5_10

LBB5_9:
	ADD  $64, R11, R8   // <--                                  // add	x8, x11, #64
	ADD  $128, R11, R14 // <--                                  // add	x14, x11, #128
	MOVD R8, R11        // <--                                  // mov	x11, x8
	CMP  R1, R14        // <--                                  // cmp	x14, x1
	BGT  LBB5_3         // <--                                  // b.gt	.LBB5_3

LBB5_10:
	ADD   R11, R13, R8                               // <--                                  // add	x8, x13, x11
	VLD1  (R8), [V4.B16, V5.B16, V6.B16, V7.B16]     // <--                                  // ld1	{ v4.16b, v5.16b, v6.16b, v7.16b }, [x8]
	ADD   R11, R12, R8                               // <--                                  // add	x8, x12, x11
	VLD1  (R8), [V16.B16, V17.B16, V18.B16, V19.B16] // <--                                  // ld1	{ v16.16b, v17.16b, v18.16b, v19.16b }, [x8]
	VORR  V1.B16, V4.B16, V20.B16                    // <--                                  // orr	v20.16b, v4.16b, v1.16b
	VORR  V1.B16, V5.B16, V22.B16                    // <--                                  // orr	v22.16b, v5.16b, v1.16b
	VORR  V1.B16, V6.B16, V23.B16                    // <--                                  // orr	v23.16b, v6.16b, v1.16b
	VORR  V1.B16, V7.B16, V4.B16                     // <--                                  // orr	v4.16b, v7.16b, v1.16b
	VCMEQ V0.B16, V4.B16, V4.B16                     // <--                                  // cmeq	v4.16b, v4.16b, v0.16b
	VORR  V2.B16, V16.B16, V21.B16                   // <--                                  // orr	v21.16b, v16.16b, v2.16b
	VORR  V2.B16, V17.B16, V5.B16                    // <--                                  // orr	v5.16b, v17.16b, v2.16b
	VORR  V2.B16, V18.B16, V6.B16                    // <--                                  // orr	v6.16b, v18.16b, v2.16b
	VORR  V2.B16, V19.B16, V7.B16                    // <--                                  // orr	v7.16b, v19.16b, v2.16b
	VCMEQ V0.B16, V20.B16, V16.B16                   // <--                                  // cmeq	v16.16b, v20.16b, v0.16b
	VCMEQ V3.B16, V21.B16, V17.B16                   // <--                                  // cmeq	v17.16b, v21.16b, v3.16b
	VCMEQ V0.B16, V22.B16, V18.B16                   // <--                                  // cmeq	v18.16b, v22.16b, v0.16b
	VCMEQ V3.B16, V5.B16, V5.B16                     // <--                                  // cmeq	v5.16b, v5.16b, v3.16b
	VCMEQ V0.B16, V23.B16, V19.B16                   // <--                                  // cmeq	v19.16b, v23.16b, v0.16b
	VCMEQ V3.B16, V6.B16, V20.B16                    // <--                                  // cmeq	v20.16b, v6.16b, v3.16b
	VCMEQ V3.B16, V7.B16, V21.B16                    // <--                                  // cmeq	v21.16b, v7.16b, v3.16b
	VAND  V16.B16, V17.B16, V7.B16                   // <--                                  // and	v7.16b, v17.16b, v16.16b
	VAND  V18.B16, V5.B16, V6.B16                    // <--                                  // and	v6.16b, v5.16b, v18.16b
	VAND  V19.B16, V20.B16, V5.B16                   // <--                                  // and	v5.16b, v20.16b, v19.16b
	VAND  V4.B16, V21.B16, V4.B16                    // <--                                  // and	v4.16b, v21.16b, v4.16b
	VORR  V4.B16, V5.B16, V16.B16                    // <--                                  // orr	v16.16b, v5.16b, v4.16b
	VORR  V7.B16, V6.B16, V17.B16                    // <--                                  // orr	v17.16b, v6.16b, v7.16b
	VORR  V17.B16, V16.B16, V16.B16                  // <--                                  // orr	v16.16b, v16.16b, v17.16b
	WORD  $0x6e30aa10                                // VUMAXV V16.B16, V16                  // umaxv	b16, v16.16b
	FMOVS F16, R8                                    // <--                                  // fmov	w8, s16
	CBZW  R8, LBB5_9                                 // <--                                  // cbz	w8, .LBB5_9
	WORD  $0x0f0c84e7                                // VSHRN $4, V7.H8, V7.B8               // shrn	v7.8b, v7.8h, #4
	FMOVD F7, R8                                     // <--                                  // fmov	x8, d7
	CBNZ  R8, LBB5_22                                // <--                                  // cbnz	x8, .LBB5_22
	WORD  $0x0f0c84c6                                // VSHRN $4, V6.H8, V6.B8               // shrn	v6.8b, v6.8h, #4
	FMOVD F6, R8                                     // <--                                  // fmov	x8, d6
	CBNZ  R8, LBB5_23                                // <--                                  // cbnz	x8, .LBB5_23
	WORD  $0x0f0c84a5                                // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD F5, R8                                     // <--                                  // fmov	x8, d5
	CBNZ  R8, LBB5_24                                // <--                                  // cbnz	x8, .LBB5_24
	WORD  $0x0f0c8484                                // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD F4, R8                                     // <--                                  // fmov	x8, d4
	CBZ   R8, LBB5_9                                 // <--                                  // cbz	x8, .LBB5_9
	RBIT  R8, R8                                     // <--                                  // rbit	x8, x8
	CLZ   R8, R8                                     // <--                                  // clz	x8, x8
	ADD   R8>>2, R11, R8                             // <--                                  // add	x8, x11, x8, lsr #2
	ADD   $48, R8, R8                                // <--                                  // add	x8, x8, #48
	JMP   LBB5_25                                    // <--                                  // b	.LBB5_25

LBB5_16:
	CMP R1, R8      // <--                                  // cmp	x8, x1
	BGE LBB5_21     // <--                                  // b.ge	.LBB5_21
	ADD R6, R0, R11 // <--                                  // add	x11, x0, x6
	ADD R3, R0, R12 // <--                                  // add	x12, x0, x3
	JMP LBB5_19     // <--                                  // b	.LBB5_19

LBB5_18:
	ADD $1, R8, R8 // <--                                  // add	x8, x8, #1
	CMP R1, R8     // <--                                  // cmp	x8, x1
	BGE LBB5_21    // <--                                  // b.ge	.LBB5_21

LBB5_19:
	WORD  $0x3868698d       // MOVBU (R12)(R8), R13                 // ldrb	w13, [x12, x8]
	CMP   $0, R4            // <--                                  // cmp	x4, #0
	WORD  $0x3868696e       // MOVBU (R11)(R8), R14                 // ldrb	w14, [x11, x8]
	ORRW  $32, R13, R15     // <--                                  // orr	w15, w13, #0x20
	CSELW EQ, R13, R15, R13 // <--                                  // csel	w13, w13, w15, eq
	ORRW  $32, R14, R15     // <--                                  // orr	w15, w14, #0x20
	CMP   $0, R7            // <--                                  // cmp	x7, #0
	ANDW  $255, R13, R16    // <--                                  // and	w16, w13, #0xff
	CSELW EQ, R14, R15, R13 // <--                                  // csel	w13, w14, w15, eq
	CMPW  R9.UXTB, R16      // <--                                  // cmp	w16, w9, uxtb
	BNE   LBB5_18           // <--                                  // b.ne	.LBB5_18
	ANDW  $255, R13, R13    // <--                                  // and	w13, w13, #0xff
	CMPW  R10.UXTB, R13     // <--                                  // cmp	w13, w10, uxtb
	BNE   LBB5_18           // <--                                  // b.ne	.LBB5_18
	JMP   LBB5_25           // <--                                  // b	.LBB5_25

LBB5_21:
	MOVD $-1, R8 // <--                                  // mov	x8, #-1
	JMP  LBB5_25 // <--                                  // b	.LBB5_25

LBB5_22:
	RBIT R8, R8         // <--                                  // rbit	x8, x8
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R11, R8 // <--                                  // add	x8, x11, x8, lsr #2
	JMP  LBB5_25        // <--                                  // b	.LBB5_25

LBB5_23:
	RBIT R8, R8         // <--                                  // rbit	x8, x8
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R11, R8 // <--                                  // add	x8, x11, x8, lsr #2
	ADD  $16, R8, R8    // <--                                  // add	x8, x8, #16
	JMP  LBB5_25        // <--                                  // b	.LBB5_25

LBB5_24:
	RBIT R8, R8         // <--                                  // rbit	x8, x8
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R11, R8 // <--                                  // add	x8, x11, x8, lsr #2
	ADD  $32, R8, R8    // <--                                  // add	x8, x8, #32

LBB5_25:
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret
