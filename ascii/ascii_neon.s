//go:build !noasm && arm64
// Code generated by gocc v0.16.5-rev-57c4d32 -- DO NOT EDIT.
//
// Source file         : ascii_neon.c
// Clang version       : Apple clang version 16.0.0 (clang-1600.0.26.6)
// Target architecture : arm64
// Compiler options    : [none]

#include "textflag.h"

DATA LCPI0_0<>+0x00(SB)/8, $0x8040201008040201
DATA LCPI0_0<>+0x08(SB)/8, $0x8040201008040201
GLOBL LCPI0_0<>(SB), (RODATA|NOPTR), $16

TEXT ·indexAnyNeonBitset(SB), NOSPLIT, $0-56
	MOVD  data+0(FP), R0
	MOVD  data_len+8(FP), R1
	MOVD  bitset0+16(FP), R2
	MOVD  bitset1+24(FP), R3
	MOVD  bitset2+32(FP), R4
	MOVD  bitset3+40(FP), R5
	CBZ   R1, LBB0_13        // <--                                  // cbz	x1, .LBB0_13
	NOP                      // (skipped)                            // stp	x29, x30, [sp, #-16]!
	ADD   R1, R0, R9         // <--                                  // add	x9, x0, x1
	AND   $15, R1, R8        // <--                                  // and	x8, x1, #0xf
	SUB   R8, R9, R10        // <--                                  // sub	x10, x9, x8
	MOVD  R0, R9             // <--                                  // mov	x9, x0
	CMP   R0, R10            // <--                                  // cmp	x10, x0
	NOP                      // (skipped)                            // mov	x29, sp
	BLS   LBB0_5             // <--                                  // b.ls	.LBB0_5
	FMOVD R2, F0             // <--                                  // fmov	d0, x2
	FMOVD R3, F2             // <--                                  // fmov	d2, x3
	MOVD  $LCPI0_0<>(SB), R9 // <--                                  // adrp	x9, .LCPI0_0
	VMOV  V2.D[0], V0.D[1]   // <--                                  // mov	v0.d[1], v2.d[0]
	FMOVD R5, F2             // <--                                  // fmov	d2, x5
	FMOVD R4, F1             // <--                                  // fmov	d1, x4
	WORD  $0x3dc00123        // FMOVQ (R9), F3                       // ldr	q3, [x9, :lo12:.LCPI0_0]
	MOVD  R0, R9             // <--                                  // mov	x9, x0
	VMOV  V2.D[0], V1.D[1]   // <--                                  // mov	v1.d[1], v2.d[0]
	WORD  $0x4f00e4e2        // VMOVI $7, V2.B16                     // movi	v2.16b, #7

LBB0_3:
	WORD   $0x3dc00124                      // FMOVQ (R9), F4                       // ldr	q4, [x9]
	WORD   $0x6f0d0485                      // VUSHR $3, V4.B16, V5.B16             // ushr	v5.16b, v4.16b, #3
	VAND   V2.B16, V4.B16, V4.B16           // <--                                  // and	v4.16b, v4.16b, v2.16b
	VTBL   V5.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v5.16b
	VTBL   V4.B16, [V3.B16], V4.B16         // <--                                  // tbl	v4.16b, { v3.16b }, v4.16b
	VCMTST V5.B16, V4.B16, V4.B16           // <--                                  // cmtst	v4.16b, v4.16b, v5.16b
	WORD   $0x0f0c8484                      // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD  F4, R11                          // <--                                  // fmov	x11, d4
	CBNZ   R11, LBB0_14                     // <--                                  // cbnz	x11, .LBB0_14
	ADD    $16, R9, R9                      // <--                                  // add	x9, x9, #16
	CMP    R10, R9                          // <--                                  // cmp	x9, x10
	BCC    LBB0_3                           // <--                                  // b.lo	.LBB0_3

LBB0_5:
	CBZ  R8, LBB0_12 // <--                                  // cbz	x8, .LBB0_12
	MOVD ZR, R10     // <--                                  // mov	x10, xzr

LBB0_7:
	WORD $0x386a692b     // MOVBU (R9)(R10), R11                 // ldrb	w11, [x9, x10]
	MOVD R2, R12         // <--                                  // mov	x12, x2
	LSRW $6, R11, R13    // <--                                  // lsr	w13, w11, #6
	CBZW R13, LBB0_10    // <--                                  // cbz	w13, .LBB0_10
	MOVD R4, R12         // <--                                  // mov	x12, x4
	CMPW $2, R13         // <--                                  // cmp	w13, #2
	BEQ  LBB0_10         // <--                                  // b.eq	.LBB0_10
	CMPW $1, R13         // <--                                  // cmp	w13, #1
	CSEL NE, R5, R3, R12 // <--                                  // csel	x12, x5, x3, ne

LBB0_10:
	LSR  R11, R12, R11    // <--                                  // lsr	x11, x12, x11
	TBNZ $0, R11, LBB0_15 // <--                                  // tbnz	w11, #0, .LBB0_15
	ADD  $1, R10, R10     // <--                                  // add	x10, x10, #1
	CMP  R10, R8          // <--                                  // cmp	x8, x10
	BNE  LBB0_7           // <--                                  // b.ne	.LBB0_7

LBB0_12:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_13:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_14:
	RBIT R11, R8        // <--                                  // rbit	x8, x11
	SUB  R0, R9, R9     // <--                                  // sub	x9, x9, x0
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R9, R0  // <--                                  // add	x0, x9, x8, lsr #2
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_15:
	SUB  R0, R9, R8     // <--                                  // sub	x8, x9, x0
	ADD  R10, R8, R0    // <--                                  // add	x0, x8, x10
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

TEXT ·ValidString(SB), NOSPLIT, $0-17
	MOVD data+0(FP), R0
	MOVD length+8(FP), R1
	NOP                   // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP  $16, R1          // <--                                  // cmp	x1, #16
	NOP                   // (skipped)                            // mov	x29, sp
	BCC  LBB1_7           // <--                                  // b.lo	.LBB1_7
	ADD  R1, R0, R9       // <--                                  // add	x9, x0, x1
	AND  $63, R1, R8      // <--                                  // and	x8, x1, #0x3f
	SUB  R8, R9, R9       // <--                                  // sub	x9, x9, x8
	CMP  R0, R9           // <--                                  // cmp	x9, x0
	BLS  LBB1_4           // <--                                  // b.ls	.LBB1_4

LBB1_2:
	VLD1  (R0), [V0.B16, V1.B16, V2.B16, V3.B16] // <--                                  // ld1	{ v0.16b, v1.16b, v2.16b, v3.16b }, [x0]
	VORR  V1.B16, V0.B16, V4.B16                 // <--                                  // orr	v4.16b, v0.16b, v1.16b
	VORR  V2.B16, V3.B16, V0.B16                 // <--                                  // orr	v0.16b, v3.16b, v2.16b
	VORR  V0.B16, V4.B16, V0.B16                 // <--                                  // orr	v0.16b, v4.16b, v0.16b
	WORD  $0x4e20a800                            // VCMLT $0, V0.B16, V0.B16             // cmlt	v0.16b, v0.16b, #0
	WORD  $0x0f0c8400                            // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R10                                // <--                                  // fmov	x10, d0
	CBNZ  R10, LBB1_12                           // <--                                  // cbnz	x10, .LBB1_12
	ADD   $64, R0, R0                            // <--                                  // add	x0, x0, #64
	CMP   R9, R0                                 // <--                                  // cmp	x0, x9
	BCC   LBB1_2                                 // <--                                  // b.lo	.LBB1_2

LBB1_4:
	ADD R8, R0, R8  // <--                                  // add	x8, x0, x8
	AND $15, R1, R1 // <--                                  // and	x1, x1, #0xf
	SUB R1, R8, R8  // <--                                  // sub	x8, x8, x1
	CMP R8, R0      // <--                                  // cmp	x0, x8
	BCS LBB1_7      // <--                                  // b.hs	.LBB1_7

LBB1_5:
	WORD  $0x3dc00000 // FMOVQ (R0), F0                       // ldr	q0, [x0]
	WORD  $0x4e20a800 // VCMLT $0, V0.B16, V0.B16             // cmlt	v0.16b, v0.16b, #0
	WORD  $0x0f0c8400 // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R9      // <--                                  // fmov	x9, d0
	CBNZ  R9, LBB1_12 // <--                                  // cbnz	x9, .LBB1_12
	ADD   $16, R0, R0 // <--                                  // add	x0, x0, #16
	CMP   R8, R0      // <--                                  // cmp	x0, x8
	BCC   LBB1_5      // <--                                  // b.lo	.LBB1_5

LBB1_7:
	CMP   $8, R1          // <--                                  // cmp	x1, #8
	BCS   LBB1_11         // <--                                  // b.hs	.LBB1_11
	TBNZ  $2, R1, LBB1_13 // <--                                  // tbnz	w1, #2, .LBB1_13
	CBZ   R1, LBB1_14     // <--                                  // cbz	x1, .LBB1_14
	LSR   $1, R1, R8      // <--                                  // lsr	x8, x1, #1
	ADD   R1, R0, R9      // <--                                  // add	x9, x0, x1
	WORD  $0x3940000a     // MOVBU (R0), R10                      // ldrb	w10, [x0]
	WORD  $0x38686808     // MOVBU (R0)(R8), R8                   // ldrb	w8, [x0, x8]
	WORD  $0x385ff129     // LDURBW -1(R9), R9                    // ldurb	w9, [x9, #-1]
	ORRW  R9, R10, R9     // <--                                  // orr	w9, w10, w9
	ORRW  R9, R8, R8      // <--                                  // orr	w8, w8, w9
	SXTBW R8, R8          // <--                                  // sxtb	w8, w8
	CMPW  $0, R8          // <--                                  // cmp	w8, #0
	CSETW GE, R0          // <--                                  // cset	w0, ge
	NOP                   // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP)  // <--
	RET                   // <--                                  // ret

LBB1_11:
	ADD   R1, R0, R8                // <--                                  // add	x8, x0, x1
	WORD  $0xf9400009               // MOVD (R0), R9                        // ldr	x9, [x0]
	WORD  $0xf85f8108               // MOVD -8(R8), R8                      // ldur	x8, [x8, #-8]
	ORR   R9, R8, R8                // <--                                  // orr	x8, x8, x9
	TST   $-9187201950435737472, R8 // <--                                  // tst	x8, #0x8080808080808080
	CSETW EQ, R0                    // <--                                  // cset	w0, eq
	NOP                             // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP)            // <--
	RET                             // <--                                  // ret

LBB1_12:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+16(FP) // <--
	RET                 // <--                                  // ret

LBB1_13:
	ADD   R1, R0, R8      // <--                                  // add	x8, x0, x1
	WORD  $0xb9400009     // MOVWU (R0), R9                       // ldr	w9, [x0]
	WORD  $0xb85fc108     // MOVWU -4(R8), R8                     // ldur	w8, [x8, #-4]
	ORRW  R9, R8, R8      // <--                                  // orr	w8, w8, w9
	TSTW  $2155905152, R8 // <--                                  // tst	w8, #0x80808080
	CSETW EQ, R0          // <--                                  // cset	w0, eq
	NOP                   // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP)  // <--
	RET                   // <--                                  // ret

LBB1_14:
	MOVW $1, R0         // <--                                  // mov	w0, #1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+16(FP) // <--
	RET                 // <--                                  // ret

TEXT ·IndexMask(SB), NOSPLIT, $0-32
	MOVD data+0(FP), R0
	MOVD length+8(FP), R1
	MOVB mask+16(FP), R2
	NOP                   // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP  $16, R1          // <--                                  // cmp	x1, #16
	NOP                   // (skipped)                            // mov	x29, sp
	BCC  LBB2_10          // <--                                  // b.lo	.LBB2_10
	ADD  R1, R0, R8       // <--                                  // add	x8, x0, x1
	AND  $63, R1, R10     // <--                                  // and	x10, x1, #0x3f
	SUB  R10, R8, R11     // <--                                  // sub	x11, x8, x10
	MOVD R0, R8           // <--                                  // mov	x8, x0
	VDUP R2, V0.B16       // <--                                  // dup	v0.16b, w2
	CMP  R0, R11          // <--                                  // cmp	x11, x0
	BLS  LBB2_14          // <--                                  // b.ls	.LBB2_14
	MOVW $16, R9          // <--                                  // mov	w9, #16
	MOVD R0, R8           // <--                                  // mov	x8, x0
	JMP  LBB2_4           // <--                                  // b	.LBB2_4

LBB2_3:
	ADD $64, R8, R8 // <--                                  // add	x8, x8, #64
	CMP R11, R8     // <--                                  // cmp	x8, x11
	BCS LBB2_14     // <--                                  // b.hs	.LBB2_14

LBB2_4:
	VLD1   (R8), [V1.B16, V2.B16, V3.B16, V4.B16] // <--                                  // ld1	{ v1.16b, v2.16b, v3.16b, v4.16b }, [x8]
	VORR   V1.B16, V2.B16, V5.B16                 // <--                                  // orr	v5.16b, v2.16b, v1.16b
	VORR   V4.B16, V3.B16, V6.B16                 // <--                                  // orr	v6.16b, v3.16b, v4.16b
	VORR   V6.B16, V5.B16, V5.B16                 // <--                                  // orr	v5.16b, v5.16b, v6.16b
	VCMTST V0.B16, V5.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v5.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBZ    R12, LBB2_3                            // <--                                  // cbz	x12, .LBB2_3
	VCMTST V0.B16, V1.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v1.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBNZ   R12, LBB2_32                           // <--                                  // cbnz	x12, .LBB2_32
	VCMTST V0.B16, V2.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v2.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBNZ   R12, LBB2_34                           // <--                                  // cbnz	x12, .LBB2_34
	VCMTST V0.B16, V3.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v3.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBNZ   R12, LBB2_33                           // <--                                  // cbnz	x12, .LBB2_33
	VCMTST V0.B16, V4.B16, V1.B16                 // <--                                  // cmtst	v1.16b, v4.16b, v0.16b
	WORD   $0x0f0c8421                            // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD  F1, R12                                // <--                                  // fmov	x12, d1
	CBZ    R12, LBB2_3                            // <--                                  // cbz	x12, .LBB2_3
	MOVW   $48, R9                                // <--                                  // mov	w9, #48
	JMP    LBB2_34                                // <--                                  // b	.LBB2_34

LBB2_10:
	MOVD R0, R8 // <--                                  // mov	x8, x0

LBB2_11:
	ANDW $255, R2, R9    // <--                                  // and	w9, w2, #0xff
	MOVW $16843009, R10  // <--                                  // mov	w10, #16843009
	MULW R10, R9, R9     // <--                                  // mul	w9, w9, w10
	SUBS $8, R1, R10     // <--                                  // subs	x10, x1, #8
	BCC  LBB2_18         // <--                                  // b.lo	.LBB2_18
	WORD $0xf940010b     // MOVD (R8), R11                       // ldr	x11, [x8]
	ORR  R9<<32, R9, R12 // <--                                  // orr	x12, x9, x9, lsl #32
	ANDS R12, R11, R11   // <--                                  // ands	x11, x11, x12
	BEQ  LBB2_17         // <--                                  // b.eq	.LBB2_17
	RBIT R11, R9         // <--                                  // rbit	x9, x11
	SUB  R0, R8, R8      // <--                                  // sub	x8, x8, x0
	CLZ  R9, R9          // <--                                  // clz	x9, x9
	ADD  R9>>3, R8, R0   // <--                                  // add	x0, x8, x9, lsr #3
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP)  // <--
	RET                  // <--                                  // ret

LBB2_14:
	ADD R10, R8, R9 // <--                                  // add	x9, x8, x10
	AND $15, R1, R1 // <--                                  // and	x1, x1, #0xf
	SUB R1, R9, R9  // <--                                  // sub	x9, x9, x1
	CMP R9, R8      // <--                                  // cmp	x8, x9
	BCS LBB2_11     // <--                                  // b.hs	.LBB2_11

LBB2_15:
	WORD   $0x3dc00101            // FMOVQ (R8), F1                       // ldr	q1, [x8]
	VCMTST V0.B16, V1.B16, V1.B16 // <--                                  // cmtst	v1.16b, v1.16b, v0.16b
	WORD   $0x0f0c8421            // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD  F1, R10                // <--                                  // fmov	x10, d1
	CBNZ   R10, LBB2_31           // <--                                  // cbnz	x10, .LBB2_31
	ADD    $16, R8, R8            // <--                                  // add	x8, x8, #16
	CMP    R9, R8                 // <--                                  // cmp	x8, x9
	BCC    LBB2_15                // <--                                  // b.lo	.LBB2_15
	JMP    LBB2_11                // <--                                  // b	.LBB2_11

LBB2_17:
	ADD  $8, R8, R8 // <--                                  // add	x8, x8, #8
	MOVD R10, R1    // <--                                  // mov	x1, x10

LBB2_18:
	SUBS  $4, R1, R10    // <--                                  // subs	x10, x1, #4
	BCC   LBB2_22        // <--                                  // b.lo	.LBB2_22
	WORD  $0xb940010b    // MOVWU (R8), R11                      // ldr	w11, [x8]
	ANDSW R9, R11, R11   // <--                                  // ands	w11, w11, w9
	BEQ   LBB2_21        // <--                                  // b.eq	.LBB2_21
	RBITW R11, R9        // <--                                  // rbit	w9, w11
	CLZW  R9, R9         // <--                                  // clz	w9, w9
	SUB   R0, R8, R8     // <--                                  // sub	x8, x8, x0
	LSRW  $3, R9, R9     // <--                                  // lsr	w9, w9, #3
	ADD   R9, R8, R0     // <--                                  // add	x0, x8, x9
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD  R0, ret+24(FP) // <--
	RET                  // <--                                  // ret

LBB2_21:
	ADD  $4, R8, R8 // <--                                  // add	x8, x8, #4
	MOVD R10, R1    // <--                                  // mov	x1, x10

LBB2_22:
	CMP   $1, R1            // <--                                  // cmp	x1, #1
	BEQ   LBB2_26           // <--                                  // b.eq	.LBB2_26
	CMP   $2, R1            // <--                                  // cmp	x1, #2
	BEQ   LBB2_27           // <--                                  // b.eq	.LBB2_27
	CMP   $3, R1            // <--                                  // cmp	x1, #3
	BNE   LBB2_29           // <--                                  // b.ne	.LBB2_29
	WORD  $0x7940010a       // MOVHU (R8), R10                      // ldrh	w10, [x8]
	WORD  $0x3940090b       // MOVBU 2(R8), R11                     // ldrb	w11, [x8, #2]
	ORRW  R11<<16, R10, R10 // <--                                  // orr	w10, w10, w11, lsl #16
	ANDSW R9, R10, R9       // <--                                  // ands	w9, w10, w9
	BNE   LBB2_28           // <--                                  // b.ne	.LBB2_28
	JMP   LBB2_30           // <--                                  // b	.LBB2_30

LBB2_26:
	WORD  $0x3940010a // MOVBU (R8), R10                      // ldrb	w10, [x8]
	ANDSW R9, R10, R9 // <--                                  // ands	w9, w10, w9
	BNE   LBB2_28     // <--                                  // b.ne	.LBB2_28
	JMP   LBB2_30     // <--                                  // b	.LBB2_30

LBB2_27:
	WORD  $0x7940010a // MOVHU (R8), R10                      // ldrh	w10, [x8]
	ANDSW R9, R10, R9 // <--                                  // ands	w9, w10, w9
	BEQ   LBB2_30     // <--                                  // b.eq	.LBB2_30

LBB2_28:
	RBITW R9, R9         // <--                                  // rbit	w9, w9
	CLZW  R9, R9         // <--                                  // clz	w9, w9
	SUB   R0, R8, R8     // <--                                  // sub	x8, x8, x0
	LSRW  $3, R9, R9     // <--                                  // lsr	w9, w9, #3
	ADD   R9, R8, R0     // <--                                  // add	x0, x8, x9
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD  R0, ret+24(FP) // <--
	RET                  // <--                                  // ret

LBB2_29:
	MOVW  ZR, R10    // <--                                  // mov	w10, wzr
	ANDSW R9, ZR, R9 // <--                                  // ands	w9, wzr, w9
	BNE   LBB2_28    // <--                                  // b.ne	.LBB2_28

LBB2_30:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

LBB2_31:
	RBIT R10, R9        // <--                                  // rbit	x9, x10
	SUB  R0, R8, R8     // <--                                  // sub	x8, x8, x0
	CLZ  R9, R9         // <--                                  // clz	x9, x9
	ADD  R9>>2, R8, R0  // <--                                  // add	x0, x8, x9, lsr #2
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

LBB2_32:
	MOVD ZR, R9  // <--                                  // mov	x9, xzr
	JMP  LBB2_34 // <--                                  // b	.LBB2_34

LBB2_33:
	MOVW $32, R9 // <--                                  // mov	w9, #32

LBB2_34:
	RBIT R12, R10       // <--                                  // rbit	x10, x12
	SUB  R0, R8, R8     // <--                                  // sub	x8, x8, x0
	CLZ  R10, R10       // <--                                  // clz	x10, x10
	ORR  R10>>2, R9, R9 // <--                                  // orr	x9, x9, x10, lsr #2
	ADD  R9, R8, R0     // <--                                  // add	x0, x8, x9
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

DATA uppercasingTable<>+0x00(SB)/8, $0x2020202020202000
DATA uppercasingTable<>+0x08(SB)/8, $0x2020202020202020
DATA uppercasingTable<>+0x10(SB)/8, $0x2020202020202020
DATA uppercasingTable<>+0x18(SB)/8, $0x0000000000202020
GLOBL uppercasingTable<>(SB), (RODATA|NOPTR), $32

TEXT ·EqualFold(SB), NOSPLIT, $0-33
	MOVD a+0(FP), R0
	MOVD a_len+8(FP), R1
	MOVD b+16(FP), R2
	MOVD b_len+24(FP), R3
	CMP  R3, R1                      // <--                                  // cmp	x1, x3
	BNE  LBB3_9                      // <--                                  // b.ne	.LBB3_9
	TBNZ $63, R1, LBB3_9             // <--                                  // tbnz	x1, #63, .LBB3_9
	NOP                              // (skipped)                            // stp	x29, x30, [sp, #-16]!
	MOVD $uppercasingTable<>(SB), R8 // <--                                  // adrp	x8, uppercasingTable
	NOP                              // (skipped)                            // add	x8, x8, :lo12:uppercasingTable
	ADD  R1, R0, R9                  // <--                                  // add	x9, x0, x1
	NOP                              // (skipped)                            // mov	x29, sp
	VLD1 (R8), [V0.B16, V1.B16]      // <--                                  // ld1	{ v0.16b, v1.16b }, [x8]
	AND  $15, R1, R8                 // <--                                  // and	x8, x1, #0xf
	SUB  R8, R9, R9                  // <--                                  // sub	x9, x9, x8
	CMP  R0, R9                      // <--                                  // cmp	x9, x0
	BLS  LBB3_6                      // <--                                  // b.ls	.LBB3_6
	WORD $0x4f05e402                 // VMOVI $160, V2.B16                   // movi	v2.16b, #160

LBB3_4:
	WORD  $0x3dc00003                      // FMOVQ (R0), F3                       // ldr	q3, [x0]
	WORD  $0x3dc00044                      // FMOVQ (R2), F4                       // ldr	q4, [x2]
	VADD  V2.B16, V3.B16, V3.B16           // <--                                  // add	v3.16b, v3.16b, v2.16b
	VADD  V2.B16, V4.B16, V4.B16           // <--                                  // add	v4.16b, v4.16b, v2.16b
	VTBL  V3.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v3.16b
	VTBL  V4.B16, [V0.B16, V1.B16], V6.B16 // <--                                  // tbl	v6.16b, { v0.16b, v1.16b }, v4.16b
	VSUB  V5.B16, V3.B16, V3.B16           // <--                                  // sub	v3.16b, v3.16b, v5.16b
	VSUB  V6.B16, V4.B16, V4.B16           // <--                                  // sub	v4.16b, v4.16b, v6.16b
	VCMEQ V4.B16, V3.B16, V3.B16           // <--                                  // cmeq	v3.16b, v3.16b, v4.16b
	WORD  $0x0f0c8463                      // VSHRN $4, V3.H8, V3.B8               // shrn	v3.8b, v3.8h, #4
	FMOVD F3, R10                          // <--                                  // fmov	x10, d3
	CMN   $1, R10                          // <--                                  // cmn	x10, #1
	BNE   LBB3_8                           // <--                                  // b.ne	.LBB3_8
	ADD   $16, R0, R0                      // <--                                  // add	x0, x0, #16
	ADD   $16, R2, R2                      // <--                                  // add	x2, x2, #16
	CMP   R9, R0                           // <--                                  // cmp	x0, x9
	BCC   LBB3_4                           // <--                                  // b.lo	.LBB3_4

LBB3_6:
	CMP   $8, R8                         // <--                                  // cmp	x8, #8
	BCC   LBB3_11                        // <--                                  // b.lo	.LBB3_11
	WORD  $0x0f05e403                    // VMOVI $160, V3.B8                    // movi	v3.8b, #160
	WORD  $0xfc408402                    // FMOVD.P 8(R0), F2                    // ldr	d2, [x0], #8
	WORD  $0xfc408444                    // FMOVD.P 8(R2), F4                    // ldr	d4, [x2], #8
	VADD  V3.B8, V2.B8, V2.B8            // <--                                  // add	v2.8b, v2.8b, v3.8b
	VADD  V3.B8, V4.B8, V3.B8            // <--                                  // add	v3.8b, v4.8b, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V5.B8 // <--                                  // tbl	v5.8b, { v0.16b, v1.16b }, v3.8b
	VSUB  V4.B8, V2.B8, V2.B8            // <--                                  // sub	v2.8b, v2.8b, v4.8b
	VSUB  V5.B8, V3.B8, V3.B8            // <--                                  // sub	v3.8b, v3.8b, v5.8b
	VCMEQ V3.B8, V2.B8, V2.B8            // <--                                  // cmeq	v2.8b, v2.8b, v3.8b
	FMOVD F2, R8                         // <--                                  // fmov	x8, d2
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	BEQ   LBB3_10                        // <--                                  // b.eq	.LBB3_10

LBB3_8:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_9:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_10:
	AND $7, R1, R8 // <--                                  // and	x8, x1, #0x7

LBB3_11:
	CBZ  R8, LBB3_17 // <--                                  // cbz	x8, .LBB3_17
	SUBS $4, R8, R11 // <--                                  // subs	x11, x8, #4
	BCC  LBB3_18     // <--                                  // b.lo	.LBB3_18
	WORD $0xb8404409 // MOVWU.P 4(R0), R9                    // ldr	w9, [x0], #4
	WORD $0xb840444a // MOVWU.P 4(R2), R10                   // ldr	w10, [x2], #4
	MOVD R11, R8     // <--                                  // mov	x8, x11
	CMP  $1, R11     // <--                                  // cmp	x11, #1
	BEQ  LBB3_19     // <--                                  // b.eq	.LBB3_19

LBB3_14:
	CMP  $2, R8           // <--                                  // cmp	x8, #2
	BEQ  LBB3_20          // <--                                  // b.eq	.LBB3_20
	CMP  $3, R8           // <--                                  // cmp	x8, #3
	BNE  LBB3_21          // <--                                  // b.ne	.LBB3_21
	WORD $0x79400008      // MOVHU (R0), R8                       // ldrh	w8, [x0]
	LSL  $24, R9, R9      // <--                                  // lsl	x9, x9, #24
	WORD $0x7940004c      // MOVHU (R2), R12                      // ldrh	w12, [x2]
	LSL  $24, R10, R10    // <--                                  // lsl	x10, x10, #24
	WORD $0x3940080b      // MOVBU 2(R0), R11                     // ldrb	w11, [x0, #2]
	WORD $0x3940084d      // MOVBU 2(R2), R13                     // ldrb	w13, [x2, #2]
	ORR  R8<<8, R9, R8    // <--                                  // orr	x8, x9, x8, lsl #8
	ORR  R12<<8, R10, R10 // <--                                  // orr	x10, x10, x12, lsl #8
	ORR  R11, R8, R9      // <--                                  // orr	x9, x8, x11
	ORR  R13, R10, R10    // <--                                  // orr	x10, x10, x13
	JMP  LBB3_21          // <--                                  // b	.LBB3_21

LBB3_17:
	MOVW $1, R0         // <--                                  // mov	w0, #1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_18:
	MOVD ZR, R10 // <--                                  // mov	x10, xzr
	MOVD ZR, R9  // <--                                  // mov	x9, xzr
	CMP  $1, R8  // <--                                  // cmp	x8, #1
	BNE  LBB3_14 // <--                                  // b.ne	.LBB3_14

LBB3_19:
	WORD $0x39400008      // MOVBU (R0), R8                       // ldrb	w8, [x0]
	WORD $0x3940004b      // MOVBU (R2), R11                      // ldrb	w11, [x2]
	ORR  R9<<8, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #8
	ORR  R10<<8, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #8
	JMP  LBB3_21          // <--                                  // b	.LBB3_21

LBB3_20:
	WORD $0x79400008       // MOVHU (R0), R8                       // ldrh	w8, [x0]
	WORD $0x7940004b       // MOVHU (R2), R11                      // ldrh	w11, [x2]
	ORR  R9<<16, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #16
	ORR  R10<<16, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #16

LBB3_21:
	WORD  $0x0f05e402                    // VMOVI $160, V2.B8                    // movi	v2.8b, #160
	FMOVD R9, F3                         // <--                                  // fmov	d3, x9
	FMOVD R10, F4                        // <--                                  // fmov	d4, x10
	VADD  V2.B8, V3.B8, V3.B8            // <--                                  // add	v3.8b, v3.8b, v2.8b
	VADD  V2.B8, V4.B8, V2.B8            // <--                                  // add	v2.8b, v4.8b, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V0.B8 // <--                                  // tbl	v0.8b, { v0.16b, v1.16b }, v2.8b
	VSUB  V4.B8, V3.B8, V1.B8            // <--                                  // sub	v1.8b, v3.8b, v4.8b
	VSUB  V0.B8, V2.B8, V0.B8            // <--                                  // sub	v0.8b, v2.8b, v0.8b
	VCMEQ V0.B8, V1.B8, V0.B8            // <--                                  // cmeq	v0.8b, v1.8b, v0.8b
	FMOVD F0, R8                         // <--                                  // fmov	x8, d0
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	CSETW EQ, R0                         // <--                                  // cset	w0, eq
	NOP                                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+32(FP)                 // <--
	RET                                  // <--                                  // ret

// func indexFoldNEON(haystack string, rare1 byte, off1 int, rare2 byte, off2 int, normNeedle string) int
TEXT ·indexFoldNEON(SB), NOSPLIT, $0-72
	MOVD  haystack+0(FP), R0      // R0 = haystack ptr
	MOVD  haystack_len+8(FP), R1  // R1 = haystack len
	MOVBU rare1+16(FP), R2        // R2 = rare1 byte
	MOVD  off1+24(FP), R3         // R3 = off1
	MOVBU rare2+32(FP), R4        // R4 = rare2 byte (save for 2-byte mode)
	MOVD  off2+40(FP), R5         // R5 = off2 (save for 2-byte mode)
	MOVD  norm_needle+48(FP), R6  // R6 = needle ptr
	MOVD  needle_len+56(FP), R7   // R7 = needle len

	// Early exits
	SUBS  R7, R1, R9              // R9 = searchLen = haystack_len - needle_len
	BLT   not_found
	CBZ   R7, found_zero

	// Compute mask and target for rare1
	// With lowercase normalization, rare bytes are already lowercase (a-z).
	// For letters: OR haystack with 0x20 to force lowercase, compare to lowercase target
	// For non-letters: mask with 0xFF (exact match)
	// Using OR 0x20 saves instructions vs AND 0xDF (0x20 is valid immediate, 0xDF is not)
	SUBW  $97, R2, R10            // R10 = rare1 - 'a'
	CMPW  $26, R10
	BCS   not_letter1
	MOVW  $0x20, R26              // mask = 0x20 (OR to force lowercase)
	MOVW  R2, R27                 // target = rare1 (already lowercase)
	B     setup_rare1
not_letter1:
	MOVW  $0x00, R26              // mask = 0x00 (OR with 0 = no change)
	MOVW  R2, R27                 // target = byte itself
setup_rare1:
	VDUP  R26, V0.B16             // V0 = rare1 mask (broadcast)
	VDUP  R27, V1.B16             // V1 = rare1 target (broadcast)

	// Magic constant for syndrome extraction
	MOVD  $0x4010040140100401, R10
	VMOV  R10, V5.D[0]
	VMOV  R10, V5.D[1]

	// Setup constants for vectorized verification (used by both 1-byte and 2-byte modes)
	// V4 = 159 (-97 as unsigned), V7 = 26, V8 = 32 (0x20)
	WORD  $0x4f04e7e4             // VMOVI $159, V4.B16
	WORD  $0x4f00e747             // VMOVI $26, V7.B16
	WORD  $0x4f01e408             // VMOVI $32, V8.B16
	MOVD  $tail_mask_table<>(SB), R24  // R24 = tail mask table (callee-saved, never clobbered)

	// Setup pointers
	ADD   R3, R0, R10             // R10 = searchPtr = haystack + off1
	MOVD  R10, R11                // R11 = original searchPtr start
	ADD   $1, R9, R12             // R12 = remaining = searchLen + 1

	// Initialize failure counter
	MOVD  ZR, R25                 // R25 = failure count = 0

// ============================================================================
// HYBRID 1-BYTE FAST PATH:
// - Small inputs (<2KB): Use 32-byte tight loop for better speculation overlap
// - Large inputs (≥2KB): Use 128-byte loop for lower per-byte overhead
// ============================================================================

	// Check if rare1 is a non-letter (R26==0x00) - skip VORR for 5-op loop vs 7-op
	// Letter: R26=0x20 (OR to lowercase), Non-letter: R26=0x00 (identity)
	CBZ   R26, dispatch_nonletter

	CMP   $768, R12               // Threshold: 768B (tuned for Graviton)
	BGE   loop128_1byte           // Large input: use 128-byte loop
	CMP   $32, R12
	BLT   loop16_1byte_entry

loop32_main:
	// Tight loop matching Go's structure for speculation overlap
	VLD1.P 32(R10), [V16.B16, V17.B16]
	SUBS  $32, R12, R12           // Decrement early for better speculation
	
	// Case-fold and compare (only 4 vector ops)
	// OR with 0x20 forces lowercase for letters
	VORR  V0.B16, V16.B16, V20.B16
	VORR  V0.B16, V17.B16, V21.B16
	VCMEQ V1.B16, V20.B16, V20.B16
	VCMEQ V1.B16, V21.B16, V21.B16
	
	// Combine and check (3 ops before VMOV)
	VORR  V20.B16, V21.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13            // Stall point - but loop is tight enough
	
	// Branch: continue if no matches and more data
	BLT   end32_main              // R12 < 0 means we're done with 32-byte chunks
	CBZ   R13, loop32_main        // No matches, continue tight loop

end32_main:
	// Either found matches (R13 != 0) or exhausted 32-byte chunks
	CBZ   R13, loop16_1byte_entry // No matches, fall through to smaller paths
	
	// Process matches - extract syndrome for each chunk
	VAND  V5.B16, V20.B16, V20.B16
	VAND  V5.B16, V21.B16, V21.B16
	
	// Check chunk 0
	VADDP V20.B16, V20.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $128, R14               // chunk offset = 0, but use 128 to mark 32-byte mode
	CBNZ  R13, try32_main
	
	// Check chunk 1
	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $144, R14               // chunk offset = 16, but use 144 (128+16) to mark 32-byte mode
	CBNZ  R13, try32_main
	
	// No matches (shouldn't happen), continue
	CMP   $32, R12
	BGE   loop32_main
	B     loop16_1byte_entry

try32_main:
	// R13 = syndrome, R14 = chunk offset encoded as 128+offset (128=chunk0, 144=chunk1)
	RBIT  R13, R15
	CLZ   R15, R15
	LSR   $1, R15, R15            // bit position -> byte position
	AND   $0x7F, R14, R17         // extract actual chunk offset (0 or 16)
	ADD   R17, R15, R15           // add chunk offset
	
	// Calculate position in haystack
	SUB   $32, R10, R16           // ptr before load (already advanced by 32)
	ADD   R15, R16, R16           // ptr to match
	SUB   R11, R16, R16           // offset from searchPtr start
	
	CMP   R9, R16
	BGT   clear32_main
	
	// Verify the match
	ADD   R0, R16, R8             // R8 = &haystack[candidate]
	B     verify_match_1byte

clear32_main:
	// Clear this bit and try next
	AND   $0x7F, R14, R17         // extract actual chunk offset (0 or 16)
	ADD   $1, R15, R20
	SUB   R17, R20, R20
	LSL   $1, R20, R20
	MOVD  $1, R19
	LSL   R20, R19, R20
	SUB   $1, R20, R20
	BIC   R20, R13, R13
	CBNZ  R13, try32_main
	
	// Move to chunk 1 if we were in chunk 0 (R14 == 128 means chunk 0)
	CMP   $128, R14
	BNE   continue32_main
	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $144, R14               // 128 + 16 = chunk 1
	CBNZ  R13, try32_main

continue32_main:
	CMP   $32, R12
	BGE   loop32_main
	B     loop16_1byte_entry

// ============================================================================
// 128-BYTE 1-BYTE PATH (for large inputs ≥2KB - lower per-byte overhead)
// ============================================================================

loop128_1byte:
	// Load 128 bytes (8 x 16-byte vectors) in two batches
	VLD1.P 64(R10), [V16.B16, V17.B16, V18.B16, V19.B16]
	VLD1.P 64(R10), [V24.B16, V25.B16, V26.B16, V27.B16]
	SUB   $128, R12, R12

	// Process first 64 bytes (chunks 0-3)
	// OR with 0x20 forces lowercase for letters
	VORR  V0.B16, V16.B16, V20.B16
	VORR  V0.B16, V17.B16, V21.B16
	VORR  V0.B16, V18.B16, V22.B16
	VORR  V0.B16, V19.B16, V23.B16
	VCMEQ V1.B16, V20.B16, V20.B16
	VCMEQ V1.B16, V21.B16, V21.B16
	VCMEQ V1.B16, V22.B16, V22.B16
	VCMEQ V1.B16, V23.B16, V23.B16

	// Process second 64 bytes (chunks 4-7)
	VORR  V0.B16, V24.B16, V28.B16
	VORR  V0.B16, V25.B16, V29.B16
	VORR  V0.B16, V26.B16, V30.B16
	VORR  V0.B16, V27.B16, V31.B16
	VCMEQ V1.B16, V28.B16, V28.B16
	VCMEQ V1.B16, V29.B16, V29.B16
	VCMEQ V1.B16, V30.B16, V30.B16
	VCMEQ V1.B16, V31.B16, V31.B16

	// Combine all 8 chunks for quick check (use V9-V12, avoiding V7/V8 constants)
	VORR  V20.B16, V21.B16, V9.B16
	VORR  V22.B16, V23.B16, V10.B16
	VORR  V28.B16, V29.B16, V11.B16
	VORR  V30.B16, V31.B16, V12.B16
	VORR  V9.B16, V10.B16, V9.B16
	VORR  V11.B16, V12.B16, V11.B16
	VORR  V9.B16, V11.B16, V9.B16

	// Fast reduce to check if any matches
	VADDP V9.D2, V9.D2, V9.D2
	VMOV  V9.D[0], R13
	
	// Early exit: no matches in 128 bytes
	CMP   $128, R12
	BLT   end128_1byte
	CBZ   R13, loop128_1byte      // No matches, continue fast path

end128_1byte:
	// We have potential matches or exhausted large chunks
	CBZ   R13, loop32_main        // No matches, fall through to 32-byte loop
	
	// Check first 64 bytes (use V6, V9 to avoid clobbering V7/V8 constants)
	VORR  V20.B16, V21.B16, V6.B16
	VORR  V22.B16, V23.B16, V9.B16
	VORR  V6.B16, V9.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13
	CBNZ  R13, end128_first64_1byte

	// Check second 64 bytes (use V6, V9 to avoid clobbering V7/V8 constants)
	VORR  V28.B16, V29.B16, V6.B16
	VORR  V30.B16, V31.B16, V9.B16
	VORR  V6.B16, V9.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13
	CBNZ  R13, end128_second64_1byte

	// No matches, continue with appropriate loop
	CMP   $128, R12
	BGE   loop128_1byte
	B     loop32_main

end128_first64_1byte:
	// Process syndrome for first 64 bytes
	VAND  V5.B16, V20.B16, V20.B16
	VAND  V5.B16, V21.B16, V21.B16
	VAND  V5.B16, V22.B16, V22.B16
	VAND  V5.B16, V23.B16, V23.B16
	MOVD  $64, R20                // R20 = block offset (64 = first block), preserved across verify
	B     check_chunks_1byte

end128_second64_1byte:
	// Process syndrome for second 64 bytes
	VMOV  V28.B16, V20.B16
	VMOV  V29.B16, V21.B16
	VMOV  V30.B16, V22.B16
	VMOV  V31.B16, V23.B16
	VAND  V5.B16, V20.B16, V20.B16
	VAND  V5.B16, V21.B16, V21.B16
	VAND  V5.B16, V22.B16, V22.B16
	VAND  V5.B16, V23.B16, V23.B16
	MOVD  ZR, R20                 // R20 = block offset (0 = second block), preserved across verify
	B     check_chunks_1byte

check_chunks_1byte:
	// Check chunk 0
	VADDP V20.B16, V20.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  ZR, R14                 // chunk offset = 0
	CBNZ  R13, try_match_1byte

	// Check chunk 1
	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $16, R14
	CBNZ  R13, try_match_1byte

	// Check chunk 2
	VADDP V22.B16, V22.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $32, R14
	CBNZ  R13, try_match_1byte

	// Check chunk 3
	VADDP V23.B16, V23.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $48, R14
	CBNZ  R13, try_match_1byte

	// No matches in this block, check if we should continue with first 64
	CBNZ  R23, check_second64_after_first    // R23 = block offset (64 = first, 0 = second)
	CMP   $128, R12
	BGE   loop128_1byte
	B     loop32_main

check_second64_after_first:
	// We were in first 64, now check second 64 (use V6, V9 to avoid clobbering V7/V8)
	VORR  V28.B16, V29.B16, V6.B16
	VORR  V30.B16, V31.B16, V9.B16
	VORR  V6.B16, V9.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13
	CBZ   R13, continue_128_check
	B     end128_second64_1byte

continue_128_check:
	CMP   $128, R12
	BGE   loop128_1byte
	B     loop32_main

try_match_1byte:
	// R13 = syndrome, R14 = chunk offset, R20 = block offset (preserved across verify)
	RBIT  R13, R15
	CLZ   R15, R15
	LSR   $1, R15, R15            // bit position -> byte position
	ADD   R14, R15, R15           // add chunk offset
	SUB   R20, R15, R15           // adjust for block (0 or 64)
	ADD   $64, R15, R15           // adjust since second VLD1 was +64

	// Calculate position in haystack
	SUB   $128, R10, R16          // ptr before both loads
	ADD   R15, R16, R16           // ptr to match
	SUB   R11, R16, R16           // offset from searchPtr start

	CMP   R9, R16
	BGT   clear_128_1byte

	// Verify the match
	ADD   R0, R16, R8             // R8 = &haystack[candidate]
	B     verify_match_1byte

clear_128_1byte:
	// Clear this bit and try next
	ADD   $1, R15, R17
	SUB   R14, R17, R17
	LSL   $1, R17, R17
	MOVD  $1, R19
	LSL   R17, R19, R17
	SUB   $1, R17, R17
	BIC   R17, R13, R13
	CBNZ  R13, try_match_1byte

	// Move to next chunk
	ADD   $16, R14, R14
	CMP   $64, R14
	BLT   check_next_chunk_1byte
exhausted_first64_1byte:
	// Exhausted this block - check if we need second 64 bytes
	CBNZ  R20, check_second64_after_first    // R20 = block offset (64 = first, 0 = second)
	B     continue_128_check

check_next_chunk_1byte:
	CMP   $16, R14
	BEQ   chunk1_1byte
	CMP   $32, R14
	BEQ   chunk2_1byte
	CMP   $48, R14
	BEQ   chunk3_1byte
	B     continue_128_check

chunk1_1byte:
	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	CBNZ  R13, try_match_1byte
	ADD   $16, R14, R14
chunk2_1byte:
	VADDP V22.B16, V22.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	CBNZ  R13, try_match_1byte
	ADD   $16, R14, R14
chunk3_1byte:
	VADDP V23.B16, V23.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	CBNZ  R13, try_match_1byte
	B     exhausted_first64_1byte

verify_match_1byte:
	// Vectorized verification - 16 bytes at a time with NEON
	// Same algorithm as 2-byte mode: XOR + case-fold for letters
	MOVD  R7, R19                  // R19 = remaining needle length
	MOVD  R8, R21                  // R21 = haystack candidate ptr
	MOVD  R6, R22                  // R22 = needle ptr

vloop_1byte:
	SUBS  $16, R19, R23            // R23 = remaining - 16
	BLT   vtail_1byte

	// Load 16 bytes from haystack and needle
	VLD1.P 16(R21), [V10.B16]
	VLD1.P 16(R22), [V11.B16]
	MOVD   R23, R19

	// Case-insensitive compare using XOR + letter detection
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V13.B16  // V13 = h | 0x20 (force lowercase)
	VADD  V4.B16, V13.B16, V13.B16  // V13 = (h|0x20) + 159
	WORD  $0x6e2d34ed               // VCMHI V13.B16, V7.B16, V13.B16 (is letter)
	VAND  V14.B16, V13.B16, V13.B16 // V13 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V13.B16, V13.B16  // V13 = mask ? 0x20 : 0
	VEOR  V13.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10 (any non-zero?)
	FMOVS F10, R23
	CBZW  R23, vloop_1byte
	B     verify_fail_1byte        // mismatch

vtail_1byte:
	// Handle 1-15 remaining bytes using tail_mask_table
	CMP   $1, R19
	BLT   found                    // R19 <= 0 means we matched everything

	// Load with tail mask
	VLD1  (R21), [V10.B16]
	VLD1  (R22), [V11.B16]
	WORD  $0x3cf37b0d               // FMOVQ (R24)(R19<<4), F13  // ldr q13, [x24, x19, lsl #4]

	// Same case-insensitive compare
	VEOR  V10.B16, V11.B16, V12.B16
	VCMEQ V8.B16, V12.B16, V14.B16
	VORR  V8.B16, V10.B16, V15.B16
	VADD  V4.B16, V15.B16, V15.B16
	WORD  $0x6e2f34ef               // VCMHI V15.B16, V7.B16, V15.B16
	VAND  V14.B16, V15.B16, V15.B16
	VAND  V8.B16, V15.B16, V15.B16
	VEOR  V15.B16, V12.B16, V10.B16
	VAND  V13.B16, V10.B16, V10.B16 // mask out bytes beyond needle
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBNZW R23, verify_fail_1byte
	B     found

verify_fail_1byte:
	// Increment failure counter
	ADD   $1, R25, R25

	// Check threshold: failures > 4 + (bytes_scanned >> 8)
	// bytes_scanned = original_remaining - current_remaining
	// >> 8 means allow ~1 extra failure per 256 bytes scanned
	// Empirically optimal: balances pure scan (36.7 GB/s) and high-FP (17.4 GB/s)
	SUB   R11, R10, R17           // bytes_scanned = current_ptr - start_ptr
	LSR   $8, R17, R17            // bytes_scanned >> 8
	ADD   $4, R17, R17            // threshold = 4 + (bytes_scanned >> 8)
	CMP   R17, R25
	BLE   verify_fail_continue    // Continue 1-byte mode
	
	// Cutover to 2-byte mode - back up to chunk start first
	// R14 encoding:
	// - 0x100: 16-byte path (both letter and non-letter)
	// - >= 128 (but < 0x100): 32-byte mode (128=chunk0, 144=chunk1)
	// - < 64: 128-byte mode
	CMP   $0x100, R14
	BEQ   cutover_16byte
	CMP   $64, R14
	BGE   cutover_32byte
	// 128-byte mode: back up 128 bytes
	SUB   $128, R10, R10
	ADD   $128, R12, R12
	B     setup_2byte_mode
cutover_32byte:
	// 32-byte mode: back up 32 bytes
	SUB   $32, R10, R10
	ADD   $32, R12, R12
	B     setup_2byte_mode
cutover_16byte:
	// 16-byte mode: back up 16 bytes
	SUB   $16, R10, R10
	ADD   $16, R12, R12
	B     setup_2byte_mode

verify_fail_continue:

	// Check if we're in non-letter mode (R26 == 0x00)
	CBZ   R26, verify_fail_nl

	// Continue 1-byte search - check which loop we came from
	// V4.D[0] != 0 means we were in 128-byte loop (first 64 block)
	// V4.D[0] == 0 could be 128-byte (second 64) or 32-byte
	// Use R14 >= 0 (chunk offset from 128-byte) vs clear32_main state
	// Simplest: if R12 + scanned >= 2KB, we were in 128-byte mode
	CMP   $64, R14               // If R14 < 64, we were processing 128-byte chunks
	BLT   clear_128_1byte
	B     clear32_main

verify_fail_nl:
	// Non-letter path: dispatch based on R14 encoding
	// R14 = 0x100: 16-byte/scalar mode
	// R14 >= 128 (but < 0x100): 32-byte mode (128=chunk0, 144=chunk1)
	// R14 < 64: 128-byte mode  
	CMP   $0x100, R14
	BEQ   clear16_nl
	CMP   $64, R14
	BLT   clear_128_nl
	B     clear32_nl

// ============================================================================
// 16-BYTE 1-BYTE PATH (for remainder < 32 bytes)
// ============================================================================

loop16_1byte_entry:
	CMP   $16, R12
	BLT   scalar_1byte_entry

loop16_1byte:
	VLD1.P 16(R10), [V16.B16]
	SUB   $16, R12, R12

	// OR with 0x20 forces lowercase for letters
	VORR  V0.B16, V16.B16, V20.B16
	VCMEQ V1.B16, V20.B16, V20.B16
	VAND  V5.B16, V20.B16, V20.B16
	VADDP V20.B16, V20.B16, V20.B16
	VADDP V20.B16, V20.B16, V20.B16
	VMOV  V20.S[0], R13
	CBZ   R13, check16_1byte_continue

try16_1byte:
	RBIT  R13, R15
	CLZ   R15, R15
	LSR   $1, R15, R15

	SUB   $16, R10, R16
	ADD   R15, R16, R16
	SUB   R11, R16, R16

	CMP   R9, R16
	BGT   clear16_1byte

	ADD   R0, R16, R8

	// Use shared vectorized verification
	MOVD  R7, R19                  // R19 = remaining needle length
	MOVD  R8, R21                  // R21 = haystack candidate ptr
	MOVD  R6, R22                  // R22 = needle ptr

vloop16_1byte:
	SUBS  $16, R19, R23
	BLT   vtail16_1byte

	VLD1.P 16(R21), [V10.B16]
	VLD1.P 16(R22), [V11.B16]
	MOVD   R23, R19

	VEOR  V10.B16, V11.B16, V12.B16
	VCMEQ V8.B16, V12.B16, V14.B16
	VORR  V8.B16, V10.B16, V13.B16
	VADD  V4.B16, V13.B16, V13.B16
	WORD  $0x6e2d34ed               // VCMHI V13.B16, V7.B16, V13.B16
	VAND  V14.B16, V13.B16, V13.B16
	VAND  V8.B16, V13.B16, V13.B16
	VEOR  V13.B16, V12.B16, V10.B16
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBZW  R23, vloop16_1byte
	B     verify_fail16_1byte

vtail16_1byte:
	CMP   $1, R19
	BLT   found

	VLD1  (R21), [V10.B16]
	VLD1  (R22), [V11.B16]
	WORD  $0x3cf37b0d               // FMOVQ (R24)(R19<<4), F13

	VEOR  V10.B16, V11.B16, V12.B16
	VCMEQ V8.B16, V12.B16, V14.B16
	VORR  V8.B16, V10.B16, V15.B16
	VADD  V4.B16, V15.B16, V15.B16
	WORD  $0x6e2f34ef               // VCMHI V15.B16, V7.B16, V15.B16
	VAND  V14.B16, V15.B16, V15.B16
	VAND  V8.B16, V15.B16, V15.B16
	VEOR  V15.B16, V12.B16, V10.B16
	VAND  V13.B16, V10.B16, V10.B16
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBNZW R23, verify_fail16_1byte
	B     found

verify_fail16_1byte:
	ADD   $1, R25, R25
	SUB   R11, R10, R17
	LSR   $8, R17, R17             // bytes_scanned >> 8 (1 failure per 256 bytes)
	ADD   $4, R17, R17
	CMP   R17, R25
	BLE   clear16_1byte
	// Cutover to 2-byte mode - back up 16 bytes to chunk start
	SUB   $16, R10, R10
	ADD   $16, R12, R12
	B     setup_2byte_mode

clear16_1byte:
	ADD   $1, R15, R17
	LSL   $1, R17, R17
	MOVD  $1, R19
	LSL   R17, R19, R17
	SUB   $1, R17, R17
	BIC   R17, R13, R13
	CBNZ  R13, try16_1byte

check16_1byte_continue:
	CMP   $16, R12
	BGE   loop16_1byte

// ============================================================================
// SCALAR 1-BYTE PATH
// ============================================================================

scalar_1byte_entry:
	CMP   $0, R12
	BLE   not_found

scalar_1byte:
	MOVBU (R10), R13
	ORRW  R26, R13, R14           // OR with 0x20 (or 0x00 for non-letter)
	CMPW  R27, R14
	BNE   scalar_next_1byte

	SUB   R11, R10, R16
	CMP   R9, R16
	BGT   scalar_next_1byte

	ADD   R0, R16, R8

	// Use vectorized verification (same as other 1-byte paths)
	MOVD  R7, R19                  // R19 = remaining needle length
	MOVD  R8, R21                  // R21 = haystack candidate ptr
	MOVD  R6, R22                  // R22 = needle ptr
	MOVD  $0x100, R14              // Mark as scalar path for cutover

vloop_scalar_1byte:
	SUBS  $16, R19, R23            // R23 = remaining - 16
	BLT   vtail_scalar_1byte

	// Load 16 bytes from haystack and needle
	VLD1.P 16(R21), [V10.B16]
	VLD1.P 16(R22), [V11.B16]
	MOVD   R23, R19

	// Case-insensitive compare using XOR + letter detection
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V13.B16  // V13 = h | 0x20 (force lowercase)
	VADD  V4.B16, V13.B16, V13.B16  // V13 = (h|0x20) + 159
	WORD  $0x6e2d34ed               // VCMHI V13.B16, V7.B16, V13.B16 (is letter)
	VAND  V14.B16, V13.B16, V13.B16 // V13 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V13.B16, V13.B16  // V13 = mask ? 0x20 : 0
	VEOR  V13.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10 (any non-zero?)
	FMOVS F10, R23
	CBZW  R23, vloop_scalar_1byte
	B     scalar_fail_1byte        // mismatch

vtail_scalar_1byte:
	// Handle 1-15 remaining bytes using tail_mask_table
	CMP   $1, R19
	BLT   found                    // R19 <= 0 means we matched everything

	// Load with tail mask
	VLD1  (R21), [V10.B16]
	VLD1  (R22), [V11.B16]
	WORD  $0x3cf37b0d               // FMOVQ (R24)(R19<<4), F13  // ldr q13, [x24, x19, lsl #4]

	// Same case-insensitive compare
	VEOR  V10.B16, V11.B16, V12.B16
	VCMEQ V8.B16, V12.B16, V14.B16
	VORR  V8.B16, V10.B16, V15.B16
	VADD  V4.B16, V15.B16, V15.B16
	WORD  $0x6e2f34ef               // VCMHI V15.B16, V7.B16, V15.B16
	VAND  V14.B16, V15.B16, V15.B16
	VAND  V8.B16, V15.B16, V15.B16
	VEOR  V15.B16, V12.B16, V10.B16
	VAND  V13.B16, V10.B16, V10.B16 // mask out bytes beyond needle
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBNZW R23, scalar_fail_1byte
	B     found

scalar_fail_1byte:
	ADD   $1, R25, R25
	SUB   R11, R10, R17
	LSR   $8, R17, R17             // bytes_scanned >> 8 (1 failure per 256 bytes)
	ADD   $4, R17, R17
	CMP   R17, R25
	BGT   setup_2byte_mode         // Scalar: R10 not yet incremented, no backup needed

scalar_next_1byte:
	ADD   $1, R10
	SUB   $1, R12
	CBNZ  R12, scalar_1byte
	B     not_found

// ============================================================================
// NON-LETTER FAST PATH: Skip VAND when rare1 is not a letter (mask=0xFF)
// For non-letters, VAND with 0xFF is a no-op. This gives us a 5-op loop
// matching Go's case-sensitive search exactly.
// ============================================================================

dispatch_nonletter:
	CMP   $768, R12
	BGE   loop128_nl
	CMP   $32, R12
	BLT   loop16_nl_entry

loop32_nl:
	// 5-op tight loop: VLD1 → SUBS → 2×VCMEQ → VORR → VADDP → VMOV
	VLD1.P 32(R10), [V16.B16, V17.B16]
	SUBS  $32, R12, R12
	
	// Direct compare (no VAND needed - mask is 0xFF)
	VCMEQ V1.B16, V16.B16, V20.B16
	VCMEQ V1.B16, V17.B16, V21.B16
	
	// Combine and check
	VORR  V20.B16, V21.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13
	
	BLT   end32_nl
	CBZ   R13, loop32_nl

end32_nl:
	CBZ   R13, loop16_nl_entry
	
	// Extract syndrome for each chunk
	VAND  V5.B16, V20.B16, V20.B16
	VAND  V5.B16, V21.B16, V21.B16
	
	// Check chunk 0
	VADDP V20.B16, V20.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $128, R14               // 128 = chunk 0 marker for 32-byte mode
	CBNZ  R13, try32_nl
	
	// Check chunk 1
	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $144, R14               // 144 = chunk 1 marker (128+16)
	CBNZ  R13, try32_nl
	
	CMP   $32, R12
	BGE   loop32_nl
	B     loop16_nl_entry

try32_nl:
	RBIT  R13, R15
	CLZ   R15, R15
	LSR   $1, R15, R15
	AND   $0x7F, R14, R17
	ADD   R17, R15, R15
	
	SUB   $32, R10, R16
	ADD   R15, R16, R16
	SUB   R11, R16, R16
	
	CMP   R9, R16
	BGT   clear32_nl
	
	ADD   R0, R16, R8
	B     verify_match_1byte      // Reuse letter path verification

clear32_nl:
	AND   $0x7F, R14, R17
	ADD   $1, R15, R20
	SUB   R17, R20, R20
	LSL   $1, R20, R20
	MOVD  $1, R19
	LSL   R20, R19, R20
	SUB   $1, R20, R20
	BIC   R20, R13, R13
	CBNZ  R13, try32_nl
	
	CMP   $128, R14
	BNE   continue32_nl
	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $144, R14
	CBNZ  R13, try32_nl

continue32_nl:
	CMP   $32, R12
	BGE   loop32_nl
	B     loop16_nl_entry

// 128-byte non-letter loop
loop128_nl:
	VLD1.P 64(R10), [V16.B16, V17.B16, V18.B16, V19.B16]
	VLD1.P 64(R10), [V24.B16, V25.B16, V26.B16, V27.B16]
	SUB   $128, R12, R12

	// Direct compare - no VAND
	VCMEQ V1.B16, V16.B16, V20.B16
	VCMEQ V1.B16, V17.B16, V21.B16
	VCMEQ V1.B16, V18.B16, V22.B16
	VCMEQ V1.B16, V19.B16, V23.B16
	VCMEQ V1.B16, V24.B16, V28.B16
	VCMEQ V1.B16, V25.B16, V29.B16
	VCMEQ V1.B16, V26.B16, V30.B16
	VCMEQ V1.B16, V27.B16, V31.B16

	// Combine all 8 chunks (use V9-V12, avoiding V7/V8 constants)
	VORR  V20.B16, V21.B16, V9.B16
	VORR  V22.B16, V23.B16, V10.B16
	VORR  V28.B16, V29.B16, V11.B16
	VORR  V30.B16, V31.B16, V12.B16
	VORR  V9.B16, V10.B16, V9.B16
	VORR  V11.B16, V12.B16, V11.B16
	VORR  V9.B16, V11.B16, V9.B16

	VADDP V9.D2, V9.D2, V9.D2
	VMOV  V9.D[0], R13
	
	CMP   $128, R12
	BLT   end128_nl
	CBZ   R13, loop128_nl

end128_nl:
	CBZ   R13, loop32_nl
	
	// Check first 64 bytes (use V6, V9 to avoid clobbering V7/V8 constants)
	VORR  V20.B16, V21.B16, V6.B16
	VORR  V22.B16, V23.B16, V9.B16
	VORR  V6.B16, V9.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13
	CBNZ  R13, end128_first64_nl

	// Check second 64 bytes (use V6, V9 to avoid clobbering V7/V8 constants)
	VORR  V28.B16, V29.B16, V6.B16
	VORR  V30.B16, V31.B16, V9.B16
	VORR  V6.B16, V9.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13
	CBNZ  R13, end128_second64_nl

	CMP   $128, R12
	BGE   loop128_nl
	B     loop32_nl

end128_first64_nl:
	VAND  V5.B16, V20.B16, V20.B16
	VAND  V5.B16, V21.B16, V21.B16
	VAND  V5.B16, V22.B16, V22.B16
	VAND  V5.B16, V23.B16, V23.B16
	MOVD  $64, R20                // R20 = block offset (64 = first block), preserved across verify
	B     check_chunks_nl

end128_second64_nl:
	VMOV  V28.B16, V20.B16
	VMOV  V29.B16, V21.B16
	VMOV  V30.B16, V22.B16
	VMOV  V31.B16, V23.B16
	VAND  V5.B16, V20.B16, V20.B16
	VAND  V5.B16, V21.B16, V21.B16
	VAND  V5.B16, V22.B16, V22.B16
	VAND  V5.B16, V23.B16, V23.B16
	MOVD  ZR, R20                 // R20 = block offset (0 = second block), preserved across verify
	B     check_chunks_nl

check_chunks_nl:
	VADDP V20.B16, V20.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  ZR, R14
	CBNZ  R13, try_match_nl

	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $16, R14
	CBNZ  R13, try_match_nl

	VADDP V22.B16, V22.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $32, R14
	CBNZ  R13, try_match_nl

	VADDP V23.B16, V23.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	MOVD  $48, R14
	CBNZ  R13, try_match_nl

	CBNZ  R23, check_second64_after_first_nl   // R23 = block offset (64 = first, 0 = second)
	CMP   $128, R12
	BGE   loop128_nl
	B     loop32_nl

check_second64_after_first_nl:
	// Use V6, V9 to avoid clobbering V7/V8 constants
	VORR  V28.B16, V29.B16, V6.B16
	VORR  V30.B16, V31.B16, V9.B16
	VORR  V6.B16, V9.B16, V6.B16
	VADDP V6.D2, V6.D2, V6.D2
	VMOV  V6.D[0], R13
	CBZ   R13, continue_128_check_nl
	B     end128_second64_nl

continue_128_check_nl:
	CMP   $128, R12
	BGE   loop128_nl
	B     loop32_nl

try_match_nl:
	RBIT  R13, R15
	CLZ   R15, R15
	LSR   $1, R15, R15
	ADD   R14, R15, R15
	SUB   R20, R15, R15           // adjust for block (0 or 64), R20 preserved across verify
	ADD   $64, R15, R15

	SUB   $128, R10, R16
	ADD   R15, R16, R16
	SUB   R11, R16, R16

	CMP   R9, R16
	BGT   clear_128_nl

	ADD   R0, R16, R8
	B     verify_match_1byte

clear_128_nl:
	ADD   $1, R15, R17
	SUB   R14, R17, R17
	LSL   $1, R17, R17
	MOVD  $1, R19
	LSL   R17, R19, R17
	SUB   $1, R17, R17
	BIC   R17, R13, R13
	CBNZ  R13, try_match_nl

	ADD   $16, R14, R14
	CMP   $64, R14
	BLT   check_next_chunk_nl
	CBNZ  R20, check_second64_after_first_nl   // R20 = block offset (64 = first, 0 = second)
	B     continue_128_check_nl

check_next_chunk_nl:
	CMP   $16, R14
	BEQ   chunk1_nl
	CMP   $32, R14
	BEQ   chunk2_nl
	CMP   $48, R14
	BEQ   chunk3_nl
	B     continue_128_check_nl

chunk1_nl:
	VADDP V21.B16, V21.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	CBNZ  R13, try_match_nl
	ADD   $16, R14, R14
chunk2_nl:
	VADDP V22.B16, V22.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	CBNZ  R13, try_match_nl
	ADD   $16, R14, R14
chunk3_nl:
	VADDP V23.B16, V23.B16, V6.B16
	VADDP V6.B16, V6.B16, V6.B16
	VMOV  V6.S[0], R13
	CBNZ  R13, try_match_nl
	CBNZ  R20, check_second64_after_first_nl   // R20 = block offset (64 = first, 0 = second)
	B     continue_128_check_nl

// 16-byte non-letter loop
loop16_nl_entry:
	CMP   $16, R12
	BLT   scalar_nl_entry

loop16_nl:
	VLD1.P 16(R10), [V16.B16]
	SUB   $16, R12, R12

	VCMEQ V1.B16, V16.B16, V20.B16
	VAND  V5.B16, V20.B16, V20.B16
	VADDP V20.B16, V20.B16, V20.B16
	VADDP V20.B16, V20.B16, V20.B16
	VMOV  V20.S[0], R13
	CBZ   R13, check16_nl_continue

try16_nl:
	RBIT  R13, R15
	CLZ   R15, R15
	LSR   $1, R15, R15

	SUB   $16, R10, R16
	ADD   R15, R16, R16
	SUB   R11, R16, R16

	CMP   R9, R16
	BGT   clear16_nl

	ADD   R0, R16, R8
	MOVD  $0x100, R14             // Mark as 16-byte non-letter path
	B     verify_match_1byte

clear16_nl:
	ADD   $1, R15, R17
	LSL   $1, R17, R17
	MOVD  $1, R19
	LSL   R17, R19, R17
	SUB   $1, R17, R17
	BIC   R17, R13, R13
	CBNZ  R13, try16_nl

check16_nl_continue:
	CMP   $16, R12
	BGE   loop16_nl

// Scalar non-letter path
scalar_nl_entry:
	CMP   $0, R12
	BLE   not_found

scalar_nl:
	MOVBU (R10), R13
	CMPW  R27, R13                // Direct compare (no mask needed)
	BNE   scalar_next_nl

	SUB   R11, R10, R16
	CMP   R9, R16
	BGT   scalar_next_nl

	ADD   R0, R16, R8

	// Use vectorized verification (same as other 1-byte paths)
	MOVD  R7, R19                  // R19 = remaining needle length
	MOVD  R8, R21                  // R21 = haystack candidate ptr
	MOVD  R6, R22                  // R22 = needle ptr
	MOVD  $0x100, R14              // Mark as scalar path for cutover

vloop_scalar_nl:
	SUBS  $16, R19, R23            // R23 = remaining - 16
	BLT   vtail_scalar_nl

	// Load 16 bytes from haystack and needle
	VLD1.P 16(R21), [V10.B16]
	VLD1.P 16(R22), [V11.B16]
	MOVD   R23, R19

	// Case-insensitive compare using XOR + letter detection
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V13.B16  // V13 = h | 0x20 (force lowercase)
	VADD  V4.B16, V13.B16, V13.B16  // V13 = (h|0x20) + 159
	WORD  $0x6e2d34ed               // VCMHI V13.B16, V7.B16, V13.B16 (is letter)
	VAND  V14.B16, V13.B16, V13.B16 // V13 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V13.B16, V13.B16  // V13 = mask ? 0x20 : 0
	VEOR  V13.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10 (any non-zero?)
	FMOVS F10, R23
	CBZW  R23, vloop_scalar_nl
	B     scalar_next_nl           // mismatch, try next position

vtail_scalar_nl:
	// Handle 1-15 remaining bytes using tail_mask_table
	CMP   $1, R19
	BLT   found                    // R19 <= 0 means we matched everything

	// Load with tail mask
	VLD1  (R21), [V10.B16]
	VLD1  (R22), [V11.B16]
	WORD  $0x3cf37b0d               // FMOVQ (R24)(R19<<4), F13  // ldr q13, [x24, x19, lsl #4]

	// Same case-insensitive compare
	VEOR  V10.B16, V11.B16, V12.B16
	VCMEQ V8.B16, V12.B16, V14.B16
	VORR  V8.B16, V10.B16, V15.B16
	VADD  V4.B16, V15.B16, V15.B16
	WORD  $0x6e2f34ef               // VCMHI V15.B16, V7.B16, V15.B16
	VAND  V14.B16, V15.B16, V15.B16
	VAND  V8.B16, V15.B16, V15.B16
	VEOR  V15.B16, V12.B16, V10.B16
	VAND  V13.B16, V10.B16, V10.B16 // mask out bytes beyond needle
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBNZW R23, scalar_next_nl
	B     found

scalar_next_nl:
	ADD   $1, R10
	SUB   $1, R12
	CBNZ  R12, scalar_nl
	B     not_found

// ============================================================================
// 2-BYTE MODE SETUP: Continue search using 2-byte filtering from current chunk
// Caller has backed up R10 to the start of the current chunk and adjusted R12.
// This ensures we only re-scan the current chunk, not the entire haystack.
// ============================================================================

setup_2byte_mode:
	// Reload rare2 from stack (R4/R5 may have been clobbered)
	MOVBU rare2+32(FP), R4        // R4 = rare2 byte
	MOVD  off2+40(FP), R5         // R5 = off2

	// Compute mask and target for rare2
	// With lowercase normalization, rare bytes are lowercase (a-z)
	// For letters: OR with 0x20 to force lowercase, compare to lowercase target
	// For non-letters: OR with 0x00 (identity)
	SUBW  $97, R4, R17            // R17 = rare2 - 'a'
	CMPW  $26, R17
	BCS   not_letter2
	MOVW  $0x20, R21              // rare2 mask = 0x20 (OR to force lowercase)
	MOVW  R4, R22                 // rare2 target = rare2 (already lowercase)
	B     setup_rare2_done
not_letter2:
	MOVW  $0x00, R21              // rare2 mask = 0x00 (OR with 0 = identity)
	MOVW  R4, R22                 // rare2 target = byte itself
setup_rare2_done:
	VDUP  R21, V2.B16             // V2 = rare2 mask
	VDUP  R22, V3.B16             // V3 = rare2 target

	// Setup vectorized verification constants (like NEON-64B)
	// V4 = 159 (-97 as unsigned), V7 = 26, V8 = 32
	// For letter detection: (byte | 0x20) + 159 < 26, i.e., byte is a-z or A-Z
	WORD  $0x4f04e7e4             // VMOVI $159, V4.B16 (for case-fold: byte + 159 = byte - 97)
	WORD  $0x4f00e747             // VMOVI $26, V7.B16
	WORD  $0x4f01e408             // VMOVI $32, V8.B16

	// Setup bit-clear constant (R24 already has tail_mask_table from function entry)
	MOVW  $15, R15                     // R15 = 0xF for clearing syndrome bits

	// Continue from current position (R10/R11/R12 already set by caller)
	// The caller backed up R10 to the start of the current chunk and
	// adjusted R12 accordingly, so 2-byte mode re-scans just that chunk

	B     loop64_2byte_entry

// ============================================================================
// 2-BYTE MODE: Filter on BOTH rare1 AND rare2 (consistent 17 GB/s)
// Optimized to match NEON-64B performance:
// 1. Load rare1 AND rare2 together upfront (not conditionally)
// 2. Use SHRN $4 + FMOVD for syndrome extraction (1 vs 3 instructions)
// 3. Use vectorized XOR+UMAXV for verification (16 bytes/iter vs 1)
// 4. Use tail_mask_table for remainder handling
// ============================================================================

loop64_2byte_entry:
	CMP   $64, R12
	BLT   loop16_2byte_entry

loop64_2byte:
	// Save position for later (R17 = position in search)
	SUB   R11, R10, R17

	// Calculate both load positions upfront
	// R10 points to off1 position, we also need off2 position
	SUB   R3, R10, R23            // R23 = haystack position (R10 - off1)
	ADD   R5, R23, R23            // R23 = off2 position (haystack + off2)

	// Load BOTH rare1 and rare2 positions unconditionally (Gap #2 fix)
	VLD1  (R10), [V16.B16, V17.B16, V18.B16, V19.B16]   // rare1 data
	VLD1  (R23), [V24.B16, V25.B16, V26.B16, V27.B16]   // rare2 data
	ADD   $64, R10, R10
	SUB   $64, R12, R12

	// Check rare1 matches (OR with 0x20 forces lowercase for letters)
	VORR  V0.B16, V16.B16, V20.B16
	VORR  V0.B16, V17.B16, V21.B16
	VORR  V0.B16, V18.B16, V22.B16
	VORR  V0.B16, V19.B16, V23.B16
	VCMEQ V1.B16, V20.B16, V20.B16
	VCMEQ V1.B16, V21.B16, V21.B16
	VCMEQ V1.B16, V22.B16, V22.B16
	VCMEQ V1.B16, V23.B16, V23.B16

	// Check rare2 matches (OR with 0x20 forces lowercase for letters)
	VORR  V2.B16, V24.B16, V28.B16
	VORR  V2.B16, V25.B16, V29.B16
	VORR  V2.B16, V26.B16, V30.B16
	VORR  V2.B16, V27.B16, V31.B16
	VCMEQ V3.B16, V28.B16, V28.B16
	VCMEQ V3.B16, V29.B16, V29.B16
	VCMEQ V3.B16, V30.B16, V30.B16
	VCMEQ V3.B16, V31.B16, V31.B16

	// AND rare1 and rare2 results
	VAND  V20.B16, V28.B16, V20.B16
	VAND  V21.B16, V29.B16, V21.B16
	VAND  V22.B16, V30.B16, V22.B16
	VAND  V23.B16, V31.B16, V23.B16

	// Quick check if any matches (using UMAXV like NEON-64B)
	VORR  V20.B16, V21.B16, V6.B16
	VORR  V22.B16, V23.B16, V9.B16
	VORR  V6.B16, V9.B16, V6.B16
	WORD  $0x6e30a8c6               // VUMAXV V6.B16, V6
	FMOVS F6, R13
	CBZW  R13, check64_2byte_continue

	// Extract syndromes using SHRN $4 + FMOVD (Gap #1 fix)
	// This packs 16 match bytes into 8 nibbles in the low 64 bits
	WORD  $0x0f0c8694               // VSHRN $4, V20.H8, V20.B8
	FMOVD F20, R13
	CBNZ  R13, try64_chunk0_2byte

chunk1_syndrome_2byte:
	WORD  $0x0f0c86b5               // VSHRN $4, V21.H8, V21.B8
	FMOVD F21, R13
	CBNZ  R13, try64_chunk1_2byte

chunk2_syndrome_2byte:
	WORD  $0x0f0c86d6               // VSHRN $4, V22.H8, V22.B8
	FMOVD F22, R13
	CBNZ  R13, try64_chunk2_2byte

chunk3_syndrome_2byte:
	WORD  $0x0f0c86f7               // VSHRN $4, V23.H8, V23.B8
	FMOVD F23, R13
	CBNZ  R13, try64_chunk3_2byte
	B     check64_2byte_continue

try64_chunk0_2byte:
	MOVD  ZR, R14
	B     try64_2byte
try64_chunk1_2byte:
	MOVD  $16, R14
	B     try64_2byte
try64_chunk2_2byte:
	MOVD  $32, R14
	B     try64_2byte
try64_chunk3_2byte:
	MOVD  $48, R14

try64_2byte:
	// R13 = syndrome, R14 = chunk offset, R17 = search position
	RBIT  R13, R19
	CLZ   R19, R19
	AND   $60, R19, R20            // R20 = (clz & 0x3c) for clearing - PRESERVED
	LSR   $2, R19, R19             // R19 = byte offset in chunk
	ADD   R14, R19, R19            // R19 = byte offset in 64-byte block

	// Position = haystack + search_position + byte_offset
	// R17 = position relative to R11 (start), R11 = haystack + off1
	// Haystack position = R11 - off1 + R17 + R19 = R0 + R17 + R19
	ADD   R17, R0, R8
	ADD   R19, R8, R8              // R8 = candidate haystack ptr

	// Check bounds
	SUB   R0, R8, R23              // position in haystack
	CMP   R9, R23
	BGT   clear64_2byte

	// Vectorized verification (Gap #3 fix)
	// Load haystack candidate and needle, XOR, apply case-folding, check non-zero
	// Note: R20 must be preserved for syndrome clearing, use R21/R22 for ptrs
	MOVD  R7, R19                  // R19 = remaining needle length
	MOVD  R8, R21                  // R21 = haystack candidate ptr
	MOVD  R6, R22                  // R22 = needle ptr

vloop64_2byte:
	SUBS  $16, R19, R23            // R23 = remaining - 16
	BLT   vtail64_2byte

	// Load 16 bytes from haystack and needle
	VLD1.P 16(R21), [V10.B16]
	VLD1.P 16(R22), [V11.B16]
	MOVD   R23, R19

	// Case-insensitive compare:
	// 1. XOR to find differences
	// 2. Only mask 0x20 if XOR==0x20 AND byte is a letter
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V13.B16  // V13 = h | 0x20 (force lowercase)
	VADD  V4.B16, V13.B16, V13.B16  // V13 = (h|0x20) + 159
	WORD  $0x6e2d34ed               // VCMHI V13.B16, V7.B16, V13.B16 (is letter)
	VAND  V14.B16, V13.B16, V13.B16 // V13 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V13.B16, V13.B16  // V13 = mask ? 0x20 : 0
	VEOR  V13.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10 (any non-zero?)
	FMOVS F10, R23
	CBZW  R23, vloop64_2byte
	B     clear64_2byte            // mismatch

vtail64_2byte:
	// Handle 1-15 remaining bytes using tail_mask_table (Gap #4 fix)
	CMP   $1, R19
	BLT   found_2byte              // R19 <= 0 means we matched everything

	// Load with tail mask
	VLD1  (R21), [V10.B16]
	VLD1  (R22), [V11.B16]
	WORD  $0x3cf37b0d               // FMOVQ (R24)(R19<<4), F13  // ldr q13, [x24, x19, lsl #4]

	// Same case-insensitive compare (with XOR==0x20 check)
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V15.B16  // V15 = h | 0x20
	VADD  V4.B16, V15.B16, V15.B16  // V15 = (h|0x20) + 159
	WORD  $0x6e2f34ef               // VCMHI V15.B16, V7.B16, V15.B16 (is letter)
	VAND  V14.B16, V15.B16, V15.B16 // V15 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V15.B16, V15.B16  // V15 = mask ? 0x20 : 0
	VEOR  V15.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	VAND  V13.B16, V10.B16, V10.B16 // mask out bytes beyond needle
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBNZW R23, clear64_2byte

found_2byte:
	SUB   R0, R8, R0
	MOVD  R0, ret+64(FP)
	RET

clear64_2byte:
	// Clear the bit we just tried and continue
	LSL   R20, R15, R20            // R20 = 0xF << (clz & 0x3c)
	BIC   R20, R13, R13
	CBNZ  R13, try64_2byte

	// Move to next chunk
	ADD   $16, R14, R14
	CMP   $16, R14
	BEQ   chunk1_syndrome_2byte
	CMP   $32, R14
	BEQ   chunk2_syndrome_2byte
	CMP   $48, R14
	BEQ   chunk3_syndrome_2byte
	// R14 >= 64: all chunks exhausted, continue to next 64-byte block
	B     check64_2byte_continue

check64_2byte_continue:
	CMP   $64, R12
	BGE   loop64_2byte
	B     loop16_2byte_entry

// ============================================================================
// 16-BYTE 2-BYTE PATH (optimized with SHRN + vectorized verification)
// ============================================================================

loop16_2byte_entry:
	CMP   $16, R12
	BLT   scalar_2byte_entry

loop16_2byte:
	// Save position for later
	SUB   R11, R10, R17

	// Calculate both load positions
	SUB   R3, R10, R23            // R23 = haystack position
	ADD   R5, R23, R23            // R23 = off2 position

	// Load BOTH rare1 and rare2 unconditionally
	VLD1  (R10), [V16.B16]        // rare1 data
	VLD1  (R23), [V24.B16]        // rare2 data
	ADD   $16, R10, R10
	SUB   $16, R12, R12

	// Check rare1 matches (OR with 0x20 forces lowercase for letters)
	VORR  V0.B16, V16.B16, V20.B16
	VCMEQ V1.B16, V20.B16, V20.B16

	// Check rare2 matches (OR with 0x20 forces lowercase for letters)
	VORR  V2.B16, V24.B16, V28.B16
	VCMEQ V3.B16, V28.B16, V28.B16

	// AND results
	VAND  V20.B16, V28.B16, V20.B16

	// Quick check using UMAXV
	WORD  $0x6e30aa94               // VUMAXV V20.B16, V20
	FMOVS F20, R13
	CBZW  R13, check16_2byte_continue

	// Reload match vector (UMAXV clobbered it) and extract syndrome
	VORR  V0.B16, V16.B16, V20.B16
	VCMEQ V1.B16, V20.B16, V20.B16
	VAND  V20.B16, V28.B16, V20.B16
	WORD  $0x0f0c8694               // VSHRN $4, V20.H8, V20.B8
	FMOVD F20, R13
	CBZ   R13, check16_2byte_continue

try16_2byte:
	RBIT  R13, R19
	CLZ   R19, R19
	AND   $60, R19, R20            // R20 = (clz & 0x3c) for clearing
	LSR   $2, R19, R19             // R19 = byte offset

	// Position = haystack + search_position + byte_offset
	ADD   R17, R0, R8
	ADD   R19, R8, R8

	// Check bounds
	SUB   R0, R8, R23
	CMP   R9, R23
	BGT   clear16_2byte

	// Vectorized verification (same as 64-byte path)
	MOVD  R7, R19                  // R19 = remaining needle length
	MOVD  R8, R21                  // R21 = haystack candidate ptr (use R21, R20 is clz mask)
	MOVD  R6, R22                  // R22 = needle ptr

vloop16_2byte:
	SUBS  $16, R19, R23
	BLT   vtail16_2byte

	VLD1.P 16(R21), [V10.B16]
	VLD1.P 16(R22), [V11.B16]
	MOVD   R23, R19

	// Case-insensitive compare (with XOR==0x20 check)
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V13.B16  // V13 = h | 0x20
	VADD  V4.B16, V13.B16, V13.B16  // V13 = (h|0x20) + 159
	WORD  $0x6e2d34ed               // VCMHI V13.B16, V7.B16, V13.B16 (is letter)
	VAND  V14.B16, V13.B16, V13.B16 // V13 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V13.B16, V13.B16  // V13 = mask ? 0x20 : 0
	VEOR  V13.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBZW  R23, vloop16_2byte
	B     clear16_2byte

vtail16_2byte:
	CMP   $1, R19
	BLT   found_2byte

	VLD1  (R21), [V10.B16]
	VLD1  (R22), [V11.B16]
	WORD  $0x3cf37b0d               // FMOVQ (R24)(R19<<4), F13

	// Case-insensitive compare (with XOR==0x20 check)
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V15.B16  // V15 = h | 0x20
	VADD  V4.B16, V15.B16, V15.B16  // V15 = (h|0x20) + 159
	WORD  $0x6e2f34ef               // VCMHI V15.B16, V7.B16, V15.B16 (is letter)
	VAND  V14.B16, V15.B16, V15.B16 // V15 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V15.B16, V15.B16  // V15 = mask ? 0x20 : 0
	VEOR  V15.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	VAND  V13.B16, V10.B16, V10.B16 // mask out bytes beyond needle
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBNZW R23, clear16_2byte
	B     found_2byte

clear16_2byte:
	LSL   R20, R15, R20
	BIC   R20, R13, R13
	CBNZ  R13, try16_2byte

check16_2byte_continue:
	CMP   $16, R12
	BGE   loop16_2byte

// ============================================================================
// SCALAR 2-BYTE PATH (with vectorized verification)
// R10 = current position at off1
// ============================================================================

scalar_2byte_entry:
	CMP   $0, R12
	BLE   not_found

scalar_2byte:
	// Check rare1 at current position (R10 points to off1 position)
	MOVBU (R10), R13
	ORRW  R26, R13, R14           // R26 = rare1 mask (OR with 0x20 or 0x00)
	CMPW  R27, R14                // R27 = rare1 target
	BNE   scalar_next_2byte

	// Check rare2 at off2 position: current_ptr - off1 + off2
	SUB   R3, R10, R17            // R17 = haystack position
	ADD   R5, R17, R23            // R23 = off2 position
	MOVBU (R23), R14
	// Extract rare2 mask/target from V2/V3
	VMOV  V2.B[0], R21
	VMOV  V3.B[0], R22
	ORRW  R21, R14, R14           // OR with rare2 mask
	CMPW  R22, R14
	BNE   scalar_next_2byte

	// Check bounds
	SUB   R0, R17, R23
	CMP   R9, R23
	BGT   scalar_next_2byte

	MOVD  R17, R8                 // R8 = candidate haystack ptr

	// Vectorized verification
	MOVD  R7, R19                 // R19 = remaining needle length
	MOVD  R8, R21                 // R21 = haystack candidate ptr
	MOVD  R6, R22                 // R22 = needle ptr

vloop_scalar_2byte:
	SUBS  $16, R19, R23
	BLT   vtail_scalar_2byte

	VLD1.P 16(R21), [V10.B16]
	VLD1.P 16(R22), [V11.B16]
	MOVD   R23, R19

	// Case-insensitive compare (with XOR==0x20 check)
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V13.B16  // V13 = h | 0x20
	VADD  V4.B16, V13.B16, V13.B16  // V13 = (h|0x20) + 159
	WORD  $0x6e2d34ed               // VCMHI V13.B16, V7.B16, V13.B16 (is letter)
	VAND  V14.B16, V13.B16, V13.B16 // V13 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V13.B16, V13.B16  // V13 = mask ? 0x20 : 0
	VEOR  V13.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBZW  R23, vloop_scalar_2byte
	B     scalar_next_2byte

vtail_scalar_2byte:
	CMP   $1, R19
	BLT   found_2byte

	VLD1  (R21), [V10.B16]
	VLD1  (R22), [V11.B16]
	WORD  $0x3cf37b0d               // FMOVQ (R24)(R19<<4), F13

	// Case-insensitive compare (with XOR==0x20 check)
	VEOR  V10.B16, V11.B16, V12.B16 // V12 = XOR diff
	VCMEQ V8.B16, V12.B16, V14.B16  // V14 = (XOR == 0x20) ? 0xFF : 0
	VORR  V8.B16, V10.B16, V15.B16  // V15 = h | 0x20
	VADD  V4.B16, V15.B16, V15.B16  // V15 = (h|0x20) + 159
	WORD  $0x6e2f34ef               // VCMHI V15.B16, V7.B16, V15.B16 (is letter)
	VAND  V14.B16, V15.B16, V15.B16 // V15 = (XOR==0x20 && is_letter) ? 0xFF : 0
	VAND  V8.B16, V15.B16, V15.B16  // V15 = mask ? 0x20 : 0
	VEOR  V15.B16, V12.B16, V10.B16 // V10 = diff with case masked out
	VAND  V13.B16, V10.B16, V10.B16 // mask out bytes beyond needle
	WORD  $0x6e30a94a               // VUMAXV V10.B16, V10
	FMOVS F10, R23
	CBNZW R23, scalar_next_2byte
	B     found_2byte

scalar_next_2byte:
	ADD   $1, R10
	SUB   $1, R12
	CBNZ  R12, scalar_2byte
	B     not_found

// ============================================================================
// COMMON RETURN PATHS
// ============================================================================

found:
	SUB   R0, R8, R0
	MOVD  R0, ret+64(FP)
	RET

not_found:
	MOVD  $-1, R0
	MOVD  R0, ret+64(FP)
	RET

found_zero:
	MOVD  ZR, R0
	MOVD  R0, ret+64(FP)
	RET

// ============================================================================
// TAIL MASK TABLE for vectorized verification
// Entry N (0-15) has first N bytes as 0xFF, rest as 0x00
// Used for masking partial vectors in tail processing
// ============================================================================
DATA tail_mask_table<>+0x00(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x08(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x10(SB)/1, $0xff
DATA tail_mask_table<>+0x11(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x19(SB)/4, $0x00000000
DATA tail_mask_table<>+0x1d(SB)/2, $0x0000
DATA tail_mask_table<>+0x1f(SB)/1, $0x00
DATA tail_mask_table<>+0x20(SB)/1, $0xff
DATA tail_mask_table<>+0x21(SB)/1, $0xff
DATA tail_mask_table<>+0x22(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x2a(SB)/4, $0x00000000
DATA tail_mask_table<>+0x2e(SB)/2, $0x0000
DATA tail_mask_table<>+0x30(SB)/1, $0xff
DATA tail_mask_table<>+0x31(SB)/1, $0xff
DATA tail_mask_table<>+0x32(SB)/1, $0xff
DATA tail_mask_table<>+0x33(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x3b(SB)/4, $0x00000000
DATA tail_mask_table<>+0x3f(SB)/1, $0x00
DATA tail_mask_table<>+0x40(SB)/1, $0xff
DATA tail_mask_table<>+0x41(SB)/1, $0xff
DATA tail_mask_table<>+0x42(SB)/1, $0xff
DATA tail_mask_table<>+0x43(SB)/1, $0xff
DATA tail_mask_table<>+0x44(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x4c(SB)/4, $0x00000000
DATA tail_mask_table<>+0x50(SB)/1, $0xff
DATA tail_mask_table<>+0x51(SB)/1, $0xff
DATA tail_mask_table<>+0x52(SB)/1, $0xff
DATA tail_mask_table<>+0x53(SB)/1, $0xff
DATA tail_mask_table<>+0x54(SB)/1, $0xff
DATA tail_mask_table<>+0x55(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x5d(SB)/2, $0x0000
DATA tail_mask_table<>+0x5f(SB)/1, $0x00
DATA tail_mask_table<>+0x60(SB)/1, $0xff
DATA tail_mask_table<>+0x61(SB)/1, $0xff
DATA tail_mask_table<>+0x62(SB)/1, $0xff
DATA tail_mask_table<>+0x63(SB)/1, $0xff
DATA tail_mask_table<>+0x64(SB)/1, $0xff
DATA tail_mask_table<>+0x65(SB)/1, $0xff
DATA tail_mask_table<>+0x66(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x6e(SB)/2, $0x0000
DATA tail_mask_table<>+0x70(SB)/1, $0xff
DATA tail_mask_table<>+0x71(SB)/1, $0xff
DATA tail_mask_table<>+0x72(SB)/1, $0xff
DATA tail_mask_table<>+0x73(SB)/1, $0xff
DATA tail_mask_table<>+0x74(SB)/1, $0xff
DATA tail_mask_table<>+0x75(SB)/1, $0xff
DATA tail_mask_table<>+0x76(SB)/1, $0xff
DATA tail_mask_table<>+0x77(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x7f(SB)/1, $0x00
DATA tail_mask_table<>+0x80(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0x88(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x90(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0x98(SB)/8, $0x00000000000000ff
DATA tail_mask_table<>+0xa0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xa8(SB)/8, $0x000000000000ffff
DATA tail_mask_table<>+0xb0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xb8(SB)/8, $0x0000000000ffffff
DATA tail_mask_table<>+0xc0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xc8(SB)/8, $0x00000000ffffffff
DATA tail_mask_table<>+0xd0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xd8(SB)/8, $0x000000ffffffffff
DATA tail_mask_table<>+0xe0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xe8(SB)/8, $0x0000ffffffffffff
DATA tail_mask_table<>+0xf0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xf8(SB)/8, $0x00ffffffffffffff
GLOBL tail_mask_table<>(SB), (RODATA|NOPTR), $256
