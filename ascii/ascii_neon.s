//go:build !noasm && arm64
// Code generated by gocc v0.16.5-rev-57c4d32 -- DO NOT EDIT.
//
// Source file         : ascii_neon.c
// Clang version       : Apple clang version 16.0.0 (clang-1600.0.26.6)
// Target architecture : arm64
// Compiler options    : [none]

#include "textflag.h"

DATA LCPI0_0<>+0x00(SB)/8, $0x8040201008040201
DATA LCPI0_0<>+0x08(SB)/8, $0x8040201008040201
GLOBL LCPI0_0<>(SB), (RODATA|NOPTR), $16

TEXT ·indexAnyNeonBitset(SB), NOSPLIT, $0-56
	MOVD  data+0(FP), R0
	MOVD  data_len+8(FP), R1
	MOVD  bitset0+16(FP), R2
	MOVD  bitset1+24(FP), R3
	MOVD  bitset2+32(FP), R4
	MOVD  bitset3+40(FP), R5
	CBZ   R1, LBB0_13        // <--                                  // cbz	x1, .LBB0_13
	NOP                      // (skipped)                            // stp	x29, x30, [sp, #-16]!
	ADD   R1, R0, R9         // <--                                  // add	x9, x0, x1
	AND   $15, R1, R8        // <--                                  // and	x8, x1, #0xf
	SUB   R8, R9, R10        // <--                                  // sub	x10, x9, x8
	MOVD  R0, R9             // <--                                  // mov	x9, x0
	CMP   R0, R10            // <--                                  // cmp	x10, x0
	NOP                      // (skipped)                            // mov	x29, sp
	BLS   LBB0_5             // <--                                  // b.ls	.LBB0_5
	FMOVD R2, F0             // <--                                  // fmov	d0, x2
	FMOVD R3, F2             // <--                                  // fmov	d2, x3
	MOVD  $LCPI0_0<>(SB), R9 // <--                                  // adrp	x9, .LCPI0_0
	VMOV  V2.D[0], V0.D[1]   // <--                                  // mov	v0.d[1], v2.d[0]
	FMOVD R5, F2             // <--                                  // fmov	d2, x5
	FMOVD R4, F1             // <--                                  // fmov	d1, x4
	WORD  $0x3dc00123        // FMOVQ (R9), F3                       // ldr	q3, [x9, :lo12:.LCPI0_0]
	MOVD  R0, R9             // <--                                  // mov	x9, x0
	VMOV  V2.D[0], V1.D[1]   // <--                                  // mov	v1.d[1], v2.d[0]
	WORD  $0x4f00e4e2        // VMOVI $7, V2.B16                     // movi	v2.16b, #7

LBB0_3:
	WORD   $0x3dc00124                      // FMOVQ (R9), F4                       // ldr	q4, [x9]
	WORD   $0x6f0d0485                      // VUSHR $3, V4.B16, V5.B16             // ushr	v5.16b, v4.16b, #3
	VAND   V2.B16, V4.B16, V4.B16           // <--                                  // and	v4.16b, v4.16b, v2.16b
	VTBL   V5.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v5.16b
	VTBL   V4.B16, [V3.B16], V4.B16         // <--                                  // tbl	v4.16b, { v3.16b }, v4.16b
	VCMTST V5.B16, V4.B16, V4.B16           // <--                                  // cmtst	v4.16b, v4.16b, v5.16b
	WORD   $0x0f0c8484                      // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD  F4, R11                          // <--                                  // fmov	x11, d4
	CBNZ   R11, LBB0_14                     // <--                                  // cbnz	x11, .LBB0_14
	ADD    $16, R9, R9                      // <--                                  // add	x9, x9, #16
	CMP    R10, R9                          // <--                                  // cmp	x9, x10
	BCC    LBB0_3                           // <--                                  // b.lo	.LBB0_3

LBB0_5:
	CBZ  R8, LBB0_12 // <--                                  // cbz	x8, .LBB0_12
	MOVD ZR, R10     // <--                                  // mov	x10, xzr

LBB0_7:
	WORD $0x386a692b     // MOVBU (R9)(R10), R11                 // ldrb	w11, [x9, x10]
	MOVD R2, R12         // <--                                  // mov	x12, x2
	LSRW $6, R11, R13    // <--                                  // lsr	w13, w11, #6
	CBZW R13, LBB0_10    // <--                                  // cbz	w13, .LBB0_10
	MOVD R4, R12         // <--                                  // mov	x12, x4
	CMPW $2, R13         // <--                                  // cmp	w13, #2
	BEQ  LBB0_10         // <--                                  // b.eq	.LBB0_10
	CMPW $1, R13         // <--                                  // cmp	w13, #1
	CSEL NE, R5, R3, R12 // <--                                  // csel	x12, x5, x3, ne

LBB0_10:
	LSR  R11, R12, R11    // <--                                  // lsr	x11, x12, x11
	TBNZ $0, R11, LBB0_15 // <--                                  // tbnz	w11, #0, .LBB0_15
	ADD  $1, R10, R10     // <--                                  // add	x10, x10, #1
	CMP  R10, R8          // <--                                  // cmp	x8, x10
	BNE  LBB0_7           // <--                                  // b.ne	.LBB0_7

LBB0_12:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_13:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_14:
	RBIT R11, R8        // <--                                  // rbit	x8, x11
	SUB  R0, R9, R9     // <--                                  // sub	x9, x9, x0
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	ADD  R8>>2, R9, R0  // <--                                  // add	x0, x9, x8, lsr #2
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

LBB0_15:
	SUB  R0, R9, R8     // <--                                  // sub	x8, x9, x0
	ADD  R10, R8, R0    // <--                                  // add	x0, x8, x10
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+48(FP) // <--
	RET                 // <--                                  // ret

TEXT ·ValidString(SB), NOSPLIT, $0-17
	MOVD data+0(FP), R0
	MOVD length+8(FP), R1
	NOP                   // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP  $16, R1          // <--                                  // cmp	x1, #16
	NOP                   // (skipped)                            // mov	x29, sp
	BCC  LBB1_7           // <--                                  // b.lo	.LBB1_7
	ADD  R1, R0, R9       // <--                                  // add	x9, x0, x1
	AND  $63, R1, R8      // <--                                  // and	x8, x1, #0x3f
	SUB  R8, R9, R9       // <--                                  // sub	x9, x9, x8
	CMP  R0, R9           // <--                                  // cmp	x9, x0
	BLS  LBB1_4           // <--                                  // b.ls	.LBB1_4

LBB1_2:
	VLD1  (R0), [V0.B16, V1.B16, V2.B16, V3.B16] // <--                                  // ld1	{ v0.16b, v1.16b, v2.16b, v3.16b }, [x0]
	VORR  V1.B16, V0.B16, V4.B16                 // <--                                  // orr	v4.16b, v0.16b, v1.16b
	VORR  V2.B16, V3.B16, V0.B16                 // <--                                  // orr	v0.16b, v3.16b, v2.16b
	VORR  V0.B16, V4.B16, V0.B16                 // <--                                  // orr	v0.16b, v4.16b, v0.16b
	WORD  $0x4e20a800                            // VCMLT $0, V0.B16, V0.B16             // cmlt	v0.16b, v0.16b, #0
	WORD  $0x0f0c8400                            // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R10                                // <--                                  // fmov	x10, d0
	CBNZ  R10, LBB1_12                           // <--                                  // cbnz	x10, .LBB1_12
	ADD   $64, R0, R0                            // <--                                  // add	x0, x0, #64
	CMP   R9, R0                                 // <--                                  // cmp	x0, x9
	BCC   LBB1_2                                 // <--                                  // b.lo	.LBB1_2

LBB1_4:
	ADD R8, R0, R8  // <--                                  // add	x8, x0, x8
	AND $15, R1, R1 // <--                                  // and	x1, x1, #0xf
	SUB R1, R8, R8  // <--                                  // sub	x8, x8, x1
	CMP R8, R0      // <--                                  // cmp	x0, x8
	BCS LBB1_7      // <--                                  // b.hs	.LBB1_7

LBB1_5:
	WORD  $0x3dc00000 // FMOVQ (R0), F0                       // ldr	q0, [x0]
	WORD  $0x4e20a800 // VCMLT $0, V0.B16, V0.B16             // cmlt	v0.16b, v0.16b, #0
	WORD  $0x0f0c8400 // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R9      // <--                                  // fmov	x9, d0
	CBNZ  R9, LBB1_12 // <--                                  // cbnz	x9, .LBB1_12
	ADD   $16, R0, R0 // <--                                  // add	x0, x0, #16
	CMP   R8, R0      // <--                                  // cmp	x0, x8
	BCC   LBB1_5      // <--                                  // b.lo	.LBB1_5

LBB1_7:
	CMP   $8, R1          // <--                                  // cmp	x1, #8
	BCS   LBB1_11         // <--                                  // b.hs	.LBB1_11
	TBNZ  $2, R1, LBB1_13 // <--                                  // tbnz	w1, #2, .LBB1_13
	CBZ   R1, LBB1_14     // <--                                  // cbz	x1, .LBB1_14
	LSR   $1, R1, R8      // <--                                  // lsr	x8, x1, #1
	ADD   R1, R0, R9      // <--                                  // add	x9, x0, x1
	WORD  $0x3940000a     // MOVBU (R0), R10                      // ldrb	w10, [x0]
	WORD  $0x38686808     // MOVBU (R0)(R8), R8                   // ldrb	w8, [x0, x8]
	WORD  $0x385ff129     // LDURBW -1(R9), R9                    // ldurb	w9, [x9, #-1]
	ORRW  R9, R10, R9     // <--                                  // orr	w9, w10, w9
	ORRW  R9, R8, R8      // <--                                  // orr	w8, w8, w9
	SXTBW R8, R8          // <--                                  // sxtb	w8, w8
	CMPW  $0, R8          // <--                                  // cmp	w8, #0
	CSETW GE, R0          // <--                                  // cset	w0, ge
	NOP                   // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP)  // <--
	RET                   // <--                                  // ret

LBB1_11:
	ADD   R1, R0, R8                // <--                                  // add	x8, x0, x1
	WORD  $0xf9400009               // MOVD (R0), R9                        // ldr	x9, [x0]
	WORD  $0xf85f8108               // MOVD -8(R8), R8                      // ldur	x8, [x8, #-8]
	ORR   R9, R8, R8                // <--                                  // orr	x8, x8, x9
	TST   $-9187201950435737472, R8 // <--                                  // tst	x8, #0x8080808080808080
	CSETW EQ, R0                    // <--                                  // cset	w0, eq
	NOP                             // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP)            // <--
	RET                             // <--                                  // ret

LBB1_12:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+16(FP) // <--
	RET                 // <--                                  // ret

LBB1_13:
	ADD   R1, R0, R8      // <--                                  // add	x8, x0, x1
	WORD  $0xb9400009     // MOVWU (R0), R9                       // ldr	w9, [x0]
	WORD  $0xb85fc108     // MOVWU -4(R8), R8                     // ldur	w8, [x8, #-4]
	ORRW  R9, R8, R8      // <--                                  // orr	w8, w8, w9
	TSTW  $2155905152, R8 // <--                                  // tst	w8, #0x80808080
	CSETW EQ, R0          // <--                                  // cset	w0, eq
	NOP                   // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+16(FP)  // <--
	RET                   // <--                                  // ret

LBB1_14:
	MOVW $1, R0         // <--                                  // mov	w0, #1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+16(FP) // <--
	RET                 // <--                                  // ret

TEXT ·IndexMask(SB), NOSPLIT, $0-32
	MOVD data+0(FP), R0
	MOVD length+8(FP), R1
	MOVB mask+16(FP), R2
	NOP                   // (skipped)                            // stp	x29, x30, [sp, #-16]!
	CMP  $16, R1          // <--                                  // cmp	x1, #16
	NOP                   // (skipped)                            // mov	x29, sp
	BCC  LBB2_10          // <--                                  // b.lo	.LBB2_10
	ADD  R1, R0, R8       // <--                                  // add	x8, x0, x1
	AND  $63, R1, R10     // <--                                  // and	x10, x1, #0x3f
	SUB  R10, R8, R11     // <--                                  // sub	x11, x8, x10
	MOVD R0, R8           // <--                                  // mov	x8, x0
	VDUP R2, V0.B16       // <--                                  // dup	v0.16b, w2
	CMP  R0, R11          // <--                                  // cmp	x11, x0
	BLS  LBB2_14          // <--                                  // b.ls	.LBB2_14
	MOVW $16, R9          // <--                                  // mov	w9, #16
	MOVD R0, R8           // <--                                  // mov	x8, x0
	JMP  LBB2_4           // <--                                  // b	.LBB2_4

LBB2_3:
	ADD $64, R8, R8 // <--                                  // add	x8, x8, #64
	CMP R11, R8     // <--                                  // cmp	x8, x11
	BCS LBB2_14     // <--                                  // b.hs	.LBB2_14

LBB2_4:
	VLD1   (R8), [V1.B16, V2.B16, V3.B16, V4.B16] // <--                                  // ld1	{ v1.16b, v2.16b, v3.16b, v4.16b }, [x8]
	VORR   V1.B16, V2.B16, V5.B16                 // <--                                  // orr	v5.16b, v2.16b, v1.16b
	VORR   V4.B16, V3.B16, V6.B16                 // <--                                  // orr	v6.16b, v3.16b, v4.16b
	VORR   V6.B16, V5.B16, V5.B16                 // <--                                  // orr	v5.16b, v5.16b, v6.16b
	VCMTST V0.B16, V5.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v5.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBZ    R12, LBB2_3                            // <--                                  // cbz	x12, .LBB2_3
	VCMTST V0.B16, V1.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v1.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBNZ   R12, LBB2_32                           // <--                                  // cbnz	x12, .LBB2_32
	VCMTST V0.B16, V2.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v2.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBNZ   R12, LBB2_34                           // <--                                  // cbnz	x12, .LBB2_34
	VCMTST V0.B16, V3.B16, V5.B16                 // <--                                  // cmtst	v5.16b, v3.16b, v0.16b
	WORD   $0x0f0c84a5                            // VSHRN $4, V5.H8, V5.B8               // shrn	v5.8b, v5.8h, #4
	FMOVD  F5, R12                                // <--                                  // fmov	x12, d5
	CBNZ   R12, LBB2_33                           // <--                                  // cbnz	x12, .LBB2_33
	VCMTST V0.B16, V4.B16, V1.B16                 // <--                                  // cmtst	v1.16b, v4.16b, v0.16b
	WORD   $0x0f0c8421                            // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD  F1, R12                                // <--                                  // fmov	x12, d1
	CBZ    R12, LBB2_3                            // <--                                  // cbz	x12, .LBB2_3
	MOVW   $48, R9                                // <--                                  // mov	w9, #48
	JMP    LBB2_34                                // <--                                  // b	.LBB2_34

LBB2_10:
	MOVD R0, R8 // <--                                  // mov	x8, x0

LBB2_11:
	ANDW $255, R2, R9    // <--                                  // and	w9, w2, #0xff
	MOVW $16843009, R10  // <--                                  // mov	w10, #16843009
	MULW R10, R9, R9     // <--                                  // mul	w9, w9, w10
	SUBS $8, R1, R10     // <--                                  // subs	x10, x1, #8
	BCC  LBB2_18         // <--                                  // b.lo	.LBB2_18
	WORD $0xf940010b     // MOVD (R8), R11                       // ldr	x11, [x8]
	ORR  R9<<32, R9, R12 // <--                                  // orr	x12, x9, x9, lsl #32
	ANDS R12, R11, R11   // <--                                  // ands	x11, x11, x12
	BEQ  LBB2_17         // <--                                  // b.eq	.LBB2_17
	RBIT R11, R9         // <--                                  // rbit	x9, x11
	SUB  R0, R8, R8      // <--                                  // sub	x8, x8, x0
	CLZ  R9, R9          // <--                                  // clz	x9, x9
	ADD  R9>>3, R8, R0   // <--                                  // add	x0, x8, x9, lsr #3
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP)  // <--
	RET                  // <--                                  // ret

LBB2_14:
	ADD R10, R8, R9 // <--                                  // add	x9, x8, x10
	AND $15, R1, R1 // <--                                  // and	x1, x1, #0xf
	SUB R1, R9, R9  // <--                                  // sub	x9, x9, x1
	CMP R9, R8      // <--                                  // cmp	x8, x9
	BCS LBB2_11     // <--                                  // b.hs	.LBB2_11

LBB2_15:
	WORD   $0x3dc00101            // FMOVQ (R8), F1                       // ldr	q1, [x8]
	VCMTST V0.B16, V1.B16, V1.B16 // <--                                  // cmtst	v1.16b, v1.16b, v0.16b
	WORD   $0x0f0c8421            // VSHRN $4, V1.H8, V1.B8               // shrn	v1.8b, v1.8h, #4
	FMOVD  F1, R10                // <--                                  // fmov	x10, d1
	CBNZ   R10, LBB2_31           // <--                                  // cbnz	x10, .LBB2_31
	ADD    $16, R8, R8            // <--                                  // add	x8, x8, #16
	CMP    R9, R8                 // <--                                  // cmp	x8, x9
	BCC    LBB2_15                // <--                                  // b.lo	.LBB2_15
	JMP    LBB2_11                // <--                                  // b	.LBB2_11

LBB2_17:
	ADD  $8, R8, R8 // <--                                  // add	x8, x8, #8
	MOVD R10, R1    // <--                                  // mov	x1, x10

LBB2_18:
	SUBS  $4, R1, R10    // <--                                  // subs	x10, x1, #4
	BCC   LBB2_22        // <--                                  // b.lo	.LBB2_22
	WORD  $0xb940010b    // MOVWU (R8), R11                      // ldr	w11, [x8]
	ANDSW R9, R11, R11   // <--                                  // ands	w11, w11, w9
	BEQ   LBB2_21        // <--                                  // b.eq	.LBB2_21
	RBITW R11, R9        // <--                                  // rbit	w9, w11
	CLZW  R9, R9         // <--                                  // clz	w9, w9
	SUB   R0, R8, R8     // <--                                  // sub	x8, x8, x0
	LSRW  $3, R9, R9     // <--                                  // lsr	w9, w9, #3
	ADD   R9, R8, R0     // <--                                  // add	x0, x8, x9
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD  R0, ret+24(FP) // <--
	RET                  // <--                                  // ret

LBB2_21:
	ADD  $4, R8, R8 // <--                                  // add	x8, x8, #4
	MOVD R10, R1    // <--                                  // mov	x1, x10

LBB2_22:
	CMP   $1, R1            // <--                                  // cmp	x1, #1
	BEQ   LBB2_26           // <--                                  // b.eq	.LBB2_26
	CMP   $2, R1            // <--                                  // cmp	x1, #2
	BEQ   LBB2_27           // <--                                  // b.eq	.LBB2_27
	CMP   $3, R1            // <--                                  // cmp	x1, #3
	BNE   LBB2_29           // <--                                  // b.ne	.LBB2_29
	WORD  $0x7940010a       // MOVHU (R8), R10                      // ldrh	w10, [x8]
	WORD  $0x3940090b       // MOVBU 2(R8), R11                     // ldrb	w11, [x8, #2]
	ORRW  R11<<16, R10, R10 // <--                                  // orr	w10, w10, w11, lsl #16
	ANDSW R9, R10, R9       // <--                                  // ands	w9, w10, w9
	BNE   LBB2_28           // <--                                  // b.ne	.LBB2_28
	JMP   LBB2_30           // <--                                  // b	.LBB2_30

LBB2_26:
	WORD  $0x3940010a // MOVBU (R8), R10                      // ldrb	w10, [x8]
	ANDSW R9, R10, R9 // <--                                  // ands	w9, w10, w9
	BNE   LBB2_28     // <--                                  // b.ne	.LBB2_28
	JMP   LBB2_30     // <--                                  // b	.LBB2_30

LBB2_27:
	WORD  $0x7940010a // MOVHU (R8), R10                      // ldrh	w10, [x8]
	ANDSW R9, R10, R9 // <--                                  // ands	w9, w10, w9
	BEQ   LBB2_30     // <--                                  // b.eq	.LBB2_30

LBB2_28:
	RBITW R9, R9         // <--                                  // rbit	w9, w9
	CLZW  R9, R9         // <--                                  // clz	w9, w9
	SUB   R0, R8, R8     // <--                                  // sub	x8, x8, x0
	LSRW  $3, R9, R9     // <--                                  // lsr	w9, w9, #3
	ADD   R9, R8, R0     // <--                                  // add	x0, x8, x9
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD  R0, ret+24(FP) // <--
	RET                  // <--                                  // ret

LBB2_29:
	MOVW  ZR, R10    // <--                                  // mov	w10, wzr
	ANDSW R9, ZR, R9 // <--                                  // ands	w9, wzr, w9
	BNE   LBB2_28    // <--                                  // b.ne	.LBB2_28

LBB2_30:
	MOVD $-1, R0        // <--                                  // mov	x0, #-1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

LBB2_31:
	RBIT R10, R9        // <--                                  // rbit	x9, x10
	SUB  R0, R8, R8     // <--                                  // sub	x8, x8, x0
	CLZ  R9, R9         // <--                                  // clz	x9, x9
	ADD  R9>>2, R8, R0  // <--                                  // add	x0, x8, x9, lsr #2
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

LBB2_32:
	MOVD ZR, R9  // <--                                  // mov	x9, xzr
	JMP  LBB2_34 // <--                                  // b	.LBB2_34

LBB2_33:
	MOVW $32, R9 // <--                                  // mov	w9, #32

LBB2_34:
	RBIT R12, R10       // <--                                  // rbit	x10, x12
	SUB  R0, R8, R8     // <--                                  // sub	x8, x8, x0
	CLZ  R10, R10       // <--                                  // clz	x10, x10
	ORR  R10>>2, R9, R9 // <--                                  // orr	x9, x9, x10, lsr #2
	ADD  R9, R8, R0     // <--                                  // add	x0, x8, x9
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVD R0, ret+24(FP) // <--
	RET                 // <--                                  // ret

TEXT ·EqualFold(SB), NOSPLIT, $0-33
	MOVD a+0(FP), R0
	MOVD a_len+8(FP), R1
	MOVD b+16(FP), R2
	MOVD b_len+24(FP), R3
	CMP  R3, R1                      // <--                                  // cmp	x1, x3
	BNE  LBB3_9                      // <--                                  // b.ne	.LBB3_9
	TBNZ $63, R1, LBB3_9             // <--                                  // tbnz	x1, #63, .LBB3_9
	NOP                              // (skipped)                            // stp	x29, x30, [sp, #-16]!
	MOVD $uppercasingTable<>(SB), R8 // <--                                  // adrp	x8, uppercasingTable
	NOP                              // (skipped)                            // add	x8, x8, :lo12:uppercasingTable
	ADD  R1, R0, R9                  // <--                                  // add	x9, x0, x1
	NOP                              // (skipped)                            // mov	x29, sp
	VLD1 (R8), [V0.B16, V1.B16]      // <--                                  // ld1	{ v0.16b, v1.16b }, [x8]
	AND  $15, R1, R8                 // <--                                  // and	x8, x1, #0xf
	SUB  R8, R9, R9                  // <--                                  // sub	x9, x9, x8
	CMP  R0, R9                      // <--                                  // cmp	x9, x0
	BLS  LBB3_6                      // <--                                  // b.ls	.LBB3_6
	WORD $0x4f05e402                 // VMOVI $160, V2.B16                   // movi	v2.16b, #160

LBB3_4:
	WORD  $0x3dc00003                      // FMOVQ (R0), F3                       // ldr	q3, [x0]
	WORD  $0x3dc00044                      // FMOVQ (R2), F4                       // ldr	q4, [x2]
	VADD  V2.B16, V3.B16, V3.B16           // <--                                  // add	v3.16b, v3.16b, v2.16b
	VADD  V2.B16, V4.B16, V4.B16           // <--                                  // add	v4.16b, v4.16b, v2.16b
	VTBL  V3.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v3.16b
	VTBL  V4.B16, [V0.B16, V1.B16], V6.B16 // <--                                  // tbl	v6.16b, { v0.16b, v1.16b }, v4.16b
	VSUB  V5.B16, V3.B16, V3.B16           // <--                                  // sub	v3.16b, v3.16b, v5.16b
	VSUB  V6.B16, V4.B16, V4.B16           // <--                                  // sub	v4.16b, v4.16b, v6.16b
	VCMEQ V4.B16, V3.B16, V3.B16           // <--                                  // cmeq	v3.16b, v3.16b, v4.16b
	WORD  $0x0f0c8463                      // VSHRN $4, V3.H8, V3.B8               // shrn	v3.8b, v3.8h, #4
	FMOVD F3, R10                          // <--                                  // fmov	x10, d3
	CMN   $1, R10                          // <--                                  // cmn	x10, #1
	BNE   LBB3_8                           // <--                                  // b.ne	.LBB3_8
	ADD   $16, R0, R0                      // <--                                  // add	x0, x0, #16
	ADD   $16, R2, R2                      // <--                                  // add	x2, x2, #16
	CMP   R9, R0                           // <--                                  // cmp	x0, x9
	BCC   LBB3_4                           // <--                                  // b.lo	.LBB3_4

LBB3_6:
	CMP   $8, R8                         // <--                                  // cmp	x8, #8
	BCC   LBB3_11                        // <--                                  // b.lo	.LBB3_11
	WORD  $0x0f05e403                    // VMOVI $160, V3.B8                    // movi	v3.8b, #160
	WORD  $0xfc408402                    // FMOVD.P 8(R0), F2                    // ldr	d2, [x0], #8
	WORD  $0xfc408444                    // FMOVD.P 8(R2), F4                    // ldr	d4, [x2], #8
	VADD  V3.B8, V2.B8, V2.B8            // <--                                  // add	v2.8b, v2.8b, v3.8b
	VADD  V3.B8, V4.B8, V3.B8            // <--                                  // add	v3.8b, v4.8b, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V5.B8 // <--                                  // tbl	v5.8b, { v0.16b, v1.16b }, v3.8b
	VSUB  V4.B8, V2.B8, V2.B8            // <--                                  // sub	v2.8b, v2.8b, v4.8b
	VSUB  V5.B8, V3.B8, V3.B8            // <--                                  // sub	v3.8b, v3.8b, v5.8b
	VCMEQ V3.B8, V2.B8, V2.B8            // <--                                  // cmeq	v2.8b, v2.8b, v3.8b
	FMOVD F2, R8                         // <--                                  // fmov	x8, d2
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	BEQ   LBB3_10                        // <--                                  // b.eq	.LBB3_10

LBB3_8:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_9:
	MOVW ZR, R0         // <--                                  // mov	w0, wzr
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_10:
	AND $7, R1, R8 // <--                                  // and	x8, x1, #0x7

LBB3_11:
	CBZ  R8, LBB3_17 // <--                                  // cbz	x8, .LBB3_17
	SUBS $4, R8, R11 // <--                                  // subs	x11, x8, #4
	BCC  LBB3_18     // <--                                  // b.lo	.LBB3_18
	WORD $0xb8404409 // MOVWU.P 4(R0), R9                    // ldr	w9, [x0], #4
	WORD $0xb840444a // MOVWU.P 4(R2), R10                   // ldr	w10, [x2], #4
	MOVD R11, R8     // <--                                  // mov	x8, x11
	CMP  $1, R11     // <--                                  // cmp	x11, #1
	BEQ  LBB3_19     // <--                                  // b.eq	.LBB3_19

LBB3_14:
	CMP  $2, R8           // <--                                  // cmp	x8, #2
	BEQ  LBB3_20          // <--                                  // b.eq	.LBB3_20
	CMP  $3, R8           // <--                                  // cmp	x8, #3
	BNE  LBB3_21          // <--                                  // b.ne	.LBB3_21
	WORD $0x79400008      // MOVHU (R0), R8                       // ldrh	w8, [x0]
	LSL  $24, R9, R9      // <--                                  // lsl	x9, x9, #24
	WORD $0x7940004c      // MOVHU (R2), R12                      // ldrh	w12, [x2]
	LSL  $24, R10, R10    // <--                                  // lsl	x10, x10, #24
	WORD $0x3940080b      // MOVBU 2(R0), R11                     // ldrb	w11, [x0, #2]
	WORD $0x3940084d      // MOVBU 2(R2), R13                     // ldrb	w13, [x2, #2]
	ORR  R8<<8, R9, R8    // <--                                  // orr	x8, x9, x8, lsl #8
	ORR  R12<<8, R10, R10 // <--                                  // orr	x10, x10, x12, lsl #8
	ORR  R11, R8, R9      // <--                                  // orr	x9, x8, x11
	ORR  R13, R10, R10    // <--                                  // orr	x10, x10, x13
	JMP  LBB3_21          // <--                                  // b	.LBB3_21

LBB3_17:
	MOVW $1, R0         // <--                                  // mov	w0, #1
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB3_18:
	MOVD ZR, R10 // <--                                  // mov	x10, xzr
	MOVD ZR, R9  // <--                                  // mov	x9, xzr
	CMP  $1, R8  // <--                                  // cmp	x8, #1
	BNE  LBB3_14 // <--                                  // b.ne	.LBB3_14

LBB3_19:
	WORD $0x39400008      // MOVBU (R0), R8                       // ldrb	w8, [x0]
	WORD $0x3940004b      // MOVBU (R2), R11                      // ldrb	w11, [x2]
	ORR  R9<<8, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #8
	ORR  R10<<8, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #8
	JMP  LBB3_21          // <--                                  // b	.LBB3_21

LBB3_20:
	WORD $0x79400008       // MOVHU (R0), R8                       // ldrh	w8, [x0]
	WORD $0x7940004b       // MOVHU (R2), R11                      // ldrh	w11, [x2]
	ORR  R9<<16, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #16
	ORR  R10<<16, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #16

LBB3_21:
	WORD  $0x0f05e402                    // VMOVI $160, V2.B8                    // movi	v2.8b, #160
	FMOVD R9, F3                         // <--                                  // fmov	d3, x9
	FMOVD R10, F4                        // <--                                  // fmov	d4, x10
	VADD  V2.B8, V3.B8, V3.B8            // <--                                  // add	v3.8b, v3.8b, v2.8b
	VADD  V2.B8, V4.B8, V2.B8            // <--                                  // add	v2.8b, v4.8b, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V0.B8 // <--                                  // tbl	v0.8b, { v0.16b, v1.16b }, v2.8b
	VSUB  V4.B8, V3.B8, V1.B8            // <--                                  // sub	v1.8b, v3.8b, v4.8b
	VSUB  V0.B8, V2.B8, V0.B8            // <--                                  // sub	v0.8b, v2.8b, v0.8b
	VCMEQ V0.B8, V1.B8, V0.B8            // <--                                  // cmeq	v0.8b, v1.8b, v0.8b
	FMOVD F0, R8                         // <--                                  // fmov	x8, d0
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	CSETW EQ, R0                         // <--                                  // cset	w0, eq
	NOP                                  // (skipped)                            // ldp	x29, x30, [sp], #16
	MOVB  R0, ret+32(FP)                 // <--
	RET                                  // <--                                  // ret

TEXT ·indexFoldRabinKarp(SB), NOSPLIT, $0-40
	MOVD haystack+0(FP), R0
	MOVD haystack_len+8(FP), R1
	MOVD needle+16(FP), R2
	MOVD needle_len+24(FP), R3
	CMP  R3, R1                 // <--                                  // cmp	x1, x3
	BGE  LBB4_2                 // <--                                  // b.ge	.LBB4_2
	MOVD $-1, R0                // <--                                  // mov	x0, #-1
	MOVD R0, ret+32(FP)         // <--
	RET                         // <--                                  // ret

LBB4_2:
	CBZ  R3, LBB4_12                 // <--                                  // cbz	x3, .LBB4_12
	NOP                              // (skipped)                            // stp	x29, x30, [sp, #-48]!
	MOVD $uppercasingTable<>(SB), R8 // <--                                  // adrp	x8, uppercasingTable
	NOP                              // (skipped)                            // add	x8, x8, :lo12:uppercasingTable
	CMP  R3, R1                      // <--                                  // cmp	x1, x3
	NOP                              // (skipped)                            // str	x21, [sp, #16]
	NOP                              // (skipped)                            // stp	x20, x19, [sp, #32]
	NOP                              // (skipped)                            // mov	x29, sp
	VLD1 (R8), [V0.B16, V1.B16]      // <--                                  // ld1	{ v0.16b, v1.16b }, [x8]
	BNE  LBB4_13                     // <--                                  // b.ne	.LBB4_13
	TBNZ $63, R1, LBB4_104           // <--                                  // tbnz	x1, #63, .LBB4_104
	ADD  R1, R0, R9                  // <--                                  // add	x9, x0, x1
	AND  $15, R1, R8                 // <--                                  // and	x8, x1, #0xf
	SUB  R8, R9, R9                  // <--                                  // sub	x9, x9, x8
	CMP  R0, R9                      // <--                                  // cmp	x9, x0
	BLS  LBB4_9                      // <--                                  // b.ls	.LBB4_9
	WORD $0x4f05e402                 // VMOVI $160, V2.B16                   // movi	v2.16b, #160

LBB4_7:
	WORD  $0x3dc00003                      // FMOVQ (R0), F3                       // ldr	q3, [x0]
	WORD  $0x3dc00044                      // FMOVQ (R2), F4                       // ldr	q4, [x2]
	VADD  V2.B16, V3.B16, V3.B16           // <--                                  // add	v3.16b, v3.16b, v2.16b
	VADD  V2.B16, V4.B16, V4.B16           // <--                                  // add	v4.16b, v4.16b, v2.16b
	VTBL  V3.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v3.16b
	VTBL  V4.B16, [V0.B16, V1.B16], V6.B16 // <--                                  // tbl	v6.16b, { v0.16b, v1.16b }, v4.16b
	VSUB  V5.B16, V3.B16, V3.B16           // <--                                  // sub	v3.16b, v3.16b, v5.16b
	VSUB  V6.B16, V4.B16, V4.B16           // <--                                  // sub	v4.16b, v4.16b, v6.16b
	VCMEQ V4.B16, V3.B16, V3.B16           // <--                                  // cmeq	v3.16b, v3.16b, v4.16b
	WORD  $0x0f0c8463                      // VSHRN $4, V3.H8, V3.B8               // shrn	v3.8b, v3.8h, #4
	FMOVD F3, R10                          // <--                                  // fmov	x10, d3
	CMN   $1, R10                          // <--                                  // cmn	x10, #1
	BNE   LBB4_104                         // <--                                  // b.ne	.LBB4_104
	ADD   $16, R0, R0                      // <--                                  // add	x0, x0, #16
	ADD   $16, R2, R2                      // <--                                  // add	x2, x2, #16
	CMP   R9, R0                           // <--                                  // cmp	x0, x9
	BCC   LBB4_7                           // <--                                  // b.lo	.LBB4_7

LBB4_9:
	CMP   $8, R8                         // <--                                  // cmp	x8, #8
	BCC   LBB4_65                        // <--                                  // b.lo	.LBB4_65
	WORD  $0x0f05e403                    // VMOVI $160, V3.B8                    // movi	v3.8b, #160
	WORD  $0xfc408402                    // FMOVD.P 8(R0), F2                    // ldr	d2, [x0], #8
	WORD  $0xfc408444                    // FMOVD.P 8(R2), F4                    // ldr	d4, [x2], #8
	VADD  V3.B8, V2.B8, V2.B8            // <--                                  // add	v2.8b, v2.8b, v3.8b
	VADD  V3.B8, V4.B8, V3.B8            // <--                                  // add	v3.8b, v4.8b, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V5.B8 // <--                                  // tbl	v5.8b, { v0.16b, v1.16b }, v3.8b
	VSUB  V4.B8, V2.B8, V2.B8            // <--                                  // sub	v2.8b, v2.8b, v4.8b
	VSUB  V5.B8, V3.B8, V3.B8            // <--                                  // sub	v3.8b, v3.8b, v5.8b
	VCMEQ V3.B8, V2.B8, V2.B8            // <--                                  // cmeq	v2.8b, v2.8b, v3.8b
	FMOVD F2, R8                         // <--                                  // fmov	x8, d2
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	BEQ   LBB4_64                        // <--                                  // b.eq	.LBB4_64
	MOVD  $-1, R8                        // <--                                  // mov	x8, #-1
	JMP   LBB4_128                       // <--                                  // b	.LBB4_128

LBB4_12:
	MOVD ZR, R0         // <--                                  // mov	x0, xzr
	MOVD R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB4_13:
	CMP  $1, R3                           // <--                                  // cmp	x3, #1
	BEQ  LBB4_39                          // <--                                  // b.eq	.LBB4_39
	CMP  $2, R3                           // <--                                  // cmp	x3, #2
	BNE  LBB4_46                          // <--                                  // b.ne	.LBB4_46
	ADD  R1, R0, R8                       // <--                                  // add	x8, x0, x1
	SUB  $1, R8, R9                       // <--                                  // sub	x9, x8, #1
	CMP  R0, R9                           // <--                                  // cmp	x9, x0
	BCC  LBB4_104                         // <--                                  // b.lo	.LBB4_104
	WORD $0x4d40c443                      // VLD1R (R2), [V3.H8]                  // ld1r	{ v3.8h }, [x2]
	MOVD ZR, R10                          // <--                                  // mov	x10, xzr
	WORD $0x4f05e402                      // VMOVI $160, V2.B16                   // movi	v2.16b, #160
	LSL  $3, R1, R11                      // <--                                  // lsl	x11, x1, #3
	WORD $0x4f078604                      // VMOVI $240, V4.H8                    // movi	v4.8h, #240
	MOVD R1, R14                          // <--                                  // mov	x14, x1
	WORD $0x6f00e406                      // VMOVI $0, V6.D2                      // movi	v6.2d, #0000000000000000
	MOVD R0, R12                          // <--                                  // mov	x12, x0
	VADD V2.B16, V3.B16, V5.B16           // <--                                  // add	v5.16b, v3.16b, v2.16b
	WORD $0x6f00e403                      // VMOVI $0, V3.D2                      // movi	v3.2d, #0000000000000000
	VTBL V5.B16, [V0.B16, V1.B16], V7.B16 // <--                                  // tbl	v7.16b, { v0.16b, v1.16b }, v5.16b
	VSUB V7.B16, V5.B16, V5.B16           // <--                                  // sub	v5.16b, v5.16b, v7.16b

LBB4_17:
	ADD  R10, R0, R8   // <--                                  // add	x8, x0, x10
	SUBS $16, R14, R13 // <--                                  // subs	x13, x14, #16
	BLT  LBB4_19       // <--                                  // b.lt	.LBB4_19
	WORD $0x3dc00107   // FMOVQ (R8), F7                       // ldr	q7, [x8]
	JMP  LBB4_37       // <--                                  // b	.LBB4_37

LBB4_19:
	CMP  $8, R14     // <--                                  // cmp	x14, #8
	BNE  LBB4_21     // <--                                  // b.ne	.LBB4_21
	WORD $0xfd400107 // FMOVD (R8), F7                       // ldr	d7, [x8]
	JMP  LBB4_37     // <--                                  // b	.LBB4_37

LBB4_21:
	CMP  $1, R14         // <--                                  // cmp	x14, #1
	BLT  LBB4_25         // <--                                  // b.lt	.LBB4_25
	TBNZ $3, R1, LBB4_26 // <--                                  // tbnz	w1, #3, .LBB4_26
	MOVD R14, R2         // <--                                  // mov	x2, x14
	MOVD R8, R17         // <--                                  // mov	x17, x8
	TBZ  $2, R2, LBB4_27 // <--                                  // tbz	w2, #2, .LBB4_27

LBB4_24:
	WORD $0xb8404630 // MOVWU.P 4(R17), R16                  // ldr	w16, [x17], #4
	SUB  $4, R2, R2  // <--                                  // sub	x2, x2, #4
	JMP  LBB4_28     // <--                                  // b	.LBB4_28

LBB4_25:
	WORD $0x6f00e407 // VMOVI $0, V7.D2                      // movi	v7.2d, #0000000000000000
	JMP  LBB4_37     // <--                                  // b	.LBB4_37

LBB4_26:
	SUB  R10, R1, R16    // <--                                  // sub	x16, x1, x10
	WORD $0xf940010f     // MOVD (R8), R15                       // ldr	x15, [x8]
	ADD  $8, R12, R17    // <--                                  // add	x17, x12, #8
	SUB  $8, R16, R2     // <--                                  // sub	x2, x16, #8
	TBNZ $2, R2, LBB4_24 // <--                                  // tbnz	w2, #2, .LBB4_24

LBB4_27:
	MOVD ZR, R16 // <--                                  // mov	x16, xzr

LBB4_28:
	CMP  $1, R2       // <--                                  // cmp	x2, #1
	BEQ  LBB4_32      // <--                                  // b.eq	.LBB4_32
	CMP  $2, R2       // <--                                  // cmp	x2, #2
	BEQ  LBB4_33      // <--                                  // b.eq	.LBB4_33
	CMP  $3, R2       // <--                                  // cmp	x2, #3
	BNE  LBB4_36      // <--                                  // b.ne	.LBB4_36
	ANDW $32, R11, R3 // <--                                  // and	w3, w11, #0x20
	WORD $0x79400222  // MOVHU (R17), R2                      // ldrh	w2, [x17]
	WORD $0x39400a31  // MOVBU 2(R17), R17                    // ldrb	w17, [x17, #2]
	ORRW $16, R3, R4  // <--                                  // orr	w4, w3, #0x10
	LSL  R3, R2, R2   // <--                                  // lsl	x2, x2, x3
	LSL  R4, R17, R17 // <--                                  // lsl	x17, x17, x4
	ORR  R17, R2, R17 // <--                                  // orr	x17, x2, x17
	JMP  LBB4_35      // <--                                  // b	.LBB4_35

LBB4_32:
	WORD $0x39400231 // MOVBU (R17), R17                     // ldrb	w17, [x17]
	JMP  LBB4_34     // <--                                  // b	.LBB4_34

LBB4_33:
	WORD $0x79400231 // MOVHU (R17), R17                     // ldrh	w17, [x17]

LBB4_34:
	AND $32, R11, R2 // <--                                  // and	x2, x11, #0x20
	LSL R2, R17, R17 // <--                                  // lsl	x17, x17, x2

LBB4_35:
	ORR R16, R17, R16 // <--                                  // orr	x16, x17, x16

LBB4_36:
	FMOVD  R15, F7           // <--                                  // fmov	d7, x15
	FMOVD  R16, F16          // <--                                  // fmov	d16, x16
	CMP    $7, R14           // <--                                  // cmp	x14, #7
	FCSELD HI, F16, F3, F17  // <--                                  // fcsel	d17, d16, d3, hi
	FCSELD HI, F7, F16, F7   // <--                                  // fcsel	d7, d7, d16, hi
	VMOV   V17.D[0], V7.D[1] // <--                                  // mov	v7.d[1], v17.d[0]

LBB4_37:
	VADD  V2.B16, V7.B16, V7.B16            // <--                                  // add	v7.16b, v7.16b, v2.16b
	VTBL  V7.B16, [V0.B16, V1.B16], V16.B16 // <--                                  // tbl	v16.16b, { v0.16b, v1.16b }, v7.16b
	VSUB  V16.B16, V7.B16, V7.B16           // <--                                  // sub	v7.16b, v7.16b, v16.16b
	VEXT  $15, V7.B16, V6.B16, V6.B16       // <--                                  // ext	v6.16b, v6.16b, v7.16b, #15
	VCMEQ V5.H8, V7.H8, V16.H8              // <--                                  // cmeq	v16.8h, v7.8h, v5.8h
	VSHL  $8, V16.H8, V16.H8                // <--                                  // shl	v16.8h, v16.8h, #8
	VCMEQ V5.H8, V6.H8, V6.H8               // <--                                  // cmeq	v6.8h, v6.8h, v5.8h
	VAND  V4.B16, V6.B16, V6.B16            // <--                                  // and	v6.16b, v6.16b, v4.16b
	VORR  V6.B16, V16.B16, V6.B16           // <--                                  // orr	v6.16b, v16.16b, v6.16b
	WORD  $0x0f0c84c6                       // VSHRN $4, V6.H8, V6.B8               // shrn	v6.8b, v6.8h, #4
	FMOVD F6, R14                           // <--                                  // fmov	x14, d6
	CMP   $0, R14                           // <--                                  // cmp	x14, #0
	AND   $-16, R14, R15                    // <--                                  // and	x15, x14, #0xfffffffffffffff0
	CCMP  NE, R10, $0, $0                   // <--                                  // ccmp	x10, #0, #0, ne
	CSEL  EQ, R15, R14, R14                 // <--                                  // csel	x14, x15, x14, eq
	CBNZ  R14, LBB4_73                      // <--                                  // cbnz	x14, .LBB4_73
	ADD   $16, R12, R12                     // <--                                  // add	x12, x12, #16
	ADD   $16, R10, R10                     // <--                                  // add	x10, x10, #16
	SUB   $128, R11, R11                    // <--                                  // sub	x11, x11, #128
	MOVD  $-1, R8                           // <--                                  // mov	x8, #-1
	MOVD  R13, R14                          // <--                                  // mov	x14, x13
	ADD   R10, R0, R15                      // <--                                  // add	x15, x0, x10
	VMOV  V7.B16, V6.B16                    // <--                                  // mov	v6.16b, v7.16b
	CMP   R9, R15                           // <--                                  // cmp	x15, x9
	BLS   LBB4_17                           // <--                                  // b.ls	.LBB4_17
	JMP   LBB4_128                          // <--                                  // b	.LBB4_128

LBB4_39:
	WORD  $0x39400048     // MOVBU (R2), R8                       // ldrb	w8, [x2]
	SUBW  $97, R8, R9     // <--                                  // sub	w9, w8, #97
	SUBW  $32, R8, R10    // <--                                  // sub	w10, w8, #32
	CMPW  $26, R9         // <--                                  // cmp	w9, #26
	ADD   R1, R0, R9      // <--                                  // add	x9, x0, x1
	CSELW LO, R10, R8, R8 // <--                                  // csel	w8, w10, w8, lo
	SUBW  $96, R8, R11    // <--                                  // sub	w11, w8, #96
	AND   $15, R1, R8     // <--                                  // and	x8, x1, #0xf
	SUB   R8, R9, R10     // <--                                  // sub	x10, x9, x8
	MOVD  R0, R9          // <--                                  // mov	x9, x0
	CMP   R0, R10         // <--                                  // cmp	x10, x0
	VDUP  R11, V2.B16     // <--                                  // dup	v2.16b, w11
	BLS   LBB4_43         // <--                                  // b.ls	.LBB4_43
	WORD  $0x4f05e403     // VMOVI $160, V3.B16                   // movi	v3.16b, #160
	MOVD  R0, R9          // <--                                  // mov	x9, x0

LBB4_41:
	WORD  $0x3dc00124                      // FMOVQ (R9), F4                       // ldr	q4, [x9]
	VADD  V3.B16, V4.B16, V4.B16           // <--                                  // add	v4.16b, v4.16b, v3.16b
	VTBL  V4.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v4.16b
	VSUB  V5.B16, V4.B16, V4.B16           // <--                                  // sub	v4.16b, v4.16b, v5.16b
	VCMEQ V2.B16, V4.B16, V4.B16           // <--                                  // cmeq	v4.16b, v4.16b, v2.16b
	WORD  $0x0f0c8484                      // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD F4, R11                          // <--                                  // fmov	x11, d4
	CBNZ  R11, LBB4_74                     // <--                                  // cbnz	x11, .LBB4_74
	ADD   $16, R9, R9                      // <--                                  // add	x9, x9, #16
	CMP   R10, R9                          // <--                                  // cmp	x9, x10
	BCC   LBB4_41                          // <--                                  // b.lo	.LBB4_41

LBB4_43:
	CBZ  R8, LBB4_104 // <--                                  // cbz	x8, .LBB4_104
	SUBS $8, R8, R11  // <--                                  // subs	x11, x8, #8
	BNE  LBB4_75      // <--                                  // b.ne	.LBB4_75
	WORD $0xfd400123  // FMOVD (R9), F3                       // ldr	d3, [x9]
	JMP  LBB4_97      // <--                                  // b	.LBB4_97

LBB4_46:
	MOVW  $403, R10       // <--                                  // mov	w10, #403
	MOVD  ZR, R8          // <--                                  // mov	x8, xzr
	MOVW  ZR, R9          // <--                                  // mov	w9, wzr
	MOVKW $(256<<16), R10 // <--                                  // movk	w10, #256, lsl #16

LBB4_47:
	WORD  $0x3868684b       // MOVBU (R2)(R8), R11                  // ldrb	w11, [x2, x8]
	MULW  R10, R9, R9       // <--                                  // mul	w9, w9, w10
	ADD   $1, R8, R8        // <--                                  // add	x8, x8, #1
	SUBW  $97, R11, R12     // <--                                  // sub	w12, w11, #97
	EORW  $128, R11, R13    // <--                                  // eor	w13, w11, #0x80
	SUBW  $96, R11, R11     // <--                                  // sub	w11, w11, #96
	CMPW  $26, R12          // <--                                  // cmp	w12, #26
	CSELW LO, R13, R11, R11 // <--                                  // csel	w11, w13, w11, lo
	CMP   R8, R3            // <--                                  // cmp	x3, x8
	ADDW  R11.UXTB, R9, R9  // <--                                  // add	w9, w9, w11, uxtb
	BNE   LBB4_47           // <--                                  // b.ne	.LBB4_47
	MOVW  $403, R12         // <--                                  // mov	w12, #403
	MOVW  $1, R11           // <--                                  // mov	w11, #1
	MOVD  R3, R8            // <--                                  // mov	x8, x3
	MOVKW $(256<<16), R12   // <--                                  // movk	w12, #256, lsl #16

LBB4_49:
	TST    $1, R8           // <--                                  // tst	x8, #0x1
	LSR    $1, R8, R14      // <--                                  // lsr	x14, x8, #1
	CSINCW NE, R12, ZR, R13 // <--                                  // csinc	w13, w12, wzr, ne
	MULW   R12, R12, R12    // <--                                  // mul	w12, w12, w12
	CMP    $1, R8           // <--                                  // cmp	x8, #1
	MOVD   R14, R8          // <--                                  // mov	x8, x14
	MULW   R11, R13, R11    // <--                                  // mul	w11, w13, w11
	BHI    LBB4_49          // <--                                  // b.hi	.LBB4_49
	MOVD   ZR, R8           // <--                                  // mov	x8, xzr
	MOVW   ZR, R12          // <--                                  // mov	w12, wzr

LBB4_51:
	WORD  $0x3868680d        // MOVBU (R0)(R8), R13                  // ldrb	w13, [x0, x8]
	MULW  R10, R12, R12      // <--                                  // mul	w12, w12, w10
	ADD   $1, R8, R8         // <--                                  // add	x8, x8, #1
	SUBW  $97, R13, R14      // <--                                  // sub	w14, w13, #97
	EORW  $128, R13, R15     // <--                                  // eor	w15, w13, #0x80
	SUBW  $96, R13, R13      // <--                                  // sub	w13, w13, #96
	CMPW  $26, R14           // <--                                  // cmp	w14, #26
	CSELW LO, R15, R13, R13  // <--                                  // csel	w13, w15, w13, lo
	CMP   R8, R3             // <--                                  // cmp	x3, x8
	ADDW  R13.UXTB, R12, R12 // <--                                  // add	w12, w12, w13, uxtb
	BNE   LBB4_51            // <--                                  // b.ne	.LBB4_51
	TBNZ  $63, R3, LBB4_102  // <--                                  // tbnz	x3, #63, .LBB4_102
	CMPW  R9, R12            // <--                                  // cmp	w12, w9
	BNE   LBB4_102           // <--                                  // b.ne	.LBB4_102
	ADD   R3, R0, R8         // <--                                  // add	x8, x0, x3
	AND   $15, R3, R14       // <--                                  // and	x14, x3, #0xf
	SUB   R14, R8, R15       // <--                                  // sub	x15, x8, x14
	MOVD  R2, R8             // <--                                  // mov	x8, x2
	MOVD  R0, R13            // <--                                  // mov	x13, x0
	CMP   R0, R15            // <--                                  // cmp	x15, x0
	BLS   LBB4_58            // <--                                  // b.ls	.LBB4_58
	WORD  $0x4f05e402        // VMOVI $160, V2.B16                   // movi	v2.16b, #160
	MOVD  R0, R13            // <--                                  // mov	x13, x0
	MOVD  R2, R8             // <--                                  // mov	x8, x2

LBB4_56:
	WORD  $0x3dc001a3                      // FMOVQ (R13), F3                      // ldr	q3, [x13]
	WORD  $0x3dc00104                      // FMOVQ (R8), F4                       // ldr	q4, [x8]
	VADD  V2.B16, V3.B16, V3.B16           // <--                                  // add	v3.16b, v3.16b, v2.16b
	VADD  V2.B16, V4.B16, V4.B16           // <--                                  // add	v4.16b, v4.16b, v2.16b
	VTBL  V3.B16, [V0.B16, V1.B16], V5.B16 // <--                                  // tbl	v5.16b, { v0.16b, v1.16b }, v3.16b
	VTBL  V4.B16, [V0.B16, V1.B16], V6.B16 // <--                                  // tbl	v6.16b, { v0.16b, v1.16b }, v4.16b
	VSUB  V5.B16, V3.B16, V3.B16           // <--                                  // sub	v3.16b, v3.16b, v5.16b
	VSUB  V6.B16, V4.B16, V4.B16           // <--                                  // sub	v4.16b, v4.16b, v6.16b
	VCMEQ V4.B16, V3.B16, V3.B16           // <--                                  // cmeq	v3.16b, v3.16b, v4.16b
	WORD  $0x0f0c8463                      // VSHRN $4, V3.H8, V3.B8               // shrn	v3.8b, v3.8h, #4
	FMOVD F3, R16                          // <--                                  // fmov	x16, d3
	CMN   $1, R16                          // <--                                  // cmn	x16, #1
	BNE   LBB4_102                         // <--                                  // b.ne	.LBB4_102
	ADD   $16, R13, R13                    // <--                                  // add	x13, x13, #16
	ADD   $16, R8, R8                      // <--                                  // add	x8, x8, #16
	CMP   R15, R13                         // <--                                  // cmp	x13, x15
	BCC   LBB4_56                          // <--                                  // b.lo	.LBB4_56

LBB4_58:
	CMP   $8, R14                        // <--                                  // cmp	x14, #8
	BCC   LBB4_61                        // <--                                  // b.lo	.LBB4_61
	WORD  $0x0f05e403                    // VMOVI $160, V3.B8                    // movi	v3.8b, #160
	WORD  $0xfc4085a2                    // FMOVD.P 8(R13), F2                   // ldr	d2, [x13], #8
	WORD  $0xfc408504                    // FMOVD.P 8(R8), F4                    // ldr	d4, [x8], #8
	VADD  V3.B8, V2.B8, V2.B8            // <--                                  // add	v2.8b, v2.8b, v3.8b
	VADD  V3.B8, V4.B8, V3.B8            // <--                                  // add	v3.8b, v4.8b, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V5.B8 // <--                                  // tbl	v5.8b, { v0.16b, v1.16b }, v3.8b
	VSUB  V4.B8, V2.B8, V2.B8            // <--                                  // sub	v2.8b, v2.8b, v4.8b
	VSUB  V5.B8, V3.B8, V3.B8            // <--                                  // sub	v3.8b, v3.8b, v5.8b
	VCMEQ V3.B8, V2.B8, V2.B8            // <--                                  // cmeq	v2.8b, v2.8b, v3.8b
	FMOVD F2, R14                        // <--                                  // fmov	x14, d2
	CMN   $1, R14                        // <--                                  // cmn	x14, #1
	BNE   LBB4_102                       // <--                                  // b.ne	.LBB4_102
	AND   $7, R3, R14                    // <--                                  // and	x14, x3, #0x7

LBB4_61:
	CBZ  R14, LBB4_129 // <--                                  // cbz	x14, .LBB4_129
	SUBS $4, R14, R17  // <--                                  // subs	x17, x14, #4
	BCC  LBB4_87       // <--                                  // b.lo	.LBB4_87
	WORD $0xb84045af   // MOVWU.P 4(R13), R15                  // ldr	w15, [x13], #4
	WORD $0xb8404510   // MOVWU.P 4(R8), R16                   // ldr	w16, [x8], #4
	MOVD R17, R14      // <--                                  // mov	x14, x17
	JMP  LBB4_88       // <--                                  // b	.LBB4_88

LBB4_64:
	AND $7, R1, R8 // <--                                  // and	x8, x1, #0x7

LBB4_65:
	CBZ  R8, LBB4_128 // <--                                  // cbz	x8, .LBB4_128
	SUBS $4, R8, R11  // <--                                  // subs	x11, x8, #4
	BCC  LBB4_68      // <--                                  // b.lo	.LBB4_68
	WORD $0xb8404409  // MOVWU.P 4(R0), R9                    // ldr	w9, [x0], #4
	WORD $0xb840444a  // MOVWU.P 4(R2), R10                   // ldr	w10, [x2], #4
	MOVD R11, R8      // <--                                  // mov	x8, x11
	JMP  LBB4_69      // <--                                  // b	.LBB4_69

LBB4_68:
	MOVD ZR, R10 // <--                                  // mov	x10, xzr
	MOVD ZR, R9  // <--                                  // mov	x9, xzr

LBB4_69:
	CMP  $1, R8           // <--                                  // cmp	x8, #1
	BEQ  LBB4_79          // <--                                  // b.eq	.LBB4_79
	CMP  $2, R8           // <--                                  // cmp	x8, #2
	BEQ  LBB4_80          // <--                                  // b.eq	.LBB4_80
	CMP  $3, R8           // <--                                  // cmp	x8, #3
	BNE  LBB4_81          // <--                                  // b.ne	.LBB4_81
	WORD $0x79400008      // MOVHU (R0), R8                       // ldrh	w8, [x0]
	LSL  $24, R9, R9      // <--                                  // lsl	x9, x9, #24
	WORD $0x7940004c      // MOVHU (R2), R12                      // ldrh	w12, [x2]
	LSL  $24, R10, R10    // <--                                  // lsl	x10, x10, #24
	WORD $0x3940080b      // MOVBU 2(R0), R11                     // ldrb	w11, [x0, #2]
	WORD $0x3940084d      // MOVBU 2(R2), R13                     // ldrb	w13, [x2, #2]
	ORR  R8<<8, R9, R8    // <--                                  // orr	x8, x9, x8, lsl #8
	ORR  R12<<8, R10, R10 // <--                                  // orr	x10, x10, x12, lsl #8
	ORR  R11, R8, R9      // <--                                  // orr	x9, x8, x11
	ORR  R13, R10, R10    // <--                                  // orr	x10, x10, x13
	JMP  LBB4_81          // <--                                  // b	.LBB4_81

LBB4_73:
	RBIT  R14, R11           // <--                                  // rbit	x11, x14
	CLZ   R11, R11           // <--                                  // clz	x11, x11
	UBFX  $2, R11, $30, R11  // <--                                  // ubfx	x11, x11, #2, #30
	MOVW  R11, R12           // <--                                  // mov	w12, w11
	SUBW  $1, R11, R11       // <--                                  // sub	w11, w11, #1
	ADD   R12, R8, R8        // <--                                  // add	x8, x8, x12
	SUB   $1, R8, R8         // <--                                  // sub	x8, x8, #1
	ADD   R11.SXTW, R10, R10 // <--                                  // add	x10, x10, w11, sxtw
	CMP   R9, R8             // <--                                  // cmp	x8, x9
	CSINV LO, R10, ZR, R8    // <--                                  // csinv	x8, x10, xzr, lo
	NOP                      // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                      // (skipped)                            // ldr	x21, [sp, #16]
	NOP                      // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD  R8, R0             // <--                                  // mov	x0, x8
	MOVD  R0, ret+32(FP)     // <--
	RET                      // <--                                  // ret

LBB4_74:
	RBIT  R11, R8        // <--                                  // rbit	x8, x11
	CLZ   R8, R8         // <--                                  // clz	x8, x8
	LSR   $2, R8, R8     // <--                                  // lsr	x8, x8, #2
	ADD   R8, R9, R11    // <--                                  // add	x11, x9, x8
	SUB   R0, R9, R9     // <--                                  // sub	x9, x9, x0
	ADD   R8, R9, R8     // <--                                  // add	x8, x9, x8
	CMP   R10, R11       // <--                                  // cmp	x11, x10
	CSINV LO, R8, ZR, R8 // <--                                  // csinv	x8, x8, xzr, lo
	NOP                  // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                  // (skipped)                            // ldr	x21, [sp, #16]
	NOP                  // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD  R8, R0         // <--                                  // mov	x0, x8
	MOVD  R0, ret+32(FP) // <--
	RET                  // <--                                  // ret

LBB4_75:
	MOVD R8, R13         // <--                                  // mov	x13, x8
	MOVD R9, R12         // <--                                  // mov	x12, x9
	TBZ  $3, R1, LBB4_77 // <--                                  // tbz	w1, #3, .LBB4_77
	MOVD R9, R12         // <--                                  // mov	x12, x9
	MOVD R11, R13        // <--                                  // mov	x13, x11
	WORD $0xf840858a     // MOVD.P 8(R12), R10                   // ldr	x10, [x12], #8

LBB4_77:
	TBNZ $2, R13, LBB4_82 // <--                                  // tbnz	w13, #2, .LBB4_82
	MOVD ZR, R11          // <--                                  // mov	x11, xzr
	JMP  LBB4_83          // <--                                  // b	.LBB4_83

LBB4_79:
	WORD $0x39400008      // MOVBU (R0), R8                       // ldrb	w8, [x0]
	WORD $0x3940004b      // MOVBU (R2), R11                      // ldrb	w11, [x2]
	ORR  R9<<8, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #8
	ORR  R10<<8, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #8
	JMP  LBB4_81          // <--                                  // b	.LBB4_81

LBB4_80:
	WORD $0x79400008       // MOVHU (R0), R8                       // ldrh	w8, [x0]
	WORD $0x7940004b       // MOVHU (R2), R11                      // ldrh	w11, [x2]
	ORR  R9<<16, R8, R9    // <--                                  // orr	x9, x8, x9, lsl #16
	ORR  R10<<16, R11, R10 // <--                                  // orr	x10, x11, x10, lsl #16

LBB4_81:
	WORD  $0x0f05e402                    // VMOVI $160, V2.B8                    // movi	v2.8b, #160
	FMOVD R9, F3                         // <--                                  // fmov	d3, x9
	FMOVD R10, F4                        // <--                                  // fmov	d4, x10
	VADD  V2.B8, V3.B8, V3.B8            // <--                                  // add	v3.8b, v3.8b, v2.8b
	VADD  V2.B8, V4.B8, V2.B8            // <--                                  // add	v2.8b, v4.8b, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V0.B8 // <--                                  // tbl	v0.8b, { v0.16b, v1.16b }, v2.8b
	VSUB  V4.B8, V3.B8, V1.B8            // <--                                  // sub	v1.8b, v3.8b, v4.8b
	VSUB  V0.B8, V2.B8, V0.B8            // <--                                  // sub	v0.8b, v2.8b, v0.8b
	VCMEQ V0.B8, V1.B8, V0.B8            // <--                                  // cmeq	v0.8b, v1.8b, v0.8b
	FMOVD F0, R8                         // <--                                  // fmov	x8, d0
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	CSETM NE, R8                         // <--                                  // csetm	x8, ne
	NOP                                  // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                                  // (skipped)                            // ldr	x21, [sp, #16]
	NOP                                  // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD  R8, R0                         // <--                                  // mov	x0, x8
	MOVD  R0, ret+32(FP)                 // <--
	RET                                  // <--                                  // ret

LBB4_82:
	WORD $0xb840458b  // MOVWU.P 4(R12), R11                  // ldr	w11, [x12], #4
	SUB  $4, R13, R13 // <--                                  // sub	x13, x13, #4

LBB4_83:
	CMP  $1, R13          // <--                                  // cmp	x13, #1
	BEQ  LBB4_92          // <--                                  // b.eq	.LBB4_92
	CMP  $2, R13          // <--                                  // cmp	x13, #2
	BEQ  LBB4_93          // <--                                  // b.eq	.LBB4_93
	CMP  $3, R13          // <--                                  // cmp	x13, #3
	BNE  LBB4_96          // <--                                  // b.ne	.LBB4_96
	LSRW $2, R8, R13      // <--                                  // lsr	w13, w8, #2
	LSLW $3, R8, R15      // <--                                  // lsl	w15, w8, #3
	MOVW $16, R16         // <--                                  // mov	w16, #16
	WORD $0x7940018e      // MOVHU (R12), R14                     // ldrh	w14, [x12]
	ANDW $32, R15, R15    // <--                                  // and	w15, w15, #0x20
	WORD $0x3940098c      // MOVBU 2(R12), R12                    // ldrb	w12, [x12, #2]
	BFIW $5, R13, $1, R16 // <--                                  // bfi	w16, w13, #5, #1
	LSL  R15, R14, R13    // <--                                  // lsl	x13, x14, x15
	LSL  R16, R12, R12    // <--                                  // lsl	x12, x12, x16
	ORR  R12, R13, R12    // <--                                  // orr	x12, x13, x12
	JMP  LBB4_95          // <--                                  // b	.LBB4_95

LBB4_87:
	MOVD ZR, R16 // <--                                  // mov	x16, xzr
	MOVD ZR, R15 // <--                                  // mov	x15, xzr

LBB4_88:
	CMP  $1, R14          // <--                                  // cmp	x14, #1
	BEQ  LBB4_99          // <--                                  // b.eq	.LBB4_99
	CMP  $2, R14          // <--                                  // cmp	x14, #2
	BEQ  LBB4_100         // <--                                  // b.eq	.LBB4_100
	CMP  $3, R14          // <--                                  // cmp	x14, #3
	BNE  LBB4_101         // <--                                  // b.ne	.LBB4_101
	WORD $0x794001ae      // MOVHU (R13), R14                     // ldrh	w14, [x13]
	LSL  $24, R15, R15    // <--                                  // lsl	x15, x15, #24
	WORD $0x79400111      // MOVHU (R8), R17                      // ldrh	w17, [x8]
	LSL  $24, R16, R16    // <--                                  // lsl	x16, x16, #24
	WORD $0x394009ad      // MOVBU 2(R13), R13                    // ldrb	w13, [x13, #2]
	WORD $0x39400908      // MOVBU 2(R8), R8                      // ldrb	w8, [x8, #2]
	ORR  R14<<8, R15, R14 // <--                                  // orr	x14, x15, x14, lsl #8
	ORR  R17<<8, R16, R16 // <--                                  // orr	x16, x16, x17, lsl #8
	ORR  R13, R14, R15    // <--                                  // orr	x15, x14, x13
	ORR  R8, R16, R16     // <--                                  // orr	x16, x16, x8
	JMP  LBB4_101         // <--                                  // b	.LBB4_101

LBB4_92:
	LSLW $3, R8, R13 // <--                                  // lsl	w13, w8, #3
	WORD $0x3940018c // MOVBU (R12), R12                     // ldrb	w12, [x12]
	JMP  LBB4_94     // <--                                  // b	.LBB4_94

LBB4_93:
	WORD $0x7940018c // MOVHU (R12), R12                     // ldrh	w12, [x12]
	LSLW $3, R8, R13 // <--                                  // lsl	w13, w8, #3

LBB4_94:
	AND $32, R13, R13 // <--                                  // and	x13, x13, #0x20
	LSL R13, R12, R12 // <--                                  // lsl	x12, x12, x13

LBB4_95:
	ORR R11, R12, R11 // <--                                  // orr	x11, x12, x11

LBB4_96:
	WORD   $0x6f00e403      // VMOVI $0, V3.D2                      // movi	v3.2d, #0000000000000000
	CMP    $7, R8           // <--                                  // cmp	x8, #7
	FMOVD  R10, F4          // <--                                  // fmov	d4, x10
	FMOVD  R11, F5          // <--                                  // fmov	d5, x11
	FCSELD HI, F5, F3, F6   // <--                                  // fcsel	d6, d5, d3, hi
	FCSELD HI, F4, F5, F3   // <--                                  // fcsel	d3, d4, d5, hi
	VMOV   V6.D[0], V3.D[1] // <--                                  // mov	v3.d[1], v6.d[0]

LBB4_97:
	WORD  $0x4f05e404                      // VMOVI $160, V4.B16                   // movi	v4.16b, #160
	VADD  V4.B16, V3.B16, V3.B16           // <--                                  // add	v3.16b, v3.16b, v4.16b
	VTBL  V3.B16, [V0.B16, V1.B16], V0.B16 // <--                                  // tbl	v0.16b, { v0.16b, v1.16b }, v3.16b
	VSUB  V0.B16, V3.B16, V0.B16           // <--                                  // sub	v0.16b, v3.16b, v0.16b
	VCMEQ V2.B16, V0.B16, V0.B16           // <--                                  // cmeq	v0.16b, v0.16b, v2.16b
	WORD  $0x0f0c8400                      // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R10                          // <--                                  // fmov	x10, d0
	CBZ   R10, LBB4_104                    // <--                                  // cbz	x10, .LBB4_104
	RBIT  R10, R10                         // <--                                  // rbit	x10, x10
	SUB   R0, R9, R9                       // <--                                  // sub	x9, x9, x0
	CLZ   R10, R10                         // <--                                  // clz	x10, x10
	LSR   $2, R10, R10                     // <--                                  // lsr	x10, x10, #2
	ADD   R10, R9, R9                      // <--                                  // add	x9, x9, x10
	CMP   R8, R10                          // <--                                  // cmp	x10, x8
	CSINV LO, R9, ZR, R8                   // <--                                  // csinv	x8, x9, xzr, lo
	NOP                                    // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                                    // (skipped)                            // ldr	x21, [sp, #16]
	NOP                                    // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD  R8, R0                           // <--                                  // mov	x0, x8
	MOVD  R0, ret+32(FP)                   // <--
	RET                                    // <--                                  // ret

LBB4_99:
	WORD $0x394001ad      // MOVBU (R13), R13                     // ldrb	w13, [x13]
	WORD $0x39400108      // MOVBU (R8), R8                       // ldrb	w8, [x8]
	ORR  R15<<8, R13, R15 // <--                                  // orr	x15, x13, x15, lsl #8
	ORR  R16<<8, R8, R16  // <--                                  // orr	x16, x8, x16, lsl #8
	JMP  LBB4_101         // <--                                  // b	.LBB4_101

LBB4_100:
	WORD $0x794001ad       // MOVHU (R13), R13                     // ldrh	w13, [x13]
	WORD $0x79400108       // MOVHU (R8), R8                       // ldrh	w8, [x8]
	ORR  R15<<16, R13, R15 // <--                                  // orr	x15, x13, x15, lsl #16
	ORR  R16<<16, R8, R16  // <--                                  // orr	x16, x8, x16, lsl #16

LBB4_101:
	WORD  $0x0f05e402                    // VMOVI $160, V2.B8                    // movi	v2.8b, #160
	FMOVD R15, F3                        // <--                                  // fmov	d3, x15
	FMOVD R16, F4                        // <--                                  // fmov	d4, x16
	VADD  V2.B8, V3.B8, V3.B8            // <--                                  // add	v3.8b, v3.8b, v2.8b
	VADD  V2.B8, V4.B8, V2.B8            // <--                                  // add	v2.8b, v4.8b, v2.8b
	VTBL  V3.B8, [V0.B16, V1.B16], V4.B8 // <--                                  // tbl	v4.8b, { v0.16b, v1.16b }, v3.8b
	VTBL  V2.B8, [V0.B16, V1.B16], V5.B8 // <--                                  // tbl	v5.8b, { v0.16b, v1.16b }, v2.8b
	VSUB  V4.B8, V3.B8, V3.B8            // <--                                  // sub	v3.8b, v3.8b, v4.8b
	VSUB  V5.B8, V2.B8, V2.B8            // <--                                  // sub	v2.8b, v2.8b, v5.8b
	VCMEQ V2.B8, V3.B8, V2.B8            // <--                                  // cmeq	v2.8b, v3.8b, v2.8b
	FMOVD F2, R8                         // <--                                  // fmov	x8, d2
	CMN   $1, R8                         // <--                                  // cmn	x8, #1
	BEQ   LBB4_129                       // <--                                  // b.eq	.LBB4_129

LBB4_102:
	CMP  R1, R3        // <--                                  // cmp	x3, x1
	BCS  LBB4_104      // <--                                  // b.hs	.LBB4_104
	AND  $-16, R3, R14 // <--                                  // and	x14, x3, #0xfffffffffffffff0
	WORD $0x0f05e403   // VMOVI $160, V3.B8                    // movi	v3.8b, #160
	WORD $0x4f05e402   // VMOVI $160, V2.B16                   // movi	v2.16b, #160
	NEG  R3, R13       // <--                                  // neg	x13, x3
	AND  $15, R3, R15  // <--                                  // and	x15, x3, #0xf
	ORR  $1, R14, R16  // <--                                  // orr	x16, x14, #0x1
	AND  $7, R3, R17   // <--                                  // and	x17, x3, #0x7
	MOVD R3, R4        // <--                                  // mov	x4, x3
	JMP  LBB4_108      // <--                                  // b	.LBB4_108

LBB4_104:
	MOVD $-1, R8        // <--                                  // mov	x8, #-1
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB4_105:
	WORD $0x394000c6     // MOVBU (R6), R6                       // ldrb	w6, [x6]
	WORD $0x394000a5     // MOVBU (R5), R5                       // ldrb	w5, [x5]
	ORR  R7<<8, R6, R7   // <--                                  // orr	x7, x6, x7, lsl #8
	ORR  R19<<8, R5, R19 // <--                                  // orr	x19, x5, x19, lsl #8

LBB4_106:
	FMOVD R7, F4                         // <--                                  // fmov	d4, x7
	FMOVD R19, F5                        // <--                                  // fmov	d5, x19
	VADD  V3.B8, V4.B8, V4.B8            // <--                                  // add	v4.8b, v4.8b, v3.8b
	VADD  V3.B8, V5.B8, V5.B8            // <--                                  // add	v5.8b, v5.8b, v3.8b
	VTBL  V4.B8, [V0.B16, V1.B16], V6.B8 // <--                                  // tbl	v6.8b, { v0.16b, v1.16b }, v4.8b
	VTBL  V5.B8, [V0.B16, V1.B16], V7.B8 // <--                                  // tbl	v7.8b, { v0.16b, v1.16b }, v5.8b
	VSUB  V6.B8, V4.B8, V4.B8            // <--                                  // sub	v4.8b, v4.8b, v6.8b
	VSUB  V7.B8, V5.B8, V5.B8            // <--                                  // sub	v5.8b, v5.8b, v7.8b
	VCMEQ V5.B8, V4.B8, V4.B8            // <--                                  // cmeq	v4.8b, v4.8b, v5.8b
	VSHL  $7, V4.B8, V4.B8               // <--                                  // shl	v4.8b, v4.8b, #7
	WORD  $0x0e20a884                    // VCMLT $0, V4.B8, V4.B8               // cmlt	v4.8b, v4.8b, #0
	FMOVD F4, R5                         // <--                                  // fmov	x5, d4
	CMN   $1, R5                         // <--                                  // cmn	x5, #1
	BEQ   LBB4_130                       // <--                                  // b.eq	.LBB4_130

LBB4_107:
	ADD  $1, R4, R4 // <--                                  // add	x4, x4, #1
	MOVD $-1, R8    // <--                                  // mov	x8, #-1
	CMP  R1, R4     // <--                                  // cmp	x4, x1
	BEQ  LBB4_128   // <--                                  // b.eq	.LBB4_128

LBB4_108:
	ADD   R4, R0, R5        // <--                                  // add	x5, x0, x4
	SUB   R3, R4, R8        // <--                                  // sub	x8, x4, x3
	MULW  R10, R12, R12     // <--                                  // mul	w12, w12, w10
	WORD  $0x394000a6       // MOVBU (R5), R6                       // ldrb	w6, [x5]
	WORD  $0x38686807       // MOVBU (R0)(R8), R7                   // ldrb	w7, [x0, x8]
	SUBW  $97, R6, R19      // <--                                  // sub	w19, w6, #97
	EORW  $128, R6, R20     // <--                                  // eor	w20, w6, #0x80
	SUBW  $96, R6, R6       // <--                                  // sub	w6, w6, #96
	CMPW  $26, R19          // <--                                  // cmp	w19, #26
	SUBW  $97, R7, R19      // <--                                  // sub	w19, w7, #97
	CSELW LO, R20, R6, R6   // <--                                  // csel	w6, w20, w6, lo
	CMPW  $26, R19          // <--                                  // cmp	w19, #26
	EORW  $128, R7, R19     // <--                                  // eor	w19, w7, #0x80
	SUBW  $96, R7, R7       // <--                                  // sub	w7, w7, #96
	ADDW  R6.UXTB, R12, R12 // <--                                  // add	w12, w12, w6, uxtb
	CSELW LO, R19, R7, R7   // <--                                  // csel	w7, w19, w7, lo
	ANDW  $255, R7, R7      // <--                                  // and	w7, w7, #0xff
	MSUBW R7, R12, R11, R12 // <--                                  // msub	w12, w11, w7, w12
	TBNZ  $63, R3, LBB4_107 // <--                                  // tbnz	x3, #63, .LBB4_107
	CMPW  R9, R12           // <--                                  // cmp	w12, w9
	BNE   LBB4_107          // <--                                  // b.ne	.LBB4_107
	ADD   R13, R5, R5       // <--                                  // add	x5, x5, x13
	CMP   $2, R16           // <--                                  // cmp	x16, #2
	ADD   $1, R5, R6        // <--                                  // add	x6, x5, #1
	BLT   LBB4_117          // <--                                  // b.lt	.LBB4_117
	ADD   R14, R6, R7       // <--                                  // add	x7, x6, x14
	MOVD  R2, R5            // <--                                  // mov	x5, x2

LBB4_112:
	WORD  $0x3dc000c4                      // FMOVQ (R6), F4                       // ldr	q4, [x6]
	WORD  $0x3dc000a5                      // FMOVQ (R5), F5                       // ldr	q5, [x5]
	VADD  V2.B16, V4.B16, V4.B16           // <--                                  // add	v4.16b, v4.16b, v2.16b
	VADD  V2.B16, V5.B16, V5.B16           // <--                                  // add	v5.16b, v5.16b, v2.16b
	VTBL  V4.B16, [V0.B16, V1.B16], V6.B16 // <--                                  // tbl	v6.16b, { v0.16b, v1.16b }, v4.16b
	VTBL  V5.B16, [V0.B16, V1.B16], V7.B16 // <--                                  // tbl	v7.16b, { v0.16b, v1.16b }, v5.16b
	VSUB  V6.B16, V4.B16, V4.B16           // <--                                  // sub	v4.16b, v4.16b, v6.16b
	VSUB  V7.B16, V5.B16, V5.B16           // <--                                  // sub	v5.16b, v5.16b, v7.16b
	VCMEQ V5.B16, V4.B16, V4.B16           // <--                                  // cmeq	v4.16b, v4.16b, v5.16b
	WORD  $0x0f0c8484                      // VSHRN $4, V4.H8, V4.B8               // shrn	v4.8b, v4.8h, #4
	FMOVD F4, R19                          // <--                                  // fmov	x19, d4
	CMN   $1, R19                          // <--                                  // cmn	x19, #1
	BNE   LBB4_107                         // <--                                  // b.ne	.LBB4_107
	ADD   $16, R6, R6                      // <--                                  // add	x6, x6, #16
	ADD   $16, R5, R5                      // <--                                  // add	x5, x5, #16
	CMP   R7, R6                           // <--                                  // cmp	x6, x7
	BCC   LBB4_112                         // <--                                  // b.lo	.LBB4_112
	CMP   $8, R15                          // <--                                  // cmp	x15, #8
	BCC   LBB4_118                         // <--                                  // b.lo	.LBB4_118

LBB4_115:
	WORD  $0xfc4084c4                    // FMOVD.P 8(R6), F4                    // ldr	d4, [x6], #8
	WORD  $0xfc4084a5                    // FMOVD.P 8(R5), F5                    // ldr	d5, [x5], #8
	VADD  V3.B8, V4.B8, V4.B8            // <--                                  // add	v4.8b, v4.8b, v3.8b
	VADD  V3.B8, V5.B8, V5.B8            // <--                                  // add	v5.8b, v5.8b, v3.8b
	VTBL  V4.B8, [V0.B16, V1.B16], V6.B8 // <--                                  // tbl	v6.8b, { v0.16b, v1.16b }, v4.8b
	VTBL  V5.B8, [V0.B16, V1.B16], V7.B8 // <--                                  // tbl	v7.8b, { v0.16b, v1.16b }, v5.8b
	VSUB  V6.B8, V4.B8, V4.B8            // <--                                  // sub	v4.8b, v4.8b, v6.8b
	VSUB  V7.B8, V5.B8, V5.B8            // <--                                  // sub	v5.8b, v5.8b, v7.8b
	VCMEQ V5.B8, V4.B8, V4.B8            // <--                                  // cmeq	v4.8b, v4.8b, v5.8b
	FMOVD F4, R7                         // <--                                  // fmov	x7, d4
	CMN   $1, R7                         // <--                                  // cmn	x7, #1
	BNE   LBB4_107                       // <--                                  // b.ne	.LBB4_107
	MOVD  R17, R20                       // <--                                  // mov	x20, x17
	JMP   LBB4_119                       // <--                                  // b	.LBB4_119

LBB4_117:
	MOVD R2, R5   // <--                                  // mov	x5, x2
	CMP  $8, R15  // <--                                  // cmp	x15, #8
	BCS  LBB4_115 // <--                                  // b.hs	.LBB4_115

LBB4_118:
	MOVD R15, R20 // <--                                  // mov	x20, x15

LBB4_119:
	CBZ  R20, LBB4_130 // <--                                  // cbz	x20, .LBB4_130
	SUBS $4, R20, R21  // <--                                  // subs	x21, x20, #4
	BCC  LBB4_122      // <--                                  // b.lo	.LBB4_122
	WORD $0xb84044c7   // MOVWU.P 4(R6), R7                    // ldr	w7, [x6], #4
	WORD $0xb84044b3   // MOVWU.P 4(R5), R19                   // ldr	w19, [x5], #4
	MOVD R21, R20      // <--                                  // mov	x20, x21
	JMP  LBB4_123      // <--                                  // b	.LBB4_123

LBB4_122:
	MOVD ZR, R19 // <--                                  // mov	x19, xzr
	MOVD ZR, R7  // <--                                  // mov	x7, xzr

LBB4_123:
	CMP  $1, R20          // <--                                  // cmp	x20, #1
	BEQ  LBB4_105         // <--                                  // b.eq	.LBB4_105
	CMP  $2, R20          // <--                                  // cmp	x20, #2
	BEQ  LBB4_127         // <--                                  // b.eq	.LBB4_127
	CMP  $3, R20          // <--                                  // cmp	x20, #3
	BNE  LBB4_106         // <--                                  // b.ne	.LBB4_106
	WORD $0x794000d4      // MOVHU (R6), R20                      // ldrh	w20, [x6]
	LSL  $24, R7, R7      // <--                                  // lsl	x7, x7, #24
	WORD $0x794000b5      // MOVHU (R5), R21                      // ldrh	w21, [x5]
	LSL  $24, R19, R19    // <--                                  // lsl	x19, x19, #24
	WORD $0x394008c6      // MOVBU 2(R6), R6                      // ldrb	w6, [x6, #2]
	WORD $0x394008a5      // MOVBU 2(R5), R5                      // ldrb	w5, [x5, #2]
	ORR  R20<<8, R7, R7   // <--                                  // orr	x7, x7, x20, lsl #8
	ORR  R21<<8, R19, R19 // <--                                  // orr	x19, x19, x21, lsl #8
	ORR  R6, R7, R7       // <--                                  // orr	x7, x7, x6
	ORR  R5, R19, R19     // <--                                  // orr	x19, x19, x5
	JMP  LBB4_106         // <--                                  // b	.LBB4_106

LBB4_127:
	WORD $0x794000c6      // MOVHU (R6), R6                       // ldrh	w6, [x6]
	WORD $0x794000a5      // MOVHU (R5), R5                       // ldrh	w5, [x5]
	ORR  R7<<16, R6, R7   // <--                                  // orr	x7, x6, x7, lsl #16
	ORR  R19<<16, R5, R19 // <--                                  // orr	x19, x5, x19, lsl #16
	JMP  LBB4_106         // <--                                  // b	.LBB4_106

LBB4_128:
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB4_129:
	MOVD ZR, R8         // <--                                  // mov	x8, xzr
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

LBB4_130:
	ADD  $1, R8, R8     // <--                                  // add	x8, x8, #1
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+32(FP) // <--
	RET                 // <--                                  // ret

DATA uppercasingTable<>+0x00(SB)/8, $0x2020202020202000
DATA uppercasingTable<>+0x08(SB)/8, $0x2020202020202020
DATA uppercasingTable<>+0x10(SB)/8, $0x2020202020202020
DATA uppercasingTable<>+0x18(SB)/8, $0x0000000000202020
GLOBL uppercasingTable<>(SB), (RODATA|NOPTR), $32

DATA tail_mask_table<>+0x00(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x08(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x10(SB)/1, $0xff
DATA tail_mask_table<>+0x11(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x19(SB)/4, $0x00000000
DATA tail_mask_table<>+0x1d(SB)/2, $0x0000
DATA tail_mask_table<>+0x1f(SB)/1, $0x00
DATA tail_mask_table<>+0x20(SB)/1, $0xff
DATA tail_mask_table<>+0x21(SB)/1, $0xff
DATA tail_mask_table<>+0x22(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x2a(SB)/4, $0x00000000
DATA tail_mask_table<>+0x2e(SB)/2, $0x0000
DATA tail_mask_table<>+0x30(SB)/1, $0xff
DATA tail_mask_table<>+0x31(SB)/1, $0xff
DATA tail_mask_table<>+0x32(SB)/1, $0xff
DATA tail_mask_table<>+0x33(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x3b(SB)/4, $0x00000000
DATA tail_mask_table<>+0x3f(SB)/1, $0x00
DATA tail_mask_table<>+0x40(SB)/1, $0xff
DATA tail_mask_table<>+0x41(SB)/1, $0xff
DATA tail_mask_table<>+0x42(SB)/1, $0xff
DATA tail_mask_table<>+0x43(SB)/1, $0xff
DATA tail_mask_table<>+0x44(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x4c(SB)/4, $0x00000000
DATA tail_mask_table<>+0x50(SB)/1, $0xff
DATA tail_mask_table<>+0x51(SB)/1, $0xff
DATA tail_mask_table<>+0x52(SB)/1, $0xff
DATA tail_mask_table<>+0x53(SB)/1, $0xff
DATA tail_mask_table<>+0x54(SB)/1, $0xff
DATA tail_mask_table<>+0x55(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x5d(SB)/2, $0x0000
DATA tail_mask_table<>+0x5f(SB)/1, $0x00
DATA tail_mask_table<>+0x60(SB)/1, $0xff
DATA tail_mask_table<>+0x61(SB)/1, $0xff
DATA tail_mask_table<>+0x62(SB)/1, $0xff
DATA tail_mask_table<>+0x63(SB)/1, $0xff
DATA tail_mask_table<>+0x64(SB)/1, $0xff
DATA tail_mask_table<>+0x65(SB)/1, $0xff
DATA tail_mask_table<>+0x66(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x6e(SB)/2, $0x0000
DATA tail_mask_table<>+0x70(SB)/1, $0xff
DATA tail_mask_table<>+0x71(SB)/1, $0xff
DATA tail_mask_table<>+0x72(SB)/1, $0xff
DATA tail_mask_table<>+0x73(SB)/1, $0xff
DATA tail_mask_table<>+0x74(SB)/1, $0xff
DATA tail_mask_table<>+0x75(SB)/1, $0xff
DATA tail_mask_table<>+0x76(SB)/1, $0xff
DATA tail_mask_table<>+0x77(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x7f(SB)/1, $0x00
DATA tail_mask_table<>+0x80(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0x88(SB)/8, $0x0000000000000000
DATA tail_mask_table<>+0x90(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0x98(SB)/8, $0x00000000000000ff
DATA tail_mask_table<>+0xa0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xa8(SB)/8, $0x000000000000ffff
DATA tail_mask_table<>+0xb0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xb8(SB)/8, $0x0000000000ffffff
DATA tail_mask_table<>+0xc0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xc8(SB)/8, $0x00000000ffffffff
DATA tail_mask_table<>+0xd0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xd8(SB)/8, $0x000000ffffffffff
DATA tail_mask_table<>+0xe0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xe8(SB)/8, $0x0000ffffffffffff
DATA tail_mask_table<>+0xf0(SB)/8, $0xffffffffffffffff
DATA tail_mask_table<>+0xf8(SB)/8, $0x00ffffffffffffff
GLOBL tail_mask_table<>(SB), (RODATA|NOPTR), $256

TEXT ·IndexFoldNeedle(SB), NOSPLIT, $0-72
	MOVD haystack+0(FP), R0
	MOVD haystack_len+8(FP), R1
	MOVB rare1+16(FP), R2
	MOVD off1+24(FP), R3
	MOVB rare2+32(FP), R4
	MOVD off2+40(FP), R5
	MOVD norm_needle+48(FP), R6
	MOVD needle_len+56(FP), R7
	SUBS R7, R1, R9             // <--                                  // subs	x9, x1, x7
	BGE  LBB6_2                 // <--                                  // b.ge	.LBB6_2
	MOVD $-1, R0                // <--                                  // mov	x0, #-1
	MOVD R0, ret+64(FP)         // <--
	RET                         // <--                                  // ret

LBB6_2:
	CMP    $1, R7               // <--                                  // cmp	x7, #1
	BLT    LBB6_10              // <--                                  // b.lt	.LBB6_10
	NOP                         // (skipped)                            // stp	x29, x30, [sp, #-48]!
	ORRW   $32, R2, R8          // <--                                  // orr	w8, w2, #0x20
	ORRW   $32, R4, R12         // <--                                  // orr	w12, w4, #0x20
	ANDW   $255, R8, R8         // <--                                  // and	w8, w8, #0xff
	ANDW   $4294967263, R2, R10 // <--                                  // and	w10, w2, #0xffffffdf
	SUBW   $97, R8, R8          // <--                                  // sub	w8, w8, #97
	MOVW   $4294967263, R11     // <--                                  // mov	w11, #-33
	CMPW   $26, R8              // <--                                  // cmp	w8, #26
	ANDW   $255, R12, R8        // <--                                  // and	w8, w12, #0xff
	SUBW   $97, R8, R8          // <--                                  // sub	w8, w8, #97
	CSELW  LO, R10, R2, R12     // <--                                  // csel	w12, w10, w2, lo
	CSINVW LO, R11, ZR, R13     // <--                                  // csinv	w13, w11, wzr, lo
	ANDW   $4294967263, R4, R10 // <--                                  // and	w10, w4, #0xffffffdf
	CMPW   $26, R8              // <--                                  // cmp	w8, #26
	NOP                         // (skipped)                            // str	x21, [sp, #16]
	CSELW  LO, R10, R4, R8      // <--                                  // csel	w8, w10, w4, lo
	CSINVW LO, R11, ZR, R11     // <--                                  // csinv	w11, w11, wzr, lo
	ADD    $1, R9, R10          // <--                                  // add	x10, x9, #1
	VDUP   R12, V0.B16          // <--                                  // dup	v0.16b, w12
	VDUP   R13, V2.B16          // <--                                  // dup	v2.16b, w13
	CMP    $63, R9              // <--                                  // cmp	x9, #63
	VDUP   R8, V1.B16           // <--                                  // dup	v1.16b, w8
	VDUP   R11, V3.B16          // <--                                  // dup	v3.16b, w11
	NOP                         // (skipped)                            // stp	x20, x19, [sp, #32]
	NOP                         // (skipped)                            // mov	x29, sp
	BGE    LBB6_11              // <--                                  // b.ge	.LBB6_11
	MOVD   ZR, R11              // <--                                  // mov	x11, xzr

LBB6_5:
	ORR  $16, R11, R16 // <--                                  // orr	x16, x11, #0x10
	CMP  R10, R16      // <--                                  // cmp	x16, x10
	BLE  LBB6_43       // <--                                  // b.le	.LBB6_43
	MOVD R11, R12      // <--                                  // mov	x12, x11

LBB6_7:
	CMP  R9, R12 // <--                                  // cmp	x12, x9
	BLE  LBB6_53 // <--                                  // b.le	.LBB6_53
	MOVD $-1, R8 // <--                                  // mov	x8, #-1

LBB6_9:
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret

LBB6_10:
	MOVD ZR, R0         // <--                                  // mov	x0, xzr
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret

LBB6_11:
	WORD $0x4f04e7e4                 // VMOVI $159, V4.B16                   // movi	v4.16b, #159
	MOVD ZR, R11                     // <--                                  // mov	x11, xzr
	WORD $0x4f00e745                 // VMOVI $26, V5.B16                    // movi	v5.16b, #26
	ADD  $16, R0, R12                // <--                                  // add	x12, x0, #16
	WORD $0x4f01e406                 // VMOVI $32, V6.B16                    // movi	v6.16b, #32
	ADD  $32, R0, R13                // <--                                  // add	x13, x0, #32
	ADD  $48, R0, R14                // <--                                  // add	x14, x0, #48
	MOVW $64, R8                     // <--                                  // mov	w8, #64
	MOVW $15, R15                    // <--                                  // mov	w15, #15
	MOVD $tail_mask_table<>(SB), R16 // <--                                  // adrp	x16, tail_mask_table
	NOP                              // (skipped)                            // add	x16, x16, :lo12:tail_mask_table

LBB6_12:
	MOVD  R11, R17                                    // <--                                  // mov	x17, x11
	ADD   R11, R0, R11                                // <--                                  // add	x11, x0, x11
	ADD   R3, R11, R1                                 // <--                                  // add	x1, x11, x3
	ADD   R5, R11, R11                                // <--                                  // add	x11, x11, x5
	VLD1  (R1), [V16.B16, V17.B16, V18.B16, V19.B16]  // <--                                  // ld1	{ v16.16b, v17.16b, v18.16b, v19.16b }, [x1]
	VLD1  (R11), [V20.B16, V21.B16, V22.B16, V23.B16] // <--                                  // ld1	{ v20.16b, v21.16b, v22.16b, v23.16b }, [x11]
	MOVD  R8, R11                                     // <--                                  // mov	x11, x8
	VAND  V2.B16, V16.B16, V7.B16                     // <--                                  // and	v7.16b, v16.16b, v2.16b
	VAND  V2.B16, V17.B16, V25.B16                    // <--                                  // and	v25.16b, v17.16b, v2.16b
	VAND  V2.B16, V18.B16, V26.B16                    // <--                                  // and	v26.16b, v18.16b, v2.16b
	VAND  V2.B16, V19.B16, V16.B16                    // <--                                  // and	v16.16b, v19.16b, v2.16b
	VCMEQ V0.B16, V7.B16, V7.B16                      // <--                                  // cmeq	v7.16b, v7.16b, v0.16b
	VAND  V3.B16, V20.B16, V24.B16                    // <--                                  // and	v24.16b, v20.16b, v3.16b
	VAND  V3.B16, V21.B16, V17.B16                    // <--                                  // and	v17.16b, v21.16b, v3.16b
	VAND  V3.B16, V22.B16, V18.B16                    // <--                                  // and	v18.16b, v22.16b, v3.16b
	VAND  V3.B16, V23.B16, V19.B16                    // <--                                  // and	v19.16b, v23.16b, v3.16b
	VCMEQ V1.B16, V24.B16, V20.B16                    // <--                                  // cmeq	v20.16b, v24.16b, v1.16b
	VCMEQ V0.B16, V25.B16, V21.B16                    // <--                                  // cmeq	v21.16b, v25.16b, v0.16b
	VCMEQ V1.B16, V17.B16, V17.B16                    // <--                                  // cmeq	v17.16b, v17.16b, v1.16b
	VCMEQ V0.B16, V26.B16, V22.B16                    // <--                                  // cmeq	v22.16b, v26.16b, v0.16b
	VCMEQ V1.B16, V18.B16, V23.B16                    // <--                                  // cmeq	v23.16b, v18.16b, v1.16b
	VCMEQ V0.B16, V16.B16, V24.B16                    // <--                                  // cmeq	v24.16b, v16.16b, v0.16b
	VCMEQ V1.B16, V19.B16, V19.B16                    // <--                                  // cmeq	v19.16b, v19.16b, v1.16b
	VAND  V7.B16, V20.B16, V18.B16                    // <--                                  // and	v18.16b, v20.16b, v7.16b
	VAND  V21.B16, V17.B16, V17.B16                   // <--                                  // and	v17.16b, v17.16b, v21.16b
	VAND  V22.B16, V23.B16, V16.B16                   // <--                                  // and	v16.16b, v23.16b, v22.16b
	VAND  V24.B16, V19.B16, V7.B16                    // <--                                  // and	v7.16b, v19.16b, v24.16b
	VORR  V7.B16, V16.B16, V19.B16                    // <--                                  // orr	v19.16b, v16.16b, v7.16b
	VORR  V18.B16, V17.B16, V20.B16                   // <--                                  // orr	v20.16b, v17.16b, v18.16b
	VORR  V20.B16, V19.B16, V19.B16                   // <--                                  // orr	v19.16b, v19.16b, v20.16b
	WORD  $0x6e30aa73                                 // VUMAXV V19.B16, V19                  // umaxv	b19, v19.16b
	FMOVS F19, R1                                     // <--                                  // fmov	w1, s19
	CBZW  R1, LBB6_17                                 // <--                                  // cbz	w1, .LBB6_17
	WORD  $0x0f0c8652                                 // VSHRN $4, V18.H8, V18.B8             // shrn	v18.8b, v18.8h, #4
	FMOVD F18, R1                                     // <--                                  // fmov	x1, d18
	CBNZ  R1, LBB6_19                                 // <--                                  // cbnz	x1, .LBB6_19

LBB6_14:
	WORD  $0x0f0c8631 // VSHRN $4, V17.H8, V17.B8             // shrn	v17.8b, v17.8h, #4
	FMOVD F17, R8     // <--                                  // fmov	x8, d17
	CBNZ  R8, LBB6_25 // <--                                  // cbnz	x8, .LBB6_25

LBB6_15:
	WORD  $0x0f0c8610 // VSHRN $4, V16.H8, V16.B8             // shrn	v16.8b, v16.8h, #4
	FMOVD F16, R8     // <--                                  // fmov	x8, d16
	CBNZ  R8, LBB6_31 // <--                                  // cbnz	x8, .LBB6_31

LBB6_16:
	WORD  $0x0f0c84e7 // VSHRN $4, V7.H8, V7.B8               // shrn	v7.8b, v7.8h, #4
	FMOVD F7, R8      // <--                                  // fmov	x8, d7
	CBNZ  R8, LBB6_37 // <--                                  // cbnz	x8, .LBB6_37

LBB6_17:
	ADD $64, R11, R8 // <--                                  // add	x8, x11, #64
	CMP R10, R8      // <--                                  // cmp	x8, x10
	BLE LBB6_12      // <--                                  // b.le	.LBB6_12
	JMP LBB6_5       // <--                                  // b	.LBB6_5

LBB6_18:
	CBZ R1, LBB6_14 // <--                                  // cbz	x1, .LBB6_14

LBB6_19:
	RBIT R1, R8         // <--                                  // rbit	x8, x1
	MOVD R7, R19        // <--                                  // mov	x19, x7
	CLZ  R8, R8         // <--                                  // clz	x8, x8
	AND  $60, R8, R4    // <--                                  // and	x4, x8, #0x3c
	ORR  R8>>2, R17, R8 // <--                                  // orr	x8, x17, x8, lsr #2
	ADD  R8, R0, R2     // <--                                  // add	x2, x0, x8
	LSL  R4, R15, R4    // <--                                  // lsl	x4, x15, x4
	BIC  R4, R1, R1     // <--                                  // bic	x1, x1, x4
	MOVD R6, R4         // <--                                  // mov	x4, x6

LBB6_20:
	SUBS  $16, R19, R20             // <--                                  // subs	x20, x19, #16
	BLT   LBB6_22                   // <--                                  // b.lt	.LBB6_22
	WORD  $0x3cc10452               // FMOVQ.P 16(R2), F18                  // ldr	q18, [x2], #16
	WORD  $0x3cc10494               // FMOVQ.P 16(R4), F20                  // ldr	q20, [x4], #16
	MOVD  R20, R19                  // <--                                  // mov	x19, x20
	VADD  V4.B16, V18.B16, V19.B16  // <--                                  // add	v19.16b, v18.16b, v4.16b
	VEOR  V18.B16, V20.B16, V18.B16 // <--                                  // eor	v18.16b, v20.16b, v18.16b
	WORD  $0x6e3334b3               // VCMHI V19.B16, V5.B16, V19.B16       // cmhi	v19.16b, v5.16b, v19.16b
	VAND  V6.B16, V19.B16, V19.B16  // <--                                  // and	v19.16b, v19.16b, v6.16b
	VEOR  V19.B16, V18.B16, V18.B16 // <--                                  // eor	v18.16b, v18.16b, v19.16b
	WORD  $0x6e30aa52               // VUMAXV V18.B16, V18                  // umaxv	b18, v18.16b
	FMOVS F18, R21                  // <--                                  // fmov	w21, s18
	CBZW  R21, LBB6_20              // <--                                  // cbz	w21, .LBB6_20
	JMP   LBB6_18                   // <--                                  // b	.LBB6_18

LBB6_22:
	CMP   $1, R19                   // <--                                  // cmp	x19, #1
	BLT   LBB6_9                    // <--                                  // b.lt	.LBB6_9
	WORD  $0x3dc00052               // FMOVQ (R2), F18                      // ldr	q18, [x2]
	WORD  $0x3dc00094               // FMOVQ (R4), F20                      // ldr	q20, [x4]
	WORD  $0x3cf37a15               // FMOVQ (R16)(R19<<4), F21             // ldr	q21, [x16, x19, lsl #4]
	VADD  V4.B16, V18.B16, V19.B16  // <--                                  // add	v19.16b, v18.16b, v4.16b
	VEOR  V18.B16, V20.B16, V18.B16 // <--                                  // eor	v18.16b, v20.16b, v18.16b
	WORD  $0x6e3334b3               // VCMHI V19.B16, V5.B16, V19.B16       // cmhi	v19.16b, v5.16b, v19.16b
	VAND  V6.B16, V19.B16, V19.B16  // <--                                  // and	v19.16b, v19.16b, v6.16b
	VEOR  V19.B16, V18.B16, V18.B16 // <--                                  // eor	v18.16b, v18.16b, v19.16b
	VAND  V21.B16, V18.B16, V18.B16 // <--                                  // and	v18.16b, v18.16b, v21.16b
	WORD  $0x6e30aa52               // VUMAXV V18.B16, V18                  // umaxv	b18, v18.16b
	FMOVS F18, R2                   // <--                                  // fmov	w2, s18
	CBNZW R2, LBB6_18               // <--                                  // cbnz	w2, .LBB6_18
	JMP   LBB6_9                    // <--                                  // b	.LBB6_9

LBB6_24:
	CBZ R8, LBB6_15 // <--                                  // cbz	x8, .LBB6_15

LBB6_25:
	RBIT R8, R1       // <--                                  // rbit	x1, x8
	ADD  R17, R12, R4 // <--                                  // add	x4, x12, x17
	CLZ  R1, R1       // <--                                  // clz	x1, x1
	AND  $60, R1, R2  // <--                                  // and	x2, x1, #0x3c
	LSR  $2, R1, R1   // <--                                  // lsr	x1, x1, #2
	ADD  R1, R4, R19  // <--                                  // add	x19, x4, x1
	MOVD R6, R4       // <--                                  // mov	x4, x6
	LSL  R2, R15, R2  // <--                                  // lsl	x2, x15, x2
	BIC  R2, R8, R8   // <--                                  // bic	x8, x8, x2
	MOVD R7, R2       // <--                                  // mov	x2, x7

LBB6_26:
	CMP   $15, R2                   // <--                                  // cmp	x2, #15
	BLE   LBB6_28                   // <--                                  // b.le	.LBB6_28
	WORD  $0x3cc10671               // FMOVQ.P 16(R19), F17                 // ldr	q17, [x19], #16
	WORD  $0x3cc10493               // FMOVQ.P 16(R4), F19                  // ldr	q19, [x4], #16
	SUB   $16, R2, R2               // <--                                  // sub	x2, x2, #16
	VADD  V4.B16, V17.B16, V18.B16  // <--                                  // add	v18.16b, v17.16b, v4.16b
	VEOR  V17.B16, V19.B16, V17.B16 // <--                                  // eor	v17.16b, v19.16b, v17.16b
	WORD  $0x6e3234b2               // VCMHI V18.B16, V5.B16, V18.B16       // cmhi	v18.16b, v5.16b, v18.16b
	VAND  V6.B16, V18.B16, V18.B16  // <--                                  // and	v18.16b, v18.16b, v6.16b
	VEOR  V18.B16, V17.B16, V17.B16 // <--                                  // eor	v17.16b, v17.16b, v18.16b
	WORD  $0x6e30aa31               // VUMAXV V17.B16, V17                  // umaxv	b17, v17.16b
	FMOVS F17, R20                  // <--                                  // fmov	w20, s17
	CBZW  R20, LBB6_26              // <--                                  // cbz	w20, .LBB6_26
	JMP   LBB6_24                   // <--                                  // b	.LBB6_24

LBB6_28:
	CMP   $1, R2                    // <--                                  // cmp	x2, #1
	BLT   LBB6_62                   // <--                                  // b.lt	.LBB6_62
	WORD  $0x3dc00271               // FMOVQ (R19), F17                     // ldr	q17, [x19]
	WORD  $0x3dc00093               // FMOVQ (R4), F19                      // ldr	q19, [x4]
	WORD  $0x3ce27a14               // FMOVQ (R16)(R2<<4), F20              // ldr	q20, [x16, x2, lsl #4]
	VADD  V4.B16, V17.B16, V18.B16  // <--                                  // add	v18.16b, v17.16b, v4.16b
	VEOR  V17.B16, V19.B16, V17.B16 // <--                                  // eor	v17.16b, v19.16b, v17.16b
	WORD  $0x6e3234b2               // VCMHI V18.B16, V5.B16, V18.B16       // cmhi	v18.16b, v5.16b, v18.16b
	VAND  V6.B16, V18.B16, V18.B16  // <--                                  // and	v18.16b, v18.16b, v6.16b
	VEOR  V18.B16, V17.B16, V17.B16 // <--                                  // eor	v17.16b, v17.16b, v18.16b
	VAND  V20.B16, V17.B16, V17.B16 // <--                                  // and	v17.16b, v17.16b, v20.16b
	WORD  $0x6e30aa31               // VUMAXV V17.B16, V17                  // umaxv	b17, v17.16b
	FMOVS F17, R2                   // <--                                  // fmov	w2, s17
	CBNZW R2, LBB6_24               // <--                                  // cbnz	w2, .LBB6_24
	JMP   LBB6_62                   // <--                                  // b	.LBB6_62

LBB6_30:
	CBZ R8, LBB6_16 // <--                                  // cbz	x8, .LBB6_16

LBB6_31:
	RBIT R8, R1       // <--                                  // rbit	x1, x8
	ADD  R17, R13, R4 // <--                                  // add	x4, x13, x17
	CLZ  R1, R1       // <--                                  // clz	x1, x1
	AND  $60, R1, R2  // <--                                  // and	x2, x1, #0x3c
	LSR  $2, R1, R1   // <--                                  // lsr	x1, x1, #2
	ADD  R1, R4, R19  // <--                                  // add	x19, x4, x1
	MOVD R6, R4       // <--                                  // mov	x4, x6
	LSL  R2, R15, R2  // <--                                  // lsl	x2, x15, x2
	BIC  R2, R8, R8   // <--                                  // bic	x8, x8, x2
	MOVD R7, R2       // <--                                  // mov	x2, x7

LBB6_32:
	CMP   $15, R2                   // <--                                  // cmp	x2, #15
	BLE   LBB6_34                   // <--                                  // b.le	.LBB6_34
	WORD  $0x3cc10670               // FMOVQ.P 16(R19), F16                 // ldr	q16, [x19], #16
	WORD  $0x3cc10492               // FMOVQ.P 16(R4), F18                  // ldr	q18, [x4], #16
	SUB   $16, R2, R2               // <--                                  // sub	x2, x2, #16
	VADD  V4.B16, V16.B16, V17.B16  // <--                                  // add	v17.16b, v16.16b, v4.16b
	VEOR  V16.B16, V18.B16, V16.B16 // <--                                  // eor	v16.16b, v18.16b, v16.16b
	WORD  $0x6e3134b1               // VCMHI V17.B16, V5.B16, V17.B16       // cmhi	v17.16b, v5.16b, v17.16b
	VAND  V6.B16, V17.B16, V17.B16  // <--                                  // and	v17.16b, v17.16b, v6.16b
	VEOR  V17.B16, V16.B16, V16.B16 // <--                                  // eor	v16.16b, v16.16b, v17.16b
	WORD  $0x6e30aa10               // VUMAXV V16.B16, V16                  // umaxv	b16, v16.16b
	FMOVS F16, R20                  // <--                                  // fmov	w20, s16
	CBZW  R20, LBB6_32              // <--                                  // cbz	w20, .LBB6_32
	JMP   LBB6_30                   // <--                                  // b	.LBB6_30

LBB6_34:
	CMP   $1, R2                    // <--                                  // cmp	x2, #1
	BLT   LBB6_63                   // <--                                  // b.lt	.LBB6_63
	WORD  $0x3dc00270               // FMOVQ (R19), F16                     // ldr	q16, [x19]
	WORD  $0x3dc00092               // FMOVQ (R4), F18                      // ldr	q18, [x4]
	WORD  $0x3ce27a13               // FMOVQ (R16)(R2<<4), F19              // ldr	q19, [x16, x2, lsl #4]
	VADD  V4.B16, V16.B16, V17.B16  // <--                                  // add	v17.16b, v16.16b, v4.16b
	VEOR  V16.B16, V18.B16, V16.B16 // <--                                  // eor	v16.16b, v18.16b, v16.16b
	WORD  $0x6e3134b1               // VCMHI V17.B16, V5.B16, V17.B16       // cmhi	v17.16b, v5.16b, v17.16b
	VAND  V6.B16, V17.B16, V17.B16  // <--                                  // and	v17.16b, v17.16b, v6.16b
	VEOR  V17.B16, V16.B16, V16.B16 // <--                                  // eor	v16.16b, v16.16b, v17.16b
	VAND  V19.B16, V16.B16, V16.B16 // <--                                  // and	v16.16b, v16.16b, v19.16b
	WORD  $0x6e30aa10               // VUMAXV V16.B16, V16                  // umaxv	b16, v16.16b
	FMOVS F16, R2                   // <--                                  // fmov	w2, s16
	CBNZW R2, LBB6_30               // <--                                  // cbnz	w2, .LBB6_30
	JMP   LBB6_63                   // <--                                  // b	.LBB6_63

LBB6_36:
	CBZ R8, LBB6_17 // <--                                  // cbz	x8, .LBB6_17

LBB6_37:
	RBIT R8, R1       // <--                                  // rbit	x1, x8
	ADD  R17, R14, R4 // <--                                  // add	x4, x14, x17
	CLZ  R1, R1       // <--                                  // clz	x1, x1
	AND  $60, R1, R2  // <--                                  // and	x2, x1, #0x3c
	LSR  $2, R1, R1   // <--                                  // lsr	x1, x1, #2
	ADD  R1, R4, R19  // <--                                  // add	x19, x4, x1
	MOVD R6, R4       // <--                                  // mov	x4, x6
	LSL  R2, R15, R2  // <--                                  // lsl	x2, x15, x2
	BIC  R2, R8, R8   // <--                                  // bic	x8, x8, x2
	MOVD R7, R2       // <--                                  // mov	x2, x7

LBB6_38:
	CMP   $15, R2                  // <--                                  // cmp	x2, #15
	BLE   LBB6_40                  // <--                                  // b.le	.LBB6_40
	WORD  $0x3cc10667              // FMOVQ.P 16(R19), F7                  // ldr	q7, [x19], #16
	WORD  $0x3cc10491              // FMOVQ.P 16(R4), F17                  // ldr	q17, [x4], #16
	SUB   $16, R2, R2              // <--                                  // sub	x2, x2, #16
	VADD  V4.B16, V7.B16, V16.B16  // <--                                  // add	v16.16b, v7.16b, v4.16b
	VEOR  V7.B16, V17.B16, V7.B16  // <--                                  // eor	v7.16b, v17.16b, v7.16b
	WORD  $0x6e3034b0              // VCMHI V16.B16, V5.B16, V16.B16       // cmhi	v16.16b, v5.16b, v16.16b
	VAND  V6.B16, V16.B16, V16.B16 // <--                                  // and	v16.16b, v16.16b, v6.16b
	VEOR  V16.B16, V7.B16, V7.B16  // <--                                  // eor	v7.16b, v7.16b, v16.16b
	WORD  $0x6e30a8e7              // VUMAXV V7.B16, V7                    // umaxv	b7, v7.16b
	FMOVS F7, R20                  // <--                                  // fmov	w20, s7
	CBZW  R20, LBB6_38             // <--                                  // cbz	w20, .LBB6_38
	JMP   LBB6_36                  // <--                                  // b	.LBB6_36

LBB6_40:
	CMP   $1, R2                   // <--                                  // cmp	x2, #1
	BLT   LBB6_42                  // <--                                  // b.lt	.LBB6_42
	WORD  $0x3dc00267              // FMOVQ (R19), F7                      // ldr	q7, [x19]
	WORD  $0x3dc00091              // FMOVQ (R4), F17                      // ldr	q17, [x4]
	WORD  $0x3ce27a12              // FMOVQ (R16)(R2<<4), F18              // ldr	q18, [x16, x2, lsl #4]
	VADD  V4.B16, V7.B16, V16.B16  // <--                                  // add	v16.16b, v7.16b, v4.16b
	VEOR  V7.B16, V17.B16, V7.B16  // <--                                  // eor	v7.16b, v17.16b, v7.16b
	WORD  $0x6e3034b0              // VCMHI V16.B16, V5.B16, V16.B16       // cmhi	v16.16b, v5.16b, v16.16b
	VAND  V6.B16, V16.B16, V16.B16 // <--                                  // and	v16.16b, v16.16b, v6.16b
	VEOR  V16.B16, V7.B16, V7.B16  // <--                                  // eor	v7.16b, v7.16b, v16.16b
	VAND  V18.B16, V7.B16, V7.B16  // <--                                  // and	v7.16b, v7.16b, v18.16b
	WORD  $0x6e30a8e7              // VUMAXV V7.B16, V7                    // umaxv	b7, v7.16b
	FMOVS F7, R2                   // <--                                  // fmov	w2, s7
	CBNZW R2, LBB6_36              // <--                                  // cbnz	w2, .LBB6_36

LBB6_42:
	ORR  R1, R17, R8    // <--                                  // orr	x8, x17, x1
	ORR  $48, R8, R8    // <--                                  // orr	x8, x8, #0x30
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret

LBB6_43:
	WORD $0x4f04e7e4                 // VMOVI $159, V4.B16                   // movi	v4.16b, #159
	MOVW $15, R8                     // <--                                  // mov	w8, #15
	WORD $0x4f00e745                 // VMOVI $26, V5.B16                    // movi	v5.16b, #26
	MOVD $tail_mask_table<>(SB), R13 // <--                                  // adrp	x13, tail_mask_table
	NOP                              // (skipped)                            // add	x13, x13, :lo12:tail_mask_table
	WORD $0x4f01e406                 // VMOVI $32, V6.B16                    // movi	v6.16b, #32

LBB6_44:
	ADD   R11, R0, R14             // <--                                  // add	x14, x0, x11
	MOVD  R16, R12                 // <--                                  // mov	x12, x16
	WORD  $0x3ce369c7              // FMOVQ (R14)(R3), F7                  // ldr	q7, [x14, x3]
	WORD  $0x3ce569d0              // FMOVQ (R14)(R5), F16                 // ldr	q16, [x14, x5]
	VAND  V2.B16, V7.B16, V7.B16   // <--                                  // and	v7.16b, v7.16b, v2.16b
	VAND  V3.B16, V16.B16, V16.B16 // <--                                  // and	v16.16b, v16.16b, v3.16b
	VCMEQ V0.B16, V7.B16, V7.B16   // <--                                  // cmeq	v7.16b, v7.16b, v0.16b
	VCMEQ V1.B16, V16.B16, V16.B16 // <--                                  // cmeq	v16.16b, v16.16b, v1.16b
	VAND  V7.B16, V16.B16, V7.B16  // <--                                  // and	v7.16b, v16.16b, v7.16b
	WORD  $0x0f0c84e7              // VSHRN $4, V7.H8, V7.B8               // shrn	v7.8b, v7.8h, #4
	FMOVD F7, R15                  // <--                                  // fmov	x15, d7
	CBNZ  R15, LBB6_47             // <--                                  // cbnz	x15, .LBB6_47

LBB6_45:
	ADD  $16, R12, R16 // <--                                  // add	x16, x12, #16
	MOVD R12, R11      // <--                                  // mov	x11, x12
	CMP  R10, R16      // <--                                  // cmp	x16, x10
	BLE  LBB6_44       // <--                                  // b.le	.LBB6_44
	JMP  LBB6_7        // <--                                  // b	.LBB6_7

LBB6_46:
	CBZ R15, LBB6_45 // <--                                  // cbz	x15, .LBB6_45

LBB6_47:
	RBIT R15, R16      // <--                                  // rbit	x16, x15
	MOVD R7, R2        // <--                                  // mov	x2, x7
	CLZ  R16, R16      // <--                                  // clz	x16, x16
	AND  $60, R16, R17 // <--                                  // and	x17, x16, #0x3c
	LSR  $2, R16, R16  // <--                                  // lsr	x16, x16, #2
	ADD  R16, R14, R1  // <--                                  // add	x1, x14, x16
	LSL  R17, R8, R17  // <--                                  // lsl	x17, x8, x17
	BIC  R17, R15, R15 // <--                                  // bic	x15, x15, x17
	MOVD R6, R17       // <--                                  // mov	x17, x6

LBB6_48:
	SUBS  $16, R2, R4              // <--                                  // subs	x4, x2, #16
	BLT   LBB6_50                  // <--                                  // b.lt	.LBB6_50
	WORD  $0x3cc10427              // FMOVQ.P 16(R1), F7                   // ldr	q7, [x1], #16
	WORD  $0x3cc10631              // FMOVQ.P 16(R17), F17                 // ldr	q17, [x17], #16
	MOVD  R4, R2                   // <--                                  // mov	x2, x4
	VADD  V4.B16, V7.B16, V16.B16  // <--                                  // add	v16.16b, v7.16b, v4.16b
	VEOR  V7.B16, V17.B16, V7.B16  // <--                                  // eor	v7.16b, v17.16b, v7.16b
	WORD  $0x6e3034b0              // VCMHI V16.B16, V5.B16, V16.B16       // cmhi	v16.16b, v5.16b, v16.16b
	VAND  V6.B16, V16.B16, V16.B16 // <--                                  // and	v16.16b, v16.16b, v6.16b
	VEOR  V16.B16, V7.B16, V7.B16  // <--                                  // eor	v7.16b, v7.16b, v16.16b
	WORD  $0x6e30a8e7              // VUMAXV V7.B16, V7                    // umaxv	b7, v7.16b
	FMOVS F7, R19                  // <--                                  // fmov	w19, s7
	CBZW  R19, LBB6_48             // <--                                  // cbz	w19, .LBB6_48
	JMP   LBB6_46                  // <--                                  // b	.LBB6_46

LBB6_50:
	CMP   $1, R2                   // <--                                  // cmp	x2, #1
	BLT   LBB6_52                  // <--                                  // b.lt	.LBB6_52
	WORD  $0x3dc00027              // FMOVQ (R1), F7                       // ldr	q7, [x1]
	WORD  $0x3dc00231              // FMOVQ (R17), F17                     // ldr	q17, [x17]
	WORD  $0x3ce279b2              // FMOVQ (R13)(R2<<4), F18              // ldr	q18, [x13, x2, lsl #4]
	VADD  V4.B16, V7.B16, V16.B16  // <--                                  // add	v16.16b, v7.16b, v4.16b
	VEOR  V7.B16, V17.B16, V7.B16  // <--                                  // eor	v7.16b, v17.16b, v7.16b
	WORD  $0x6e3034b0              // VCMHI V16.B16, V5.B16, V16.B16       // cmhi	v16.16b, v5.16b, v16.16b
	VAND  V6.B16, V16.B16, V16.B16 // <--                                  // and	v16.16b, v16.16b, v6.16b
	VEOR  V16.B16, V7.B16, V7.B16  // <--                                  // eor	v7.16b, v7.16b, v16.16b
	VAND  V18.B16, V7.B16, V7.B16  // <--                                  // and	v7.16b, v7.16b, v18.16b
	WORD  $0x6e30a8e7              // VUMAXV V7.B16, V7                    // umaxv	b7, v7.16b
	FMOVS F7, R17                  // <--                                  // fmov	w17, s7
	CBNZW R17, LBB6_46             // <--                                  // cbnz	w17, .LBB6_46

LBB6_52:
	ORR  R11, R16, R8   // <--                                  // orr	x8, x16, x11
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret

LBB6_53:
	SUB   R12, R10, R8                // <--                                  // sub	x8, x10, x12
	ADD   R12, R0, R10                // <--                                  // add	x10, x0, x12
	MOVW  $15, R11                    // <--                                  // mov	w11, #15
	CMP   $15, R8                     // <--                                  // cmp	x8, #15
	WORD  $0x3ce36944                 // FMOVQ (R10)(R3), F4                  // ldr	q4, [x10, x3]
	CSEL  LT, R8, R11, R8             // <--                                  // csel	x8, x8, x11, lt
	WORD  $0x3ce56945                 // FMOVQ (R10)(R5), F5                  // ldr	q5, [x10, x5]
	MOVD  $tail_mask_table<>(SB), R11 // <--                                  // adrp	x11, tail_mask_table
	NOP                               // (skipped)                            // add	x11, x11, :lo12:tail_mask_table
	WORD  $0x3ce87966                 // FMOVQ (R11)(R8<<4), F6               // ldr	q6, [x11, x8, lsl #4]
	VAND  V2.B16, V4.B16, V2.B16      // <--                                  // and	v2.16b, v4.16b, v2.16b
	VAND  V3.B16, V5.B16, V3.B16      // <--                                  // and	v3.16b, v5.16b, v3.16b
	VCMEQ V0.B16, V2.B16, V0.B16      // <--                                  // cmeq	v0.16b, v2.16b, v0.16b
	VCMEQ V1.B16, V3.B16, V1.B16      // <--                                  // cmeq	v1.16b, v3.16b, v1.16b
	VAND  V0.B16, V1.B16, V0.B16      // <--                                  // and	v0.16b, v1.16b, v0.16b
	VAND  V0.B16, V6.B16, V0.B16      // <--                                  // and	v0.16b, v6.16b, v0.16b
	WORD  $0x0f0c8400                 // VSHRN $4, V0.H8, V0.B8               // shrn	v0.8b, v0.8h, #4
	FMOVD F0, R13                     // <--                                  // fmov	x13, d0
	CBZ   R13, LBB6_64                // <--                                  // cbz	x13, .LBB6_64
	WORD  $0x4f04e7e0                 // VMOVI $159, V0.B16                   // movi	v0.16b, #159
	MOVW  $15, R14                    // <--                                  // mov	w14, #15
	WORD  $0x4f00e741                 // VMOVI $26, V1.B16                    // movi	v1.16b, #26
	WORD  $0x4f01e402                 // VMOVI $32, V2.B16                    // movi	v2.16b, #32
	JMP   LBB6_56                     // <--                                  // b	.LBB6_56

LBB6_55:
	AND  $60, R15, R8 // <--                                  // and	x8, x15, #0x3c
	LSL  R8, R14, R8  // <--                                  // lsl	x8, x14, x8
	BIC  R8, R13, R13 // <--                                  // bic	x13, x13, x8
	MOVD $-1, R8      // <--                                  // mov	x8, #-1
	CBZ  R13, LBB6_9  // <--                                  // cbz	x13, .LBB6_9

LBB6_56:
	RBIT R13, R8       // <--                                  // rbit	x8, x13
	CLZ  R8, R15       // <--                                  // clz	x15, x8
	LSR  $2, R15, R16  // <--                                  // lsr	x16, x15, #2
	ORR  R12, R16, R8  // <--                                  // orr	x8, x16, x12
	CMP  R9, R8        // <--                                  // cmp	x8, x9
	BGT  LBB6_55       // <--                                  // b.gt	.LBB6_55
	ADD  R16, R10, R17 // <--                                  // add	x17, x10, x16
	MOVD R7, R0        // <--                                  // mov	x0, x7
	MOVD R6, R16       // <--                                  // mov	x16, x6

LBB6_58:
	SUBS  $16, R0, R1            // <--                                  // subs	x1, x0, #16
	BLT   LBB6_60                // <--                                  // b.lt	.LBB6_60
	WORD  $0x3cc10623            // FMOVQ.P 16(R17), F3                  // ldr	q3, [x17], #16
	WORD  $0x3cc10605            // FMOVQ.P 16(R16), F5                  // ldr	q5, [x16], #16
	MOVD  R1, R0                 // <--                                  // mov	x0, x1
	VADD  V0.B16, V3.B16, V4.B16 // <--                                  // add	v4.16b, v3.16b, v0.16b
	VEOR  V3.B16, V5.B16, V3.B16 // <--                                  // eor	v3.16b, v5.16b, v3.16b
	WORD  $0x6e243424            // VCMHI V4.B16, V1.B16, V4.B16         // cmhi	v4.16b, v1.16b, v4.16b
	VAND  V2.B16, V4.B16, V4.B16 // <--                                  // and	v4.16b, v4.16b, v2.16b
	VEOR  V4.B16, V3.B16, V3.B16 // <--                                  // eor	v3.16b, v3.16b, v4.16b
	WORD  $0x6e30a863            // VUMAXV V3.B16, V3                    // umaxv	b3, v3.16b
	FMOVS F3, R2                 // <--                                  // fmov	w2, s3
	CBZW  R2, LBB6_58            // <--                                  // cbz	w2, .LBB6_58
	JMP   LBB6_55                // <--                                  // b	.LBB6_55

LBB6_60:
	CMP   $1, R0                 // <--                                  // cmp	x0, #1
	BLT   LBB6_9                 // <--                                  // b.lt	.LBB6_9
	WORD  $0x3dc00223            // FMOVQ (R17), F3                      // ldr	q3, [x17]
	WORD  $0x3dc00205            // FMOVQ (R16), F5                      // ldr	q5, [x16]
	WORD  $0x3ce07966            // FMOVQ (R11)(R0<<4), F6               // ldr	q6, [x11, x0, lsl #4]
	VADD  V0.B16, V3.B16, V4.B16 // <--                                  // add	v4.16b, v3.16b, v0.16b
	VEOR  V3.B16, V5.B16, V3.B16 // <--                                  // eor	v3.16b, v5.16b, v3.16b
	WORD  $0x6e243424            // VCMHI V4.B16, V1.B16, V4.B16         // cmhi	v4.16b, v1.16b, v4.16b
	VAND  V2.B16, V4.B16, V4.B16 // <--                                  // and	v4.16b, v4.16b, v2.16b
	VEOR  V4.B16, V3.B16, V3.B16 // <--                                  // eor	v3.16b, v3.16b, v4.16b
	VAND  V6.B16, V3.B16, V3.B16 // <--                                  // and	v3.16b, v3.16b, v6.16b
	WORD  $0x6e30a863            // VUMAXV V3.B16, V3                    // umaxv	b3, v3.16b
	FMOVS F3, R16                // <--                                  // fmov	w16, s3
	CBNZW R16, LBB6_55           // <--                                  // cbnz	w16, .LBB6_55
	JMP   LBB6_9                 // <--                                  // b	.LBB6_9

LBB6_62:
	ORR  R1, R17, R8    // <--                                  // orr	x8, x17, x1
	ORR  $16, R8, R8    // <--                                  // orr	x8, x8, #0x10
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret

LBB6_63:
	ORR  R1, R17, R8    // <--                                  // orr	x8, x17, x1
	ORR  $32, R8, R8    // <--                                  // orr	x8, x8, #0x20
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret

LBB6_64:
	MOVD $-1, R8        // <--                                  // mov	x8, #-1
	NOP                 // (skipped)                            // ldp	x20, x19, [sp, #32]
	NOP                 // (skipped)                            // ldr	x21, [sp, #16]
	NOP                 // (skipped)                            // ldp	x29, x30, [sp], #48
	MOVD R8, R0         // <--                                  // mov	x0, x8
	MOVD R0, ret+64(FP) // <--
	RET                 // <--                                  // ret
